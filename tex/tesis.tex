\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{polyglossia}
\setmainlanguage{spanish}
\usepackage{pythonhighlight}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage[showframe=true]{geometry}
\usepackage{changepage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{svg}
\usepackage[bookmarks = true, colorlinks=true, linkcolor = black, citecolor = black, menucolor = black, urlcolor = black]{hyperref}
%\usepackage[spanish,activeacute]{babel}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{float}
\renewcommand{\baselinestretch}{1} % Interlineado. 1 es estandar
\usepackage[T1]{fontenc}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{mathtools}
\usepackage{fancyhdr}
\fancyhead[R]{2021}\fancyhead[L]{UNC - FCEFyN} \fancyfoot[C]{\thepage}
\pagestyle{fancy}
\usepackage[numbered]{bookmark} % Para que figure las secciones en el PDF
\graphicspath{ {img/} }
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\begin{titlepage}
	
	{\scshape\LARGE Universidad Nacional de Córdoba \par}
	%\vspace{1cm}
	{\Large Facultad de Ciencias Exactas, Físicas y Naturales \par}
	\vspace{0.5cm}
	\centering
	\includegraphics[width=0.5\textwidth]{unc.png}
	\par\vspace{0.5cm}
	\vspace{0.5cm}
	{\scshape\Large Proyecto Final Integrador\par}
	\vspace{1.5cm}
	{\large\bfseries ``Predicción de cantidad de defectos graves en vehículos utilitarios en planta automotriz'' \par}
	\vspace{1.5cm}
	{\Large\bfseries Gerardo A. Collante\par}
	
	\vfill
	\textbf{Supervisor}\par
	Dr. Ing.~Orlando \textsc{Micolini}

	\vfill

% Bottom of the page
	{\large \today\par}
\end{titlepage}

\tableofcontents

\clearpage

\section{Motivación}

El \textit{machine learning} se ha erigido como un campo más en el mundo de las tecnologías de la información, sumado a su vertiginoso crecimiento y amparado bajo la constante mejora del \textit{hardware} ha hecho que su popularidad se dispare.

Más allá del todo el \textit{marketing} que envuelve a la tecnología, es innegable que los años venideros y mejoras en todos los campos serán en gran parte a la IA. Por tanto en búsqueda de mejorar profesionalmente emprendí este proyecto para a través de la práctica y la teoría obtener las herramientas necesarias para poder aspirar a un puesto como ingeniero de inteligencia artificial una vez finalizada mi etapa universitaria.

\clearpage

\section{Objetivo}

Se desea realizar un modelo de \textit{machine learning} capaz de predecir la cantidad de defectos graves utilizando como datos de entrada los defectos anteriores (de menor gravedad usualmente) considerando una ventana de tiempo a determinar.

En funcionamiento es muy similar a lo que se conoce como \textit{forecasting}, utilizado generalmente en la predicción del clima.

\clearpage

\section{Clasificación de modelos de inteligencia artificial}

Hagamos algunas definiciones para ponernos en contexto del campo sobre el cual este proyecto integrador será desarrollado.

\subsection{Inteligencia Artificial}

\begin{quote}
  La \textit{Inteligencia Artificial (Artificial Intelligence)} se define como el estudio de los "agentes inteligentes", i.e. cualquier dispositivo que perciba su entorno y tome medidas que maximicen sus posibilidades de lograr con éxito sus objetivos.
  
  \hfill \citet{poole1998}
\end{quote}

Esta definición nos da la idea de que la IA es un sistema reactivo, que reacciona a cambios externos y actúa en consecuencia.

90000.

\subsubsection{Aprendizaje automático} \label{machinelearning}

\begin{quote}
  El \textit{aprendizaje automático (Machine Learning)} es el estudio científico de algoritmos y modelos estadísticos que los sistemas informáticos utilizan para realizar una tarea específica sin utilizar instrucciones explícitas, sino que se basan en patrones e inferencia. Es visto como un subcampo de inteligencia artificial. Los algoritmos de aprendizaje automático crean un modelo matemático basado en datos de muestra, conocidos como "datos de entrenamiento", para hacer predicciones o decisiones sin ser programado explícitamente para realizar la tarea.
  
  \hfill \citet{bishop2006pattern}
\end{quote}

%El Aprendizaje Automático (\textit{Machine Learning}) es un conjunto de técnicas desarrolladas y aplicadas para generar nuevo conocimiento, la extracción de este se basa en datos o información ya existentes. En los últimos años, el crecimiento y disponibilidad de los datos ha crecido exponencialmente, fomentado por la evolución de la capacidad de cómputo a menor precio.  Esta herramienta nos permite extraer patrones a partir de grandes muestras de datos logrando solucionar diversos problemas de la ingeniería. 

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_1.png}
  	\caption{Diagrama de flujo de una aplicación de \textit{Machine Learning}.}
  	\label{fig:flowchartml.}
  	\end{center}
\end{figure}

Por tanto el Aprendizaje Automático es la generación de un modelo de predicción de salida a partir de grandes cantidades de datos de entrada, realizando un tratamiento de los mismos a través de diferentes etapas bien definidas, como se pueden apreciar en la Fig.~\ref{fig:flowchartml.}, las cuales iremos desarrollando en diferentes secciones.

Es importante destacar la independencia del aprendizaje automático al momento de tomar decisiones a partir de los datos proporcionados sin intervención externa, es decir que no hay una especificación de reglas que dictan cómo deben ser tomadas estas decisiones. A su vez, los modelos obtenidos a partir de los algoritmos de \textit{Machine Learning} deben tener la capacidad de predecir a partir de nuevos datos, nunca antes procesados por el modelo, a esto se lo conoce como \textbf{generalización}. 

\subsection{Preprocesamiento} \label{preprocessing}
Este punto es vital para cualquier proyecto que utiliza algoritmos de \textit{Machine Learning}, debido a que los datos incluidos en los conjuntos conocidos como \textit{datasets}, no suelen presentarse en condiciones para obtener el óptimo rendimiento de los algoritmos de aprendizaje. Estos datos suelen estar desbalanceados, haya faltantes o sean demasiado ruidosos, etc. 

Por lo tanto, una vez que se obtiene el \textit{dataset} de entrada, es primordial investigar, limpiar y transformar los datos con diversas técnicas, de forma que presentemos un conjunto de datos que esté en condiciones de ser entrenado y luego el modelo resultante, al momento de ser probado con datos desconocidos, tenga un desempeño óptimo.  

Para lograr este objetivo se aplican técnicas tales como normalización, reescalado, reducción de dimensionalidad, discretización, tratamiento de anomalías y \textit{outliers}. También de ser necesario se utilizan algoritmos de Aprendizaje no supervisado.

\subsection{Aprendizaje supervisado} \label{supervised}
El aprendizaje supervisado es el enfoque más utilizado y mejor entendido para el aprendizaje automático. Implica una entrada y salida para cada pieza de datos en su \textit{dataset}. 

Por ejemplo, una entrada podría ser una imagen y la salida podría ser la respuesta a ``?`es esto un gato?''. En el aprendizaje supervisado siempre hay una distinción entre el conjunto de entrenamiento o \textit{training} para el cual se nos proporciona la etiqueta (o \textit{label}), y el conjunto de test para el cual la etiqueta debe ser inferida. El algoritmo de aprendizaje debe ajustar el modelo predictivo al \textit{dataset} de entrenamiento y usamos el conjunto de test para evaluar la capacidad de generalización. El aprendizaje supervisado es ideal para tareas donde el modelo necesita predecir resultados. 

\textbf{Agregar validación, test, etc}

\textbf{features}

Estos problemas de predicción podrían involucrar el uso de estadísticas para predecir un valor (por ejemplo, $20 kg$, $\$1498$, $0.80 cm$) o categorizar datos basados en clasificaciones dadas (por ejemplo, ``gato'', ``verde'', ``feliz'') \cite{norman2019aprendizaje}.  El siguiente paso es profundizar en las dos categorías de aprendizaje supervisado que existen: clasificación y regresión.

\subsubsection{Clasificación}

En clasificación, la etiqueta es discreta, por ejemplo \texttt{Spam} y \texttt{No Spam}. En otras palabras, se proporciona una distinción clara entre las categorías. 

Es más, es importante indicar que estas categorías son nominales y no ordinales. Las variables nominales y ordinales son ambas subcategorías de las variables categóricas. Las variables ordinales tienen asociado un orden, por ejemplo, las tallas de las camisetas ``$XL > L > M > S$''. Por el contrario, las variables nominales no implican un orden, por ejemplo, no podemos asumir (en general) ``$naranja > azul > verde$'' \cite{GitHubpa25:online}.

\textbf{multilabel-etc}

Elegir entre dos categorías se denomina \textbf{clasificación binaria}, como lo es el ejemplo de \texttt{Spam} y \texttt{No Spam}, mientras que elegir entre más de dos categorías se denomina \textbf{clasificación multiclase}. 

Un ejemplo de clasificación multiclase podría ser clasificar un conjunto de imágenes de frutas, donde habrá manzanas, naranjas y peras. Es importante resaltar que si en el dataset de entrenamiento no aparece determinada categoría (por ejemplo bananas), nuestro modelo será incapaz de reconocer esa fruta.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{tesis_2.png}
  	\caption{Ejemplo de clasificación binaria.}
  	\label{fig:binaryclassification.}
  	\end{center}
\end{figure}

Para poder apreciar el concepto de clasificación binaria, en la Fig.~\ref{fig:binaryclassification.}  \cite{GitHubpa25:online} se han graficado datos bidimensionales, es decir que cada dato tiene dos valores asociados de acuerdo a los ejes X e Y. El dataset cuenta con 100 muestras, las cuales están divididas en dos clases: ceros (círculos azules) y unos (cuadrados rojos). 

El modelo a utilizar para la predicción será uno de los más conocidos y simples de utilizar en clasificación, \textbf{regresión logística}. Éste es un modelo lineal, lo que significa que creará una frontera de decisión que es lineal en el espacio de entrada, en 2D esto quiere decir que generará una línea recta para separar los puntos azules de los rojos.

Como puede observarse, el modelo no es 100\% preciso ya que algunos puntos azules están en la categoría de los rojos y viceversa, por eso es que existen diversos modelos y debemos elegir según nuestro criterio cuál de ellos se adecúa mejor a nuestras necesidades.

Debido a la cantidad de diversos algoritmos que existen para clasificación es posible caracterizarlos en función de sus pros y contras como se observa en la Figura~\ref{fig:proconsclassification.}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_6.png}
  	\caption{Ventajas y desventajas de los algoritmos de clasificación.}
  	\label{fig:proconsclassification.}
  	\end{center}
\end{figure}

\subsubsection{Regresión}

En regresión, la etiqueta es continua, es decir una salida real. Por ejemplo, en astronomía, la tarea de determinar si un objeto es una estrella, una galaxia o un cuásar es un problema de clasificación: la etiqueta viene de tres categorías distintas. Por otro lado, podríamos querer estimar la edad de un objeto basándonos en su imagen: esto sería regresión, porque la etiqueta (edad) es una cantidad continua \citep{GitHubpa25:online}.

En los problemas de regresión, tenemos como entradas las variables independientes o explicativas y las salidas o etiquetas son variables continuas. Por lo tanto, los modelos de regresión deben encontrar una relación (función lineal, polinomial, entre otras) que nos permitan predecir la salida.

\begin{figure}[]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{tesis_9.png}
  	\caption{Ejemplo de regresión lineal.}
  	\label{fig:regressionlinear.}
  	\end{center}
\end{figure}

Para poder apreciar el concepto de regresión lineal, en la Fig.~\ref{fig:regressionlinear.} se tienen 100 muestras con sus respectivas etiquetas, entonces lo que hace el algoritmo de regresión lineal es ajustar una línea recta que minimice la distancia (en este caso distancia euclídea) entre los puntos de la muestra y dicha recta. Al obtener la recta, disponemos de los parámetros del modelo como son los coeficientes y la intersección, por ende estamos en condiciones de predecir la salida de nuevas muestras.

Así como pudimos listar las ventajas y desventajas con los distintos algoritmos de clasificación también es aplicable a los de regresión como vemos en la Fig.~\ref{fig:proconsregression.}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_10.png}
  	\caption{Ventajas y desventajas de los algoritmos de regresión.}
  	\label{fig:proconsregression.}
  	\end{center}
\end{figure}

\clearpage

\subsection{Aprendizaje no supervisado} \label{unsupervised}

A diferencia de lo que sucede en el aprendizaje supervisado (sección~\ref{supervised}), no disponemos de una salida deseada, tampoco se disponen datos etiquetados o con estructuras definidas. Por lo que el objetivo principal del Aprendizaje no Supervisado es generar esas etiquetas a partir de la información que se extrae de datos proporcionados en el \textit{dataset}, sin tener una referencia de salida.  

Un ejemplo de aprendizaje no supervisado podría ser si nos encontramos con un texto extenso y queremos obtener una especie de resumen, de los temas o tópicos relevantes, probablemente de antemano no se sabe cuales son o su cantidad, por lo tanto nos enfrentamos a la situación de no conocer cuales serian las salidas esperadas del modelo. 
Otros ejemplos clásicos pueden ser agrupar fotografías similares o separación de diferentes fuentes que originan un determinado sonido. 

Como se comentó anteriormente en la sección~\ref{preprocessing}, en la etapa de preprocesamiento, es muy útil aplicar las técnicas del aprendizaje no supervisado ya que se cuentan con grandes cantidades de datos en contextos no conocidos y lo más importante, sin etiquetar. Entonces suele ser una buena práctica dar un primer paso mediante algoritmos de aprendizaje no supervisado antes de pasar los datos a un proceso de aprendizaje supervisado. Como por ejemplo cuando se realizan transformaciones de datos mediante reescalado o estandarización. 

En las próximas subsecciones explicaremos brevemente cada una de las tareas que comprenden el Aprendizaje no Supervisado.

\subsubsection{Detección de anomalías}

Uno de los primeros pasos a realizar cuando se nos presenta un conjunto de datos, es proceder con la tarea llamada detección de anomalías (\textit{anomaly detection}, AD), o identificación de \textit{outliers} o datos fuera de rango. 

Un \textit{outlier} puede ser considerado como un dato atípico en un dataset. O bien “un \textit{outlier} es una observación en un dataset que parece ser inconsistente con el resto del conjunto”. Johnson 1992.

Tipos de entornos en los que se produce la detección de anomalías:
\begin{itemize}
	\item AD supervisada:
	\begin{itemize}
		\item Las etiquetas están disponibles, tanto para casos normales como para casos anómalos.
		\item En cierto modo, similar a minería de clases poco comunes o clasificación no balanceada.
	\end{itemize}
	\item AD semi-supervisada (detección de novedades, \textit{Novelty Detection})
	\begin{itemize}
		\item Durante el entrenamiento, solo tenemos datos normales.
		\item El algoritmo aprende únicamente usando los datos normales.
	\end{itemize}
	\item AD no supervisada (detección de \textit{outliers}, \textit{Outlier Detection})
	\begin{itemize}
		\item No hay etiquetas y el conjunto de entrenamiento tiene datos normales y datos anómalos.
		\item Asume que los datos anómalos son poco frecuentes.
		\item Algunos ejemplos típicos de detección de anomalías pueden ser, cuando se quiere detectar intrusos en tráfico de red o bien detectar acciones fraudulentas en transacciones con tarjetas de crédito.
	\end{itemize}
\end{itemize}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_11.png}
  	\caption{Ventajas y desventajas de los algoritmos de detección de anomalías.}
  	\label{fig:proconsanomaly.}
  	\end{center}
\end{figure}

\subsubsection{Reducción de dimensionalidad }

A menudo las muestras disponibles contienen una gran variedad de características, que pueden dar como resultado un sobreajuste del modelo utilizado, por lo tanto es necesario reducir la dimensionalidad de dichos datos pero manteniendo la información relevante. Al reducir la dimensionalidad no solo se evita el sobreajuste, sino que también se obtiene una mejor visualización de los datos y se reduce el costo computacional. 

Uno de los modelos más conocidos y que requieren menor costo computacional es Principal Component Analysis (PCA), pero si lo que estamos buscando es una mejor visualización de los datos y además las características no son lineales, sería recomendable usar T-SNE. 

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{tesis_12.png}
  	\caption{Reducción de dimensionalidad de 3D a 2D.}
  	\label{fig:reduxdimension.}
  	\end{center}
\end{figure}

En la Fig.~\ref{fig:reduxdimension.} \cite{trejoml} se muestra un ejemplo de cómo la reducción de dimensionalidad facilita la visualización de un dataset de alta dimensionalidad en una proyección de 1, 2 o 3 dimensiones.

Enumeramos en la Fig.~\ref{fig:proconsreduxdim.} las ventajas y desventajas de los algoritmos disponibles para esta tarea.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_15.png}
  	\caption{Pro y contras de algoritmos de reducción de dimensionalidad.}
  	\label{fig:proconsreduxdim.}
  	\end{center}
\end{figure}

\subsubsection{\textit{Clustering}}

El \textit{clustering} es una técnica que conceptualmente es simple de comprender, consiste agrupar objetos con características similares. Por lo tanto, obtenemos diferentes grupos llamados clústers, donde en cada uno de ellos están contenidos los datos que son más similares entre ellos que con los que pertenecen a otros clústers, obteniendo de esta forma una útil subdivisión del \textit{dataset}. Como no suele tenerse conocimiento sobre los datos, es decir no están etiquetados, esta técnica pertenece al Aprendizaje no Supervisado.

\clearpage

El algoritmo más simple de \textit{clustering} es K-means, el cual funciona para agrupar datos que se distribuyen en formas esféricas, si se usa la distancia euclídea. y a su vez hay que proporcionar la cantidad k de grupos en los cuales queremos distribuir el \textit{dataset}, por ello se debe tener un conocimiento previo de cuantos clúster se espera tener. 
Otras alternativas pueden ser, realizar \textit{clustering} jerárquico o \textit{clustering} basados en densidades.

En \textit{clustering} jerárquico, vemos como resultado un dendograma, es decir un diagrama de árbol. A partir de esto, se decide un umbral de profundidad, donde se corta el árbol y  de esta forma se obtiene un agrupamientos, por lo tanto a diferencia con K-means, no necesitamos tener información para poder decidir la cantidad de grupos. En la Fig.~\ref{fig:dendogram.} podemos observar como quedan evidenciados a través del dendograma los diferentes clusters.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{tesis_16.png}
  	\caption{Dendograma generado por \textit{clustering} jerárquico.}
  	\label{fig:dendogram.}
  	\end{center}
\end{figure}

En cambio DBSCAN (\textit{Density-based Spatial Clustering of Applications with Noise}), divide el \textit{dataset} buscando las regiones densas de puntos, como podemos observar claramente en Fig.~\ref{fig:DBSCAN.}. Con esta técnica, tampoco especificamos el número de parámetros a priori, sino que se establecen hiperparámetros adicionales, como lo son la cantidad mínima de puntos y un radio $\epsilon$, para lograr un óptimo funcionamiento.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{tesis_17.png}
  	\caption{Clustering basado en densidades.}
  	\label{fig:DBSCAN.}
  	\end{center}
\end{figure}

Enumeramos en la Fig.~\ref{fig:proconsreduxdim.} las ventajas y desventajas de los algoritmos disponibles para esta tarea.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_34.png}
  	\caption{Pros y contras de los algoritmos de \textit{clustering}.}
  	\label{fig:proconsclustering.}
  	\end{center}
\end{figure}

\subsection{Selección de algoritmo en base al \textit{dataset}}
En la Fig.~\ref{fig:maindiagram.} obtenemos una vista general sobre que hacer con nuestros datos en función a nuestro objetivo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{maindiagram.png}
  	\caption{Diagrama general de algoritmos de aprendizaje supervisado y no supervisado.}
  	\label{fig:maindiagram.}
  	\end{center}
\end{figure}

\subsubsection{Algoritmos supervisados}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.9\textwidth]{regressiondiagram.png}
  	\caption{Diagrama general de los algoritmos de regresión.}
  	\label{fig:regressiondiagram.}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.9\textwidth]{classificationdiagram.png}
  	\caption{Diagrama general de los algoritmos de clasificación.}
  	\label{fig:classificationdiagram.}
  	\end{center}
\end{figure}

\subsubsection{Algoritmos no supervisados}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.8\textwidth]{dimreduxdiagram.png}
  	\caption{Diagrama general de los algoritmos de reducción de dimensión.}
  	\label{fig:dimreduxdiagram.}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.9\textwidth]{clusteringdiagram.png}
  	\caption{Diagrama general de los algoritmos de \textit{clustering}.}
  	\label{fig:clusteringdiagram.}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.9\textwidth]{anomalydiagram.png}
  	\caption{Diagrama general de los algoritmos de detección de anomalías.}
  	\label{fig:anomalydiagram.}
  	\end{center}
\end{figure}

\section{Redes neuronales}
La palabra neuronal es la forma adjetiva de "neurona", y red denota una estructura tipo grafo; por lo tanto, una \textit{Red Neuronal Artificial} es un sistema de computación que intenta imitar (o al menos, está inspirado en) las conexiones neuronales en nuestro sistema nervioso. Las redes neuronales artificiales también se denominan \textit{redes neuronales} o \textit{sistemas neuronales artificiales}. Es común abreviar la red neuronal artificial y referirse a ellas como "NN". \citep{rosebrock2017deep}

Para que un sistema se considere un NN, debe contener una estructura de grafo dirigida y etiquetada donde cada nodo del gráfico realice un cálculo simple. Según la teoría de grafos, sabemos que un gráfico dirigido consiste en un conjunto de nodos (es decir, vértices) y un conjunto de conexiones (es decir, bordes) que unen pares de nodos.

\begin{itemize}
\item Las entradas ingresan a la red. 
\item Cada conexión lleva una señal a través de las dos capas ocultas en la red. 
\item Una función final calcula la etiqueta de clase de salida.
\end{itemize}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{018.png}
  	\caption{Arquitectura simple de red neuronal. \cite{matich}}
  	\label{fig:nn}
  	\end{center}
\end{figure}

Cada nodo realiza un cálculo simple. Cada conexión transporta una señal (es decir, la salida del cálculo) de un nodo a otro, marcada por un peso que indica el grado en que la señal se amplifica o disminuye. Algunas conexiones tienen grandes pesos positivos que amplifican la señal, lo que indica que la señal es muy importante al hacer una clasificación. Otros tienen pesos negativos, lo que disminuye la intensidad de la señal, lo que especifica que la salida del nodo es menos importante en la clasificación final. Llamamos a dicho sistema una Red Neuronal Artificial si consta de una estructura de grafo (como en la Figura \ref{fig:nn}) con pesos de conexión que se pueden modificar utilizando un algoritmo de aprendizaje.

\subsection{Relación con la biología}
Nuestros cerebros están compuestos por aproximadamente 10 mil millones de neuronas, cada una conectada a unas 10,000 otras neuronas. El cuerpo celular de la neurona se llama soma, donde las entradas (dendritas) y las salidas (axones) conectan el soma con otro (Figura \ref{fig:realneuron}).

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{019.png}
  	\caption{Estructura de una neurona biológica.}
  	\label{fig:realneuron}
  	\end{center}
\end{figure}


Cada neurona recibe entradas electroquímicas de otras neuronas en sus dendritas. Si estas entradas eléctricas son lo suficientemente potentes como para activar la neurona, entonces la neurona activada transmite la señal a lo largo de su axón, transmitiéndola a las dendritas de otras neuronas. Estas neuronas unidas también pueden activarse, continuando así el proceso de transmitir el mensaje.
La conclusión clave aquí es que el disparo de una neurona es una operación binaria: la neurona se dispara o no se dispara. No hay diferentes "grados" de disparo. En pocas palabras, una neurona solo se disparará si la señal total recibida en el soma excede un umbral dado.
Sin embargo, tenga en cuenta que los ANN simplemente se inspiran en lo que sabemos sobre el cerebro y cómo funciona. El objetivo del aprendizaje profundo no es imitar cómo funcionan nuestros cerebros, sino tomar las piezas que entendemos y permitirnos trazar paralelos similares en nuestro propio trabajo.

\subsection{Modelos artificiales}
Comencemos por ver un NN básico que realiza una suma ponderada simple de las entradas o \textit{inputs} en la Figura \ref{fig:simplenn}. Los valores $x_1$, $x_2$ y $x_3$ son las \textit{inputs} a nuestro NN y generalmente corresponden a una sola fila (es decir, punto de datos) de nuestra matriz de diseño. El valor constante 1 es nuestro sesgo o \textit{bias} que se supone incrustado en la matriz de diseño. Podemos pensar en estas \textit{inputs} como los vectores de características o \textit{features} de entrada a la NN.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{020.png}
  	\caption{Simple NN.}
  	\label{fig:simplenn}
  	\end{center}
\end{figure}

Cada $x$ está conectada a una neurona a través de un vector de peso $W$ que consiste en $w_1, w_2, \ldots w_n$, lo que significa que para cada entrada $x$ también tenemos un peso asociado $w$.
Finalmente, el nodo de salida a la derecha toma la suma ponderada, aplica una función de activación $f$ (utilizada para determinar si la neurona se "dispara" o no) y genera un valor. Expresando la salida matemáticamente, normalmente encontrarás las siguientes tres formas:
\begin{itemize}
\item $f(w_1x_1 + w_2x_2 + \cdots + w_nx_n)$
\item $f(\sum_{i=1}^{n} w_ix_i)$	
\item O $f(net)$, donde $net = \sum_{i=1}^{n} w_ix_i$
\end{itemize}

\subsection{Funciones de activación}
La función de activación más simple es la "función de paso", utilizada por el algoritmo Perceptron.

$
f(net) =
\left\{
	\begin{array}{ll}
		1  	& \mbox{si } net > 0 \\
		0 	& \mbox{si } net \leq 0
	\end{array}
\right.
$

Esta es una función de umbral muy simple, sin embargo, aunque es fácil de usar e intuitiva, no es diferenciable, lo cual puede llevar a problemas cuando apliquemos el descenso por gradiente.
Por ello se presentan en la Figura \ref{fig:typesfactivation} diferentes tipos de funciones de activación con sus respectivos gráficos.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{021.png}
  	\caption{\textbf{Arriba-izquierda}: Función escalón. \textbf{Arriba-derecha}: Función \texttt{sig}oidea. \textbf{Medio-izquierda}: Tangente hiperbólica. \textbf{Medio-derecha}: activación ReLU (función activación más usada en \textit{Deep Learning}).
 \textbf{Abajo-izquierda}: Leaky ReLU, variante de ReLU que permite valores negativos. \textbf{Abajo-derecha}:
ELU, otra variante de ReLU que obtiene mejor performance que Leaky ReLU.}
  	\label{fig:typesfactivation}
  	\end{center}
\end{figure}

Una de las funciones de activación más usadas en la historia de la literatura de NN es la función \texttt{sig}oidea, que sigue la siguiente ecuación:

\begin{equation}
t=\sum_{i=1}^{n}w_ix_i \quad s(t)=\frac{1}{1+e^{-t}}
\end{equation}

La función \texttt{sig}oidea es una mejor opción para el aprendizaje que la función de paso simple, ya que:
\begin{enumerate}
\item Es continua y diferenciable en todas partes.
\item Es simétrica alrededor del eje y.
\item Se acerca asintóticamente a sus valores de saturación.
\end{enumerate}
La principal ventaja aquí es que la suavidad de la función \texttt{sig}oidea hace que sea más fácil diseñar algoritmos de aprendizaje. Sin embargo, hay dos grandes problemas con la función \texttt{sig}oidea:
\begin{enumerate}
\item Las salidas del \texttt{sig}oide no están centradas en cero.
\item Las neuronas saturadas esencialmente eliminan el gradiente, ya que el delta del gradiente será extremadamente pequeño.
\end{enumerate}

La tangente hiperbólica, o tanh (con una forma similar del \texttt{sig}oide) también se usó fuertemente como una función de activación hasta fines de la década de 1990.
La ecuación para tanh sigue:
\begin{equation}
f(z) = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}
La función tanh está centrada en cero, pero los gradientes aún se eliminan cuando las neuronas se saturan.
Ahora sabemos que hay mejores opciones para las funciones de activación que las funciones \texttt{sig}oide y tanh. Específicamente, la Unidad Lineal Rectificada (ReLU), definida como:
\begin{equation}
f(x) = max(0, x)
\end{equation}
Las ReLU también se denominan "funciones de rampa" debido a cómo se ven cuando se trazan. La función es cero para entradas negativas pero luego aumenta linealmente para positivas valores. La función ReLU no es saturable y también es extremadamente  eficiente computacionalmente.
Empíricamente, la función de activación ReLU tiende a superar a las funciones \texttt{sig}oide y tanh en casi todas las aplicaciones. Sin embargo, surge un problema cuando tenemos un valor de cero: no se puede tomar el gradiente.

\subsection{Arquitecturas de redes \textit{feedforward}}

Si bien hay muchas, muchas arquitecturas NN diferentes, la arquitectura más común es la red hacía adelante o \textit{feedforward}, como se presenta en la Figura \ref{fig:nnff}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{022.png}
  	\caption{Un ejemplo de una red neuronal \textit{feedforward}.}
  	\label{fig:nnff}
  	\end{center}
\end{figure}

En este tipo de arquitectura, solo se permite una conexión entre los nodos de los nodos en la capa $i$ a los nodos en la capa $i + 1$ (de ahí el término \textit{feedforward}). No hay conexiones hacia atrás o entre capas permitidas. Cuando las redes de retroalimentación incluyen conexiones de retroalimentación (conexiones de salida que retroalimentan las entradas) se denominan redes neuronales recurrentes.

Para describir una red \textit{feedforward}, normalmente usamos una secuencia de enteros para depositar rápida y concisamente el número de nodos en cada capa. Por ejemplo, la red en la Figura 10.5 anterior es una red de alimentación directa 3-2-3-2:

\begin{itemize}
\item La capa 0 contiene 3 entradas, nuestros valores $x_i$. Estos podrían ser intensidades de píxeles sin procesar de una imagen o un vector de características extraído de la imagen.
\item Las capas 1 y 2 son capas ocultas que contienen 2 y 3 nodos, respectivamente.
\item La capa 3 es la capa de salida o la capa visible: allí es donde obtenemos la clasificación de salida general de nuestra red. La capa de salida generalmente tiene tantos nodos como etiquetas de clase; un nodo para cada salida potencial.
\end{itemize}

\subsection{Redes multicapa}
Las redes multicapas, \textit{i.e.} con varias capas de neuronas pueden ser modeladas matemáticamente como se muestra a continuación.
Supongamos $W$ como la matriz de pesos y el vector $b$ como el vector sesgo o \textit{bias}.
Consideremos:
\begin{equation}
z(x)=Wx+b=\sum_{i=1}^{n}w_ix_i+b
\end{equation}
Además cabe mencionar que la multiplicación punto a punto entre dos matrices de igual dimensión es lo que se conoce como el producto Hadamard.
\begin{equation}
\\texttt{sig}a(z)=\frac{1}{1+e^{-z}}
\end{equation}
Por último definimos la salida de nuestro modelo como:
\begin{equation}
\hat{y}=\\texttt{sig}a(\sum_{i=1}^{n}w_ix_i+b)
\end{equation}

\subsection{Función pérdida}

El objetivo del algoritmo de descenso de gradiente es minimizar la función de costo para que nuestro modelo neuronal pueda aprender.
Pero antes debemos definir que es la función costo o pérdida \cite{sgd}.
En el cálculo, los máximos (o mínimos) de cualquier función pueden ser descubiertos por:
\begin{enumerate}
\item Tomando la derivada de primer orden de la función e igualándola a 0. El punto encontrado de esta manera puede ser el punto de máximo o mínimo.
\item Sustituimos estos valores (el punto que acabamos de encontrar) en la derivada de segundo orden de la función y si el valor es positivo, \textit{i.e.}$>0$, entonces ese punto ($s$) representa el punto ($s$) de mínimos locales o máximos locales.
\end{enumerate}

Necesitamos cerrar la brecha entre la salida del modelo y la salida real. Cuanto menor sea la brecha, mejor será nuestro modelo en sus predicciones y más confianza mostrará al predecir.

La \textbf{función de pérdida o costo} esencialmente modela la diferencia entre la predicción de nuestro modelo y la salida real. Idealmente, si estos dos valores están muy separados, el valor de pérdida o el valor de error deberían ser mayores. Del mismo modo, si estos dos valores están más cerca, el valor del error debería ser bajo.
Una posible función de pérdida podría ser:
\begin{equation}
J(\Theta)=\hat{y}-y / y\in\{0,1\}
\end{equation}
Pero, en lugar de tomar esta función como nuestra función de pérdida, terminamos considerando la siguiente función:
\begin{equation}
J(\Theta)=\frac{\|\hat{y}-y\|^2}{2}
\end{equation}
Esta función se conoce como error al cuadrado . Simplemente tomamos la diferencia entre la salida real $y$ y la salida predicha $\hat{y}$ elevamos al cuadrado ese valor (de ahí el nombre) y lo dividimos entre 2.

Una de las principales razones para preferir el error al cuadrado en lugar del error absoluto es que el error al cuadrado es diferenciable en todas partes , mientras que el error absoluto no lo es (su derivada no está definida en 0).

Además, los beneficios de la cuadratura incluyen:
\begin{itemize}


\item La cuadratura siempre da un valor positivo , por lo que la suma no será cero.
\item Hablamos de suma aquí porque sumaremos los valores de pérdida o error para cada imagen en nuestro conjunto de datos de entrenamiento y luego haremos un promedio para encontrar la pérdida para todo el lote de ejemplos de entrenamiento.
\item La cuadratura enfatiza las diferencias más grandes, una característica que resulta ser buena y mala.

\end{itemize}

La función de error que usaremos aquí se conoce como el error cuadrático medio y la fórmula es la siguiente:

\begin{equation}
J(\Theta)=\frac{1}{2m} \sum_{i=1}^{m} (\hat{y_i}-y_i)^2
\label{funcperdida}
\end{equation}

Calculamos el error al cuadrado para cada \textit{feature} en nuestro \textit{dataset} y luego encontramos el promedio de estos valores y esto representa el error general del modelo en nuestro conjunto de entrenamiento.

Consideremos el ejemplo de una sola \textit{feature} con solo 2 características de antes. Dos características significan que tenemos 2 valores de peso correspondientes y un valor de sesgo. En total, tenemos 3 parámetros para nuestro modelo.

\begin{equation}
\hat{y} = w_1x_1 + w_2x_2 + b
\end{equation}

\begin{equation}
J(\Theta)=\frac{1}{2m} \sum_{i=1}^{m} (w_1x_1^{(i)}+w_2x_2^{(i)}+b-y^{(i)})^2
\end{equation}

Queremos encontrar valores para nuestros pesos y el sesgo que minimiza el valor de nuestra función de pérdida. Dado que esta es una ecuación de múltiples variables, eso significa que tendríamos que tratar con derivadas parciales de la función de pérdida correspondiente a cada una de nuestras variables $w_1$, $w_2$ y $b$.

\begin{equation}
\frac{\partial J}{\partial w_1} \frac{\partial J}{\partial w_2}
\frac{\partial J}{\partial b}
\end{equation}

Esto puede parecer lo suficientemente simple porque solo tenemos 3 variables diferentes.
Sin embargo tenemos tantos pesos como features \textit{i.e.} $w_n$ pesos.

Hacer una optimización multivariante con tantas variables es computacionalmente ineficiente y no es manejable. Por lo tanto, recurrimos a alternativas y aproximaciones.

\subsection{Descenso de gradiente}

Es la capacidad de aprendizaje que otorga el algoritmo de descenso de gradiente lo que hace que el aprendizaje automático y los modelos de aprendizaje profundo funcionen.

El objetivo de este algoritmo es minimizar el valor de nuestra función de pérdida y queremos hacer esto de manera eficiente.
Como se discutió anteriormente, la forma más rápida sería encontrar derivadas de segundo orden de la función de pérdida con respecto a los parámetros del modelo. Pero, eso es computacionalmente costoso.

La intuición básica detrás del descenso del gradiente puede ilustrarse mediante un escenario hipotético \citep{gdanalogy}: una persona está atrapada en las montañas y está tratando de bajar (es decir, tratando de encontrar los mínimos). Hay mucha niebla de tal manera que la visibilidad es extremadamente baja. Por lo tanto, el camino hacia abajo de la montaña no es visible, por lo que deben usar la información local para encontrar los mínimos.

Pueden usar el método de descenso en gradiente, que consiste en mirar la inclinación de la colina en su posición actual, luego proceder en la dirección con el descenso más empinado (es decir, cuesta abajo).
Si trataban de encontrar la cima de la montaña (es decir, los máximos), entonces avanzarían en la dirección con el ascenso más empinado (es decir, cuesta arriba). Usando este método, eventualmente encontrarían su camino.

Sin embargo, suponga también que la pendiente de la colina no es inmediatamente obvia con una simple observación, sino que requiere un instrumento sofisticado para medir, que la persona tiene en ese momento.

Se necesita bastante tiempo para medir la inclinación de la colina con el instrumento, por lo tanto, deben minimizar el uso del instrumento si quieren bajar la montaña antes del atardecer.
La dificultad es elegir la frecuencia con la que deben medir la inclinación de la colina para no desviarse.

En esta analogía:
\begin{itemize}

\item La persona representa nuestro \textbf{algoritmo de aprendizaje}, y
el camino que baja por la montaña representa la \textbf{secuencia de actualizaciones de parámetros} que nuestro modelo eventualmente explorará.
\item La inclinación de la colina representa la \textbf{pendiente de la superficie de error en ese punto}.
\item El instrumento utilizado para medir la inclinación es la \textbf{diferenciación} (la pendiente de la superficie de error se puede calcular tomando la derivada de la función de error al cuadrado en ese punto). Esta es la aproximación que hacemos cuando aplicamos el descenso de gradiente. Realmente no sabemos el punto mínimo, pero sí sabemos la dirección que nos llevará a los mínimos (locales o globales) y damos un paso en esa dirección.
\item La dirección en la que la persona elige viajar se alinea con el gradiente de la superficie de error en ese punto.
\item La cantidad de tiempo que viajan antes de tomar otra medida es la \textbf{velocidad de aprendizaje del algoritmo}. Esto es esencialmente lo importante que nuestro modelo (o la persona que va cuesta abajo) decide dar cada vez.

\end{itemize}

Entonces el descenso del gradiente mide el gradiente local de la función de pérdida (costo) para un conjunto dado de parámetros ($\Theta$) y da pasos en la dirección del gradiente descendente. Como ilustra la Figura \ref{fig:gd}, una vez que el gradiente es cero, hemos alcanzado un mínimo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{023.png}
  	\caption{Representación gráfica del descenso de gradiente.}
  	\label{fig:gd}
  	\end{center}
\end{figure}

Como vemos en la Fig~\ref{fig:lr}.Es importante ajustar apropiadamente el valor de la tasa de aprendizaje \textit{(learning rate)}. Si es demasiado pequeña, entonces el algoritmo tomará muchas iteraciones (pasos) para encontrar el mínimo. Por otro lado, si es muy alta, es posible que supere el mínimo y termine más lejos que cuando comenzó.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_37.png}
  	\caption{Ajuste de la tasa de aprendizaje.}
  	\label{fig:lr}
  	\end{center}
\end{figure}

Por tanto para actualizar la matriz de pesos y de sesgo serán utilizadas las siguientes ecuaciones.

\begin{equation}
W' = W - \alpha \frac{\partial J}{\partial W}
\end{equation}

\begin{equation}
b' = b - \alpha \frac{\partial J}{\partial b}
\end{equation}

El $\alpha$ representa la tasa de aprendizaje o  \textit{learning rate}.

\subsection{Backpropagation}

Ya sabemos cómo fluyen las activaciones en la dirección hacia adelante. Tomamos las \textit{features} de entrada, las transformamos linealmente, aplicamos la activación \texttt{sig}oidea en el valor resultante y finalmente tenemos nuestra activación que luego usamos para hacer una predicción \citep{sgd}.

Lo que veremos en esta sección es el flujo de gradientes a lo largo de la línea roja en la Figura \ref{fig:back} mediante un proceso conocido como retropropagación o \textit{backpropagation}, que es esencialmente la regla de la cadena de cálculo aplicada a los gráficos computacionales.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{024.png}
  	\caption{Las activaciones se propagan hacía adelante, pero los gradientes fluyen hacía atrás.}
  	\label{fig:back}
  	\end{center}
\end{figure}

Digamos que queríamos encontrar la derivada parcial de la variable $y$ con respecto a $x$ de la Figura \ref{fig:func}. No podemos descubrirlo directamente porque hay otras 3 variables involucradas en el gráfico computacional. Entonces, hacemos este proceso iterativamente yendo hacia atrás en el gráfico de cálculo.

Primero descubrimos la derivada parcial de la salida $y$ con respecto a la variable $C$. Luego usamos la regla de la cadena de cálculo y determinamos la derivada parcial con respecto a la variable $B$ y así sucesivamente hasta que obtengamos la derivada parcial que estamos buscando.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{025.png}
  	\caption{Representación de grafo simple.}
  	\label{fig:func}
  	\end{center}
\end{figure}

Utilizando la función pérdida definida en la ecuación \ref{funcperdida} y reescribiéndola en su forma vectorial.

\begin{equation}
J(\Theta)=\frac{1}{2} \|\hat{Y}-Y\|^2
\end{equation}

La derivada parcial de la función de pérdida con respecto a la activación de nuestro modelo es:

\begin{equation}
\frac{\partial J}{\partial \hat{Y}}
=\frac{1}{2} \frac{\partial J}{\partial \hat{Y}} \|\hat{Y}-Y\|^2 = \frac{1}{2} 2 \hat{Y} - Y \frac{\partial}{\partial \hat{Y}} (\hat{Y}-Y) = (\hat{Y}-Y) \frac{\partial}{\partial \hat{Y}} \|\hat{Y}-Y\| = (\hat{Y} - Y)
\label{dj/dy}
\end{equation}

Avancemos un paso hacia atrás y calculemos nuestra próxima derivada parcial. Esto nos llevará un paso más cerca de los gradientes reales que queremos calcular.

Este es el punto donde aplicamos la regla de la cadena que mencionamos antes. Entonces, para calcular la derivada parcial de la función de pérdida con respecto a la salida transformada lineal, es decir, la salida de nuestro modelo antes de aplicar la activación \texttt{sig}oidea:

\begin{equation}
\frac{\partial J}{\partial D} = \frac{\partial J}{\partial \hat{Y}} \frac{\partial \hat{Y}}{\partial A}
\label{dJdD}
\end{equation}

La primera parte de esta ecuación es el valor que habíamos calculado en la ecuación \ref{dj/dy}. Lo esencial para calcular aquí es la derivada parcial de la predicción de nuestro modelo con respecto a la salida transformada linealmente.

Veamos la ecuación para la predicción de nuestro modelo, la función de activación \texttt{sig}oidea.

\begin{equation}
\hat{Y} = \\texttt{sig}a(A) = \frac{1}{1+e^{-A}}
\end{equation}

Derivada de la salida final de nuestro modelo, \textit{i.e.} significa la derivada parcial de la función \texttt{sig}oide con respecto a su entrada.

\begin{equation}
\frac{\partial}{\partial A} \\texttt{sig}a (A) = \frac{\partial}{\partial A} \frac{1}{1+e^{-A}} = \frac{\partial}{\partial A} (1+e^{-A})^{-1}
\end{equation}

\begin{equation}
 -1  (1+e^{-A})^{-2} \frac{\partial}{\partial A} (1+e^{-A}) = -1  (1+e^{-A})^{-2} (-e^{-A}) = \frac{e^{-A}}{(1+e^{-A})^2}
\end{equation}

Continuando, podemos simplificar aún más esta ecuación.

\begin{equation}
\\texttt{sig}a(A) = \frac{1}{1+e^{-A}}
\end{equation}

\begin{equation}
e^{-A} = \frac{1}{\\texttt{sig}a (A)} - 1 = \frac{1-\\texttt{sig}a (A)}{\\texttt{sig}a(A)}
\end{equation}

Substituyendo este valor en la ecuación \ref{dJdD} obtenemos:

\begin{equation}
\frac{\partial J}{\partial A} = \frac{\partial J}{\partial \hat{Y}} \frac{e^{-A}}{(1+e^{-A})^2}
\end{equation}

\begin{equation}
\frac{\partial J}{\partial A} = \frac{\partial J} {\partial \hat{Y} } \frac{1-\\texttt{sig}a(A)}{\\texttt{sig}a(A)} \\texttt{sig}a (A) \\texttt{sig}a (A)
\end{equation}

\begin{equation}
\frac{\partial J}{\partial A} = \frac{\partial J} {\partial \hat{Y}} \\texttt{sig}a(A) (1-\\texttt{sig}a(A))
\end{equation}

Necesitamos la derivada parcial de la función de pérdida correspondiente a cada uno de los pesos. Pero como estamos recurriendo a la vectorización, podemos encontrarlo todo de una vez. Es por eso que hemos estado usando la notación mayúscula $W$ en vez de $w_1, w_2, \ldots, w_n$.

\begin{equation}
\frac{\partial J}{\partial W} = \frac{\partial J}{\partial A} \frac{\partial A}{\partial W}
\label{dJ/dW}
\end{equation}

\begin{equation}
\frac{\partial J}{\partial b} = \frac{\partial J}{\partial A} \frac{\partial A}{\partial b}
\label{dJ/db}
\end{equation}

La derivación de los pesos queda partiendo de la ecuación \ref{dJ/dW}:

\begin{equation}
\frac{\partial J}{\partial W} = \frac{\partial J}{\partial A} \frac{\partial}{\partial W}(W^T X + b) = \frac{\partial J}{\partial A} X
\end{equation}

Y de la ecuación \ref{dJ/db}:

\begin{equation}
\frac{\partial J}{\partial b} = \frac{\partial J}{\partial A} \frac{\partial}{\partial b}((W^T X + b) = \frac{\partial J}{\partial A} 1 = \frac{\partial J}{\partial A}
\end{equation}

Se demostró desde un punto de vista matemático el concepto de \textit{backpropagation} como se realiza la actualización de los pesos y sesgos utilizando el descenso por gradiente.

\subsection{Descenso de gradiente estocástico (SGD)}
Sin embargo el descenso de gradiente puede ser excepcionalmente lento en datasets muy grandes debido a que en cada iteración requiere calcular una predicción por cada punto de entrenamiento en nuestros datos de entrenamientos antes que actualizaciones nuestra matriz de pesos.

En cambio lo que se utiliza es una variante de éste, el descenso de gradiente estocástico o \textit{Stochastic Gradient Descent (SGD)}.
El SGD es una simple modificación del algoritmo de descenso de gradiente estándar que computa el gradiente y actualiza la matriz de pesos $W$ en pequeños lotes o \textit{batches} de datos de entrenamiento, en vez del \textit{dataset} entero. Mientras esta modificación nos lleva a actualizaciones más " ruidosas", también nos permite tomar más pasos a lo largo del gradiente, llevando en ultima instancia a una convergencia más rápida y sin afectar negativamente a la pérdida y precisión del modelo.

En lugar de calcular nuestro gradiente en todo el conjunto de datos, en su lugar muestreamos nuestros datos, produciendo un lote. Evaluamos el gradiente en el lote y actualizamos nuestra matriz de peso W. Desde una perspectiva de implementación, también tratamos de aleatorizar nuestras muestras de entrenamiento antes de aplicar SGD ya que el algoritmo es sensible a los lotes.

En una implementación "purista" de SGD, el tamaño de su mini lote sería 1, lo que implica que muestrearíamos aleatoriamente un punto de datos del conjunto de entrenamiento, calcularíamos el gradiente
y actualizamos nuestros parámetros.

Sin embargo, a menudo utilizamos mini lotes que son mayores a 1. Los tamaños de lote típicos incluyen 32, 64, 128 y 256.

A continuación enumeramos las justificaciones a esta decisión.

\begin{enumerate}
\item Ayudan a reducir la variación en la actualización de parámetros, lo que conduce a una convergencia más estable. 
\item Las potencias de dos a menudo son deseables para los tamaños de lote, ya que permiten que las bibliotecas de optimización de álgebra lineal interna sean más eficientes.
\end{enumerate}
En general, el tamaño del mini lote no es un hiperparámetro por el que debería preocuparse demasiado. Si está usando una GPU para entrenar su red neuronal, usted determina cuántos ejemplos de entrenamiento encajarán en su GPU y luego usa la potencia más cercana de dos, ya que el tamaño del lote se ajustará en la GPU. Para el entrenamiento de CPU, normalmente utiliza uno de los tamaños de lote enumerados anteriormente para asegurarse de cosechar los beneficios de las bibliotecas de optimización de álgebra lineal.

\subsection{Sobreajuste y bajo-ajuste}
El sobreajuste o \textit{overfitting} y la falta de ajuste o \textit{underfitting} \cite{quora} es muy importante para saber si el modelo predictivo está generalizando bien los datos o no. Un buen modelo debe poder generalizar bien los datos.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.8\textwidth]{026.png}
  	\caption{Distintas representaciones del ajuste en un mismo modelo.}
  	\label{fig:fitting}
  	\end{center}
\end{figure}

En la Figura \ref{fig:fitting} \textbf{Izquierda} el modelo está sobreajustado, \textit{i.e.} cuando funciona bien en el ejemplo de entrenamiento pero no funciona bien en datos no vistos. A menudo es el resultado de un modelo excesivamente complejo y ocurre porque el modelo está memorizando la relación entre el ejemplo de entrada (a menudo llamado $X$) y la variable objetivo (a menudo llamada $y$) o, por lo tanto, no puede generalizar bien los datos. El modelo de sobreajuste predice el objetivo en el conjunto de datos de entrenamiento con mucha precisión.

En cambio en la Figura \ref{fig:fitting} \textbf{Derecha} se dice que el modelo predictivo tiene bajo-ajuste, si funciona mal en los datos de entrenamiento. Esto sucede porque el modelo no puede capturar la relación entre el ejemplo de entrada y la variable objetivo. Podría deberse a que el modelo es demasiado simple, es decir, las características de entrada no son lo suficientemente expresivas como para describir bien la variable objetivo. El modelo con bajo-ajuste no predice los objetivos en los conjuntos de datos de entrenamiento con mucha precisión.

Un buen modelo debe ser como el de la Figura \ref{fig:fitting} \textbf{Medio} que posee una buena precisión en su conjunto de datos de entrenamiento pero a su vez también tiene una buena \textit{performance} con datos que no haya visto.

\subsection{Regularización}
Para disminuir los efectos del sobreajuste se utiliza la \textbf{regularización} que después de la tasa de aprendizaje, es el parámetro más importante de su modelo que puede ajustar.

Existen varios tipos de técnicas de regularización, como la regularización L1, la regularización L2 (comúnmente llamada “pérdida de peso”) y Elastic Net, que se utilizan al actualizar la función de pérdida en sí, agregando un parámetro adicional para restringir la capacidad de el modelo.

La regularización nos ayuda a controlar la capacidad de nuestro modelo, asegurando que nuestros modelos sean mejores para hacer clasificaciones (correctas) en los puntos de datos en los que no fueron entrenados, lo que llamamos la capacidad de generalizar. Si no aplicamos la regularización, nuestros clasificadores pueden volverse demasiado complejos y ajustarse fácilmente a nuestros datos de entrenamiento, en cuyo caso perdemos la capacidad de generalizar a nuestros datos de prueba.

\subsection{Los cuatro ingredientes de una red neuronal}
Hay cuatro ingredientes principales \citep{rosebrock2017deep} que necesita para armar su propia red neuronal y algoritmo de aprendizaje profundo: un conjunto de datos, un modelo/arquitectura, una función de pérdida y una optimización.
\subsubsection{Conjunto de datos}
También llamado \textit{dataset}, es el primer ingrediente en el entrenamiento de una red neuronal: los datos en sí mismos junto con el problema que estamos tratando de resolver definen nuestros objetivos finales.

La combinación de su conjunto de datos y el problema que está tratando de resolver influye en su elección en la función de pérdida, la arquitectura de red y el método de optimización utilizado para entrenar el modelo. Por lo general, tenemos pocas opciones en nuestro conjunto de datos (a menos que esté trabajando en un proyecto de pasatiempo): se nos da un conjunto de datos con cierta expectativa sobre cuáles deberían ser los resultados de nuestro proyecto. Depende de nosotros entrenar un modelo de aprendizaje automático en el conjunto de datos para que funcione bien en la tarea dada.

\subsubsection{Función de pérdida}
Dado nuestro conjunto de datos y objetivo objetivo, necesitamos definir una función de pérdida que se alinee con el problema que estamos tratando de resolver.

\subsubsection{Modelo/Arquitectura}
La arquitectura de su red puede considerarse la primera "elección" real que tiene que hacer como ingrediente. Es probable que su conjunto de datos sea elegido para usted (o al menos ha decidido que desea trabajar con un conjunto de datos determinado). Y si está realizando una clasificación, probablemente utilizará la entropía cruzada como su función de pérdida.
Sin embargo, su arquitectura de red puede variar dramáticamente, especialmente cuando con qué método de optimización elige entrenar su red.

\subsubsection{Método de optimización}
El ingrediente final es definir un método de optimización. El SGD se usa con bastante frecuencia.
SGD sigue siendo el caballo de batalla del aprendizaje profundo: la mayoría de las redes neuronales se entrenan a través de SGD, aunque existen otros métodos de optimización como Adam.
Luego debe establecer una tasa de aprendizaje adecuada, la fuerza de regularización y el número total de épocas para las que se debe entrenar la red.

\section{Redes neuronales convolucionales}
Las redes neuronales convolucionales \citep{rosebrock2017deep} (\textit{CNNs} en Inglés) son principalmente útiles si en la entrada los datos presentados son imágenes, permite el desarrollo de modelos supervisados y no supervisados.

Podemos definir una \textit{CNN} como una red neuronal que cambia una capa totalmente conectada (\textit{fully-connected}) por una convolucional para al menos una de las capas de la red.

Cada capa en una \textit{CNN} aplica un conjunto de filtros, usualmente cientos o miles de ellos y combinan los resultados, alimentando la entrada de la siguiente capa de la red. Durante el entrenamiento, una \textit{CNN} automáticamente aprende los valores para esos filtros.

En el contexto de la clasificación de imágenes, una \textit{CNN} puede aprender a:
\begin{itemize}
	\item Detectar bordes a partir de datos de píxeles sin procesar en la primera capa.
	\item Usar esos bordes para detectar formas (\textit{i.e. blobs}) en la segunda capa.
	\item Usar esas formas para detectar características de alto nivel tales como estructuras faciales, partes de un auto, etc. en las capas de más alto nivel.
\end{itemize}

La última capa en una \textit{CNN} usa esas características de alto nivel para realizar predicciones considerando los contenidos de una imagen.

Las \textit{CNNs} nos dan dos beneficios claves con respecto al reconocimiento de imágenes:
\begin{itemize}
	\item \textbf{invariancia local}: nos permite clasificar una imagen que contiene un objeto particular sin importar donde aparece éste en la imagen.
	\item \textbf{composicionalidad}: cada filtro compone un parche local de características de nivel inferior en una representación de nivel superior, similar a cómo podemos componer un conjunto de funciones matemáticas que se basan en la salida de funciones anteriores. Esta composición permite que nuestra red aprenda características más ricas de forma más profunda.

\end{itemize}

Las convoluciones bi-dimensionales (2D) son usadas para tratar las imágenes, mientras que las convoluciones unidimensionales (1D) nos permiten analizar entradas secuenciales, obteniendo la información con dependencias temporales. Entonces al combinar estas dos técnicas, se puede apreciar cómo evolucionan en el tiempo las imágenes capturadas y así hacer predicciones a futuro.

\subsection{Convolución 1D}
Si $f$ y $g$ son funciones discretas \citep{keller}, entonces $f * g$ es la convolución de $f$ y $g$ y está definida como:

$$(f*g)(x)=\sum_{u=-\infty}^{\infty} f(u)g(x-u)$$

Intuitivamente, la convolución de dos funciones representa la cantidad de superposición entre estas. La función $g$ es la entrada y $f$ el \textit{kernel} o núcleo de la convolución.

Sin embargo en los algoritmos de \textit{machine learning} lo que manejamos usualmente son vectores o arreglos de tal forma que nos resultará más provechoso analizar la convolución entre ellos.

Si la función $f$ varía sobre un conjunto finito de valores $a = a_1, a_2, \dots, a_n$ entonces puede ser representado como el vector 
$\begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}$.

Si las funciones $f$ y $g$ son representadas como vectores $a = \begin{bmatrix} a_1 & a_2 & \cdots & a_m \end{bmatrix}$ y $b = \begin{bmatrix} b_1 & b_2 & \cdots & b_n \end{bmatrix}$, entonces $f*g$ es un vector $c = \begin{bmatrix} c_1 & c_2 & \cdots & c_{m+n-1} \end{bmatrix}$ definido de la siguiente forma:

$$ c = \sum_{u} a_u b_{x-u+1}$$

donde $u$ abarca todos los subíndices legales para $a_u$ y $b_{x-u+1}$, específicamente $u=max(1, x-n+1)\dots min(x,m).$

Lo que puede parecer complicado en la teoría no lo es en la práctica, observemos la Fig.~\ref{fig:conv1dk1} \citep{Cogneethi2019Aug}. El vector \textit{input} también se denomina vector de características y el vector \textit{output} mapa de características.

Lo que sucede es que si el \textit{kernel} tiene un único valor sólo es necesario multiplicarlo por cada valor del vector \textit{input} y guardarlo en el índice correspondiente del vector \textit{output}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{tesis_24.png}
  	\caption{Vectores de convolución unidimensional con kernel simple.}
  	\label{fig:conv1dk1}
  	\end{center}
\end{figure}

En cambio si tenemos un \textit{kernel} de dimensiones $2 \times 1$ como la Fig.\ref{fig:conv1dk2} para obtener el valor de salida $i$ debemos usar los valores de entrada $i$ y su vecino $i+1$.

Para obtener el primer valor del vector de salida se realizó la operación \texttt{o[0] = i[0]k[0] + i[1]k[1] = 69}. De esta forma iteramos a lo largo de todo el vector de entrada hasta obtener todos los valores de salida. Podemos notar que el tamaño del vector de salida es menor ahora, a medida que aumentamos el tamaño del kernel disminuye el del vector de salida.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{tesis_25.png}
  	\caption{Vectores de convolución unidimensional con kernel doble.}
  	\label{fig:conv1dk2}
  	\end{center}
\end{figure}

Con objeto de dejar totalmente en claro el algoritmo observemos la Fig.~\ref{fig:conv1dk3}. Para obtener el valor del índice 4 del vector de salida operamos \texttt{o[4] = i[3]k[0]+i[4]k[1]+i[5]k[2] = 23}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.75\textwidth]{tesis_26.png}
  	\caption{Vectores de convolución unidimensional con kernel triple.}
  	\label{fig:conv1dk3}
  	\end{center}
\end{figure}

Para finalizar se hará una pequeña mención al tamaño del vector de salida, que viene determinado por la siguiente formula \citep{SOOutputConv}:

$$ output_{size} = \frac{W-F+2P}{S+1}$$

donde $W = input_{size}$, $F=kernel_{size}$, $P$=\textit{padding} y $S$=\textit{stride}.

\subsection{Convolución 2D}

A su vez podemos extender esto a convoluciones para funciones de dos variables.

Si $f$ y $g$ son funciones discretas de dos variables, entonces $f*g$ es la convolución de $f$ y $g$ y se define:

$$
(f*g)(x,y) = \sum_{u=-\infty}^{+\infty} \sum_{v=-\infty}^{+\infty} f(u,v)g(x-u,y-v)
$$

Podemos considerar funciones de dos variables como matrices con $A_{xy} = f(x,y)$ y obtener una definición matricial de la convolución.

Si las funciones $f$ y $g$ son representadas como las matrices $A$ y $B$ con dimensiones de $n \times m$ y $k \times i$ respectivamente, entonces $f*g$ es una matriz $C$ de dimensiones $(n+k-1) \times (m+i-1)$ definida:

$$
c_{xy} = \sum_{u} \sum_{v} a_{uv} b_{x-u+1,y-v+1}
$$

donde $u$ y $v$ abarcan todos los subíndices posibles para $a_{uv}$ y $b_{x-u+1,y-v+1}$.

Así como notamos que el algoritmo para la convolución 1D no era tan complejo como su definición formal, lo mismo sucede para la convolución 2D pero extrapolando el mecanismo a una dimensión más.

En la Fig.~\ref{fig:conv2d} \citep{andrianaivo2019architecture} analizamos el procedimiento. Se debe centrar el kernel $K$ sobre el primer valor a calcular, para luego realizar las respectivas multiplicaciones y luego guardarlas en la matriz de salida $O$, de esta manera iremos iterando de derecha a izquierda y de arriba hacia abajo toda la matriz $I$.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.4\textwidth]{convolution2d.jpg}
  	\caption{Vectores de convolución bidimensional con kernel $3 \times 3$.}
  	\label{fig:conv2d}
  	\end{center}
\end{figure}

Consideremos la Fig.~\ref{fig:conv2dimg} \citep{Saha2020Oct}, tenemos una imagen RGB que ha sido separada por sus tres canales de color: rojo, verde y azul. Hay varios espacios de color en los que existen las imágenes: escala de grises, RGB, HSV, CMYK, etc.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{imgrgb.png}
  	\caption{Imagen RGB $4 \times 4 \times 3$.}
  	\label{fig:conv2dimg}
  	\end{center}
\end{figure}

Si consideramos la totalidad de la imagen como un prisma donde la profundidad corresponde a cada canal de color, podemos ver en la Figura~\ref{fig:kernelmove} el movimiento que realiza el kernel (con forma de cubo) a través del volumen del prisma.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.3\textwidth]{kernelmove.png}
  	\caption{Movimiento del \textit{kernel}.}
  	\label{fig:kernelmove}
  	\end{center}
\end{figure}

\subsubsection{\textit{Padding}}

Un problema a abordar al aplicar la convolución es que tendemos a perder píxeles en el perímetro de nuestra imagen (o vector). Dado que normalmente usamos núcleos pequeños, para cualquier convolución dada, es posible que solo perdamos unos pocos píxeles, pero esto puede sumarse a medida que aplicamos muchas capas convolucionales sucesivas. Una solución sencilla a este problema es agregar píxeles adicionales de relleno (\textit{padding}) alrededor del límite de nuestra imagen de entrada, aumentando así el tamaño efectivo de la imagen. Normalmente, establecemos los valores de los píxeles adicionales en cero. \citep{padding}

En la figura~\ref{fig:padding}, rellenamos una entrada de $3 \times 3$, aumentando su tamaño a $5 \times 5$. La salida correspondiente aumenta entonces a una matriz de $4 \times 4$.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{conv-pad.png}
  	\caption{Relleno o \textit{padding}.}
  	\label{fig:padding}
  	\end{center}
\end{figure}

\subsubsection{\textit{Stride}}

Al realizar la convolución, comenzamos con la ventana en la esquina superior izquierda del tensor de entrada y luego la deslizamos sobre todas las ubicaciones, tanto hacia abajo como hacia la derecha. Usualmente deslizamos un elemento a la vez. Sin embargo, a veces, ya sea por eficiencia computacional o porque deseamos reducir la resolución, movemos nuestra ventana más de un elemento a la vez, omitiendo las ubicaciones intermedias.

Nos referimos al número de filas y columnas atravesadas por diapositiva como la zancada o \textit{stride}. Hasta ahora, hemos utilizado \textit{stride} de 1, tanto para altura como para ancho. A veces, es posible que deseemos usar un paso más grande, como en la figura~\ref{fig:stride} \citep{Saha2020Oct}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{tesis_44.png}
  	\caption{Convolución con \textit{stride} de 2.}
  	\label{fig:stride}
  	\end{center}
\end{figure}

\subsection{Tipos de capas}
Existen varias tipos de capas usadas \citep{rosebrock2017deep} para construir \textit{CNNs} pero las más comunes incluyen:

\begin{itemize}
	\item Convolucional (\texttt{CONV})
	\item Activación (\texttt{ACT})
	\item \textit{Pooling} (\texttt{POOL})
	\item \textit{Fully-connected} (\texttt{FC})
	\item \texttt{\textit{Dropout}} (\texttt{DO})
\end{itemize}

Apilando estas capas de una manera específica producimos una \textit{CNN}. De estos tipos de capas, \texttt{CONV} y \texttt{FC} (y en menor medida, \texttt{BN}) son las únicas capas que contienen parámetros que se aprenden durante el proceso de entrenamiento.

Las capas \texttt{ACT} y \texttt{DO} no se consideran verdaderas capas en sí mismas, pero a menudo se incluyen en los diagramas de red para que la arquitectura sea explícitamente clara.

Las capas (\texttt{POOL}), de igual importancia que \texttt{CONV} y \texttt{FC}, también se incluyen en los diagramas de red, ya que tienen un impacto sustancial en las dimensiones espaciales de un
imagen mientras se mueve a través de una \textit{CNN}.

\subsubsection{Convolución}

La capa convolucional (\texttt{CONV}) es el componente básico de una red neuronal convolucional. Los parámetros de la capa \texttt{CONV} consisten en un conjunto de $K$ \textit{kernels} entrenables, donde cada uno tiene un ancho y un alto, y casi siempre son cuadrados. 

Una capa o filtro puede tener varios \textit{kernels} que al convolucionar con el vector de entrada (que podría ser una imagen) producen mapas de características (\textit{features map}).

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.45\textwidth]{layer-kernels.png}
  	\caption{Una capa está compuesta de una colección de \textit{kernels}.}
  	\label{fig:layer-kernel}
  	\end{center}
\end{figure}

La diferencia entre filtro y \textit{kernel} es un poco complicada. A veces, se usan indistintamente, lo que podría crear confusiones. Esencialmente, estos dos términos tienen una sutil diferencia. Un "\textit{kernel}" se refiere a una matriz 2D de pesos. El término "filtro" se refiere a estructuras 3D de varios \textit{kernels} apilados juntos. Para un filtro 2D, el filtro es igual que el kernel. Pero para un filtro 3D y la mayoría de las convoluciones en el aprendizaje profundo, un filtro es una colección de \textit{kernels} (fig.~\ref{fig:layer-kernel}). Cada \textit{kernel} es único, enfatizando diferentes aspectos del canal de entrada.

El funcionamiento de la capa convolucional se resume en pensar en cada uno de los $K$ \textit{kernels} deslizándose a través del vector de entrada, computando un producto de Hadamard, sumando cada uno de sus valores y luego almacenando el valor generado en un mapa 2D de activación. En la figura~\ref{fig:conv-mechanism} se puede observar una visualización de la secuencia.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{feature-map-clean.png}
  	\caption{\textbf{Izquierda:} en cada capa convolucional de una \textit{CNN}, hay $K$ \textit{kernels} distintos. \textbf{Medio:} Cada uno de los \textit{kernels} es convolucionado con el vector de entrada. \textbf{Derecha:} Cada \textit{kernel} produce una salida 2D, llamada mapa de activación o \textit{features}.}
  	\label{fig:conv-mechanism}
  	\end{center}
\end{figure}

Luego de aplicar los $K$ filtros al vector de entrada, ahora tenemos $K \times 2D$ mapas de activación. Luego apilamos nuestro $K$ mapas de activación a través de la dimensión de profundidad de nuestra matriz para formar el volumen final de salida (figura~\ref{fig:k-maps}).

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.8\textwidth]{k-activation-map.png}
  	\caption{Luego de obtener los $K$ mapas de activación, son apilados juntos para formar el volumen de entrada a la siguiente capa en la red.}
  	\label{fig:k-maps}
  	\end{center}
\end{figure}

Cada entrada en el volumen de salida es, por tanto, una salida de una neurona que "mira" sólo una pequeña región de la entrada. De esta manera, la red "aprende" los filtros que se activan cuando ven un tipo específico de característica en una ubicación espacial determinada en el volumen de entrada.

En las capas inferiores de la red, los filtros pueden activarse cuando ven regiones con forma de borde o de esquina. Luego, en las capas más profundas de la red, los filtros pueden activarse en presencia de características de alto nivel, como partes de la cara, la pata de un perro, el capó de un automóvil, etc.

\subsubsection{Activación}

Luego de cada capa \texttt{CONV} en una \textit{CNN}, normalmente aplicamos una función no lineal, como ReLU, ELU, etc (se ejemplifica en la figura~\ref{fig:relu-act}). Las capas \texttt{ACT} no son técnicamente "capas" (debido al hecho de que no se aprenden parámetros/pesos dentro de una capa de activación) y, a veces, se omiten en los diagramas de arquitectura de red, ya que se supone que una activación sigue inmediatamente a una convolución.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.65\textwidth]{tesis_47.png}
  	\caption{Un ejemplo de un volumen de entrada desplazándose a través de una \texttt{ACT} ReLU.}
  	\label{fig:relu-act}
  	\end{center}
\end{figure}

\subsubsection{\textit{Fully-connected}}
Las neuronas en las capas \texttt{FC} están totalmente conectadas a todas las activaciones de la capa anterior, como en una red neuronal \textit{feed-forward}. Siempre se ubican al final de la red.

\subsubsection{\textit{Pooling}}

Similar a la capa convolucional, la capa de \texttt{POOL} es responsable de reducir el tamaño espacial de la entidad convolucionada. Esto es para disminuir la potencia computacional requerida para procesar los datos a través de la reducción de dimensionalidad. Además, es útil para extraer características dominantes que son invariantes rotacionales y posicionales, manteniendo así el proceso de entrenamiento efectivo del modelo.

Existen dos tipos:
\begin{itemize}
	\item \textit{Max pooling} (\texttt{MPOOL}): devuelve el valor máximo de la parte de la imagen cubierta por el \textit{kernel} (figura \ref{fig:type-pooling} arriba).
	\item \textit{Average pooling} (\texttt{APOOL}): devuelve el promedio de todos los valores de la parte de la imagen cubierta por el \textit{kernel} (figura \ref{fig:type-pooling} abajo).
\end{itemize}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{tesis_45.png}
  	\caption{Tipos de \textit{pooling}.}
  	\label{fig:type-pooling}
  	\end{center}
\end{figure}

Cabe destacar que \texttt{MAXPOOL} también actúa como supresor de ruido ya que descarta las activaciones ruidosas por completo además de la reducción de dimensionalidad. Por otro lado, el \texttt{AVGPOOL} simplemente realiza la reducción de dimensionalidad como un mecanismo de supresión de ruido. Por lo tanto, podemos decir que \texttt{MAXPOOL} funciona mucho mejor que \texttt{AVGPOOL}.

\subsubsection{\textit{Batch Normalization}}

%Las capas de normalización por lotes o \textit{Batch Normalization} (\texttt{BN}), como su nombre indica, se utilizan para normalizar las activaciones de un volumen de entrada determinado antes de pasarlo a la siguiente capa de la red.

Por lo general, para entrenar una red neuronal, realizamos un preprocesamiento de los datos de entrada, por ejemplo, normalizar todos los datos para que se parezcan a una distribución normal (es decir, media cero y una varianza unitaria). Algunas razones para realizar esto puede ser prevenir la saturación temprana de funciones de activación no lineales como la función \texttt{sig}oidea, asegurar que todos los datos de entrada estén en el mismo rango de valores, etc. \citep{bn}

Pero el problema aparece en las capas intermedias porque la distribución de las activaciones cambia constantemente durante el entrenamiento. Esto ralentiza el proceso de entrenamiento porque cada capa debe aprender a adaptarse a una nueva distribución en cada paso del entrenamiento. Este problema se conoce como \textbf{cambio de covariables interno}.

Podemos utilizar la normalización por lotes o \textit{Batch Normalization} (\texttt{BN}) como un método para normalizar las entradas de cada capa para así forzarlo a tener aproximadamente la misma distribución en cada paso de entrenamiento, con el fin de combatir el problema expresado anteriormente.

Durante el tiempo de entrenamiento, una capa de normalización por lotes se obtiene el promedio y la varianza del lote:

$$ \mu_{\beta} = \frac{1}{m} \sum_{i=1}^m x_i $$

$$ \\texttt{sig}a_{\beta}^2 = \frac{1}{m}  \sum_{i=1}^m (x_i - \mu_{\beta} ) ^2$$

Normalizamos las entradas de la capa usando las estadísticas del lote calculados previamente:

$$ \bar{x_i} = \frac{x_i - \mu_{\beta}}{\sqrt{\\texttt{sig}a_{\beta}^2+\epsilon}} $$

%Escalamos y \textit{shifteamos} en orden para obtener la salida de la capa:

%$$ y_i = \gamma \hat{x_i} + \beta $$

Establecemos $1e-7 \leq \epsilon \leq 0$ para evitar sacar la raíz cuadrada de cero. Aplicar esta ecuación implica que las activaciones que salen de una capa \texttt{BN} tendrán una media y una varianza unitaria aproximadamente cero (es decir, centrada en cero).
Reemplazamos el mini-lote $\mu_{\beta}$ y $\\texttt{sig}a_{\beta}$ con promedios de $\mu_{\beta}$ y $\\texttt{sig}a_{\beta}$ calculados durante el proceso de entrenamiento. Esto asegura que podemos pasar vectores a través de nuestra red y aún así obtener predicciones precisas sin ser sesgados por $\mu_{\beta}$ y $\\texttt{sig}a_{\beta}$ del mini-lote final pasado a través de la red en el momento del entrenamiento. 

La \texttt{BN} también tiene el beneficio adicional de ayudar a "estabilizar" el entrenamiento, lo que permite una mayor variedad de tasas de aprendizaje y fortalezas de regularización. Esto no alivia la necesidad de ajustar estos parámetros, por supuesto, pero le facilitará la vida al hacer que la tasa de aprendizaje y la regularización sean menos volátiles y más fáciles de ajustar. También tenderá a notar pérdidas finales más bajas y una curva de pérdida más estable en sus redes.

\subsubsection{\textit{Dropout}}
El \textit{dropout} (\texttt{DO}) es en realidad una forma de regularización que tiene como objetivo ayudar a prevenir el sobreajuste aumentando la precisión de las pruebas, quizás a expensas de la precisión del entrenamiento. \citep{rosebrock2017deep}

La razón está en reducir el sobreajuste alterando de forma explicita la arquitectura de la red en tiempo de entrenamiento. La desconexión aleatoria de las conexiones garantiza que ningún nodo de la red sea responsable de la activación cuando se le presenta un patrón determinado. El \texttt{DO} garantiza que haya múltiples nodos redundantes que se activarán cuando se les presenten entradas similares (lo que a su vez ayuda a que nuestro modelo a generalizar).

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_46.png}
  	\caption{\textbf{Izquierda:} Dos capas \texttt{FC} sin \texttt{DO}. \textbf{Derecha:} Las mismas dos capas luego de realizar \textit{dropout} sobre la mitad de las conexiones.}
  	\label{fig:type-pooling}
  	\end{center}
\end{figure}

\subsection{\textit{WaveNet} y capas convolucionales causales dilatadas}

\textit{WaveNet} es una red neuronal profunda para generar audio muestra a muestra. La arquitectura de este modelo permite aprovechar las eficiencias de las capas de convolución al mismo tiempo que alivia el desafío de aprender las dependencias a largo plazo en una gran cantidad de pasos de tiempo (más de 1000) \citep{wavenet2}.

En el núcleo de \textit{WaveNet} se encuentra la \textbf{capa de convolución causal dilatada} (figura~\ref{fig:causal-conv}), que le permite tratar adecuadamente el orden temporal y manejar las dependencias a largo plazo sin una explosión en la complejidad del modelo. \citep{wavenet}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.8\textwidth]{causal-conv.png}
  	\caption{Paso de la información a través de la capa de convolución causal dilatada.}
  	\label{fig:causal-conv}
  	\end{center}
\end{figure}

En una capa de convolución unidimensional tradicional, deslizamos un filtro de pesos a través de una serie de entrada, aplicándolo secuencialmente a las regiones (generalmente superpuestas) de la serie. Pero cuando utilizamos el historial de una serie temporal para predecir su futuro, debemos tener cuidado. A medida que formamos capas que eventualmente conectan los pasos de entrada a las salidas, debemos asegurarnos de que las entradas no influyan en los pasos de salida que los siguen a tiempo. De lo contrario, estaríamos usando el futuro para predecir el pasado, lo que sería hacer trampa.

Para asegurarnos de no hacer trampa de esta manera, ajustamos nuestro diseño de convolución para prohibir explícitamente que el futuro influya en el pasado. En otras palabras, solo permitimos que las entradas se conecten a salidas de pasos de tiempo futuros en una estructura \textbf{causal}, como se muestra a continuación en una visualización del documento WaveNet. En la práctica, esta estructura 1D causal es fácil de implementar \textit{shifteando} las salidas convolucionales tradicionales en varios pasos de tiempo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{stackcausal-conv.png}
  	\caption{Visualización de una pila de capas causales convoluciones.}
  	\label{fig:stackcausal-conv}
  	\end{center}
\end{figure}

Las convoluciones causales proporcionan la herramienta adecuada para manejar el flujo temporal, pero necesitamos una modificación adicional para manejar adecuadamente las dependencias a largo plazo. En la figura \ref{fig:stackcausal-conv} de convolución causal simple, puede ver que solo los 5 pasos de tiempo más recientes pueden influir en la salida resaltada. De hecho, necesitaríamos una capa adicional por paso de tiempo para llegar más atrás en la serie (para usar la terminología adecuada, para aumentar el \textbf{campo receptivo de la salida}). Con una serie de tiempo que tiene una gran cantidad de pasos, el uso de convoluciones causales simples para aprender de toda la historia rápidamente haría un modelo demasiado complejo computacional y estadísticamente.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{dilated-conv.png}
  	\caption{Visualización de una pila de capas causales convoluciones dilatadas.}
  	\label{fig:dilated-conv}
  	\end{center}
\end{figure}

En lugar de cometer ese error, \textit{WaveNet} utiliza \textbf{convoluciones dilatadas}, que permiten que el campo receptivo aumente exponencialmente en función de la profundidad de la capa de convolución. 

En una capa de convolución dilatada, los filtros no se aplican a las entradas de una manera secuencial simple, sino que omiten una entrada de tasa de dilatación constante entre cada una de las entradas que procesan, como en el diagrama \textit{WaveNet} a continuación. Al aumentar la tasa de dilatación multiplicativamente en cada capa (por ejemplo, 1, 2, 4, 8,…), podemos lograr la relación exponencial entre la profundidad de la capa y el tamaño del campo receptivo que deseamos. 

En la figura~\ref{fig:dilated-conv}, puede ver cómo ahora solo necesitamos 4 capas para conectar los 16 valores de la serie de entrada a la salida resaltada (digamos, el valor de paso de tiempo 17). Por extensión, cuando se trabaja con una serie de tiempo diaria, se puede capturar más de un año de historia con solo 9 capas de convolución dilatadas de esta forma.

\section{Redes neuronales recurrentes}
Anteriormente hemos visto cómo las redes neuronales o convolucionales nos permiten clasificar un dato, por ejemplo una palabra, un sonido, o una imagen, pero tienen un inconveniente, y es que cuando tenemos una secuencia de datos, por ejemplo una secuencia de palabras, o una conversación, o una secuencia de imágenes, es decir un vídeo, este tipo de arquitecturas no pueden procesar ese tipo de datos. 

Las Redes Neuronales Recurrentes \textit{(RNN)} \citep{karpathy:rnn} resuelven este inconveniente, porque son capaces de procesar diferentes tipos de secuencias, como textos, conversaciones, vídeos, música, y además de eso no sólo clasifican los datos como lo hacen las redes neuronales o convolucionales, sino que también están en capacidad de generar nuevas secuencias.

Si a una red neuronal o convolucional se le presenta una imagen o una palabra, con el entrenamiento adecuado estas arquitecturas lograrán clasificar un sinnúmero de datos, logrando a la vez una alta precisión. 

Pero, ¿qué sucede si en lugar de una única imagen o palabra se introduce a la red una secuencia de imágenes, es decir un vídeo, o una secuencia de palabras (una conversación)? En este caso en ninguna de estas redes será capaz de procesar los datos por dos motivos:
\begin{itemize}
	\item Estas arquitecturas están diseñadas para que los datos de entrada y de salida siempre tengan el mismo tamaño; sin embargo, un vídeo o una conversación se caracterizan por ser un tipo de datos con un tamaño variable: una cantidad variable de "frames"
en el caso del vídeo o una cantidad variable de palabras en el caso de la conversación. 
	\item En un vídeo o en una conversación los datos están \textbf{correlacionados}, esto quiere decir que la siguiente palabra pronunciada o la siguiente imagen en la secuencia de vídeo dependerá de la palabra o imagen anterior. E incluso estas palabras e imágenes estarán relacionadas con aquellas que se presenten más adelante en la secuencia y una \textit{NN} ó \textit{CNN} no está en capacidad de analizar la relación entre varias palabras o imágenes de la secuencia.
\end{itemize}

Una secuencia es una serie de datos que siguen un orden específico y tienen únicamente significado cuando se analizan en conjunto y no de manera individual. Dichos datos, analizados de forma individual o en un orden diferente, carecen de significado. Es evidente que una secuencia no tiene un tamaño predefinido pues no podemos saber con antelación el número de datos. 

Las \textit{RNN} resuelven los inconvenientes expresados anteriormente, pues pueden procesar tanto a la entrada como a la salida secuencias sin importar su tamaño, y además teniendo en cuenta la correlación existente entre los diferentes elementos de esa secuencia.

Para ello este tipo de redes usan el concepto de recurrencia: para generar la salida, que también se conoce como activación, la red usa no sólo la entrada actual sino la activación generada en la iteración previa. En pocas palabras, las redes neuronales recurrentes usan un cierto tipo de memoria para generar la salida deseada. 

\subsection{Arquitecturas} \label{rnnarchitecture}
Existen diversas arquitecturas disponibles para estas redes como observamos en la Figura~\ref{fig:rnnarch} \cite{karpathy:rnn}, donde cada rectángulo es un vector y cada fecha representa funciones. Los vectores de entrada están en rojo, los vectores de salida están en azul y los vectores verdes mantienen el estado de la \textit{RNN}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{rnnarch.jpg}
  	\caption{Tipos de arquitecturas para una \textit{RNN}.}
  	\label{fig:rnnarch}
  	\end{center}
\end{figure}

\textbf{\textit{One-to-one}}
Modo de procesamiento vanilla \textit{i.e.} sin \textit{RNN}, desde una entrada de tamaño fijo a una salida de tamaño fijo, por ejemplo la clasificación de imágenes.

\textbf{\textit{One-to-many}}

La entrada es un único dato y la salida es una secuencia. Un ejemplo de esta arquitectura es el "\textit{image captioning}" en donde la entrada es una y la salida es una secuencia de caracteres, un texto, que describe el contenido de la imagen. 

\textbf{\textit{Many-to-one}}

La entrada es una secuencia y la salida es por ejemplo una categoría. Un ejemplo de esto es la clasificación de sentimientos, en donde por ejemplo la entrada es un texto que contiene una crítica a una película y la salida es una categoría indicando si la película le gustó a la persona o no.

\textbf{\textit{Many-to-many}}
Tanto la entrada como a la salida se tienen secuencias. La primer figura se refiere a \textit{RNN} utilizadas en traductores automáticos: en este caso la secuencia de salida no se genera al mismo tiempo que la secuencia de entrada pues para poder traducir por ejemplo una frase al español se requiere primero conocer la totalidad del texto en inglés. Y desde luego, en esta misma arquitectura podemos encontrar los conversores de voz a texto o texto a voz.
La segunda figura se refiere a secuencias sincronizadas de entrada y salida, por ejemplo clasificación de vídeo donde deseamos etiquetar cada fotograma.

Como era de esperar, el régimen secuencial de operación es mucho más poderoso en comparación con las redes fijas que están condenadas desde el principio por un número fijo de pasos computacionales y, por lo tanto, también es más provechoso a la hora de construir sistemas más inteligentes. 

Además, las \textit{RNN} combinan el vector de entrada con su vector de estado con una función fija (pero aprendida) para producir un nuevo vector de estado. En términos de programación, esto puede interpretarse como ejecutar un programa fijo con ciertas entradas y algunas variables internas. Visto de esta manera, los RNN esencialmente describen programas.

\textbf{Si entrenar redes neuronales es optimización sobre funciones, entrenar redes recurrentes es optimización sobre programas.}

\subsection{Funcionamiento}

Consideremos la Figura~\ref{fig:rnnunit} \citep{olahlstm}, aquí podemos observar como se define de manera gráfica una unidad funcional de una \textit{RNN} denominada $A$, que toma una entrada $x_t$ y genera un valor $h_t$.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.2\textwidth]{tesis_27.png}
  	\caption{Unidad funcional de \textit{RNN}.}
  	\label{fig:rnnunit}
  	\end{center}
\end{figure}

El \textit{loop} de A permite que la información pase de un paso de la red al siguiente.

Una red neuronal recurrente se puede considerar como múltiples copias de la misma red, cada una de las cuales pasa un mensaje a un sucesor. si desenrollamos el ciclo podemos representar la \textit{RNN} a través del eje del tiempo, como se muestra en la Figura~\ref{fig:rnnunrolled}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{tesis_28.png}
  	\caption{Una \textit{RNN} desenrrollada.}
  	\label{fig:rnnunrolled}
  	\end{center}
\end{figure}

Esta naturaleza en cadena revela que las redes neuronales recurrentes están íntimamente relacionadas con secuencias y listas.

En su esencia una \textit{RNN} se parece demasiado a una \textit{FFNN}, excepto que también tiene conexiones hacía atrás.
La \textit{RNN} más simple posible es la que mostramos en la figura 


\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{rnnunitv2.png}
  	\caption{Unidad funcional \textit{RNN} detallada.}
  	\label{fig:rnnunitv2}
  	\end{center}
\end{figure}

En la Figura~\ref{fig:rnnunitv2} se observa que en cada instante de tiempo la red tiene realmente dos entradas y dos salidas. Las entradas son el dato actual, $x_t$ y la activación anterior, $a_{t-1}$, mientras que las salidas son la predicción actual, $y_t$, y la activación actual, $a_t$. Esta activación también recibe el nombre de \textit{hidden state} o estado oculto.

Se define:

$$ a_{t} = tanh(W_{aa}a_{t-1}+W_{ax}x_t+b_a) $$
$$ h_t = softmax(W_{ya}a_t + b_y) $$

Donde:

$ W_{ax}:$ matriz de pesos multiplicando la entrada.

$ W_{aa}:$ matriz de pesos multiplicando el estado oculto.

$ W_{ya}:$ matriz de pesos que relaciona el estado oculto a la salida.

$b_a:$ bias.

$b_y:$ bias que relaciona el estado oculto a la salida.

Es posible entrenar una RNN con una gran cantidad de texto y le pediremos que modele la distribución de probabilidad del siguiente carácter en la secuencia dada una secuencia de caracteres anteriores. Esto nos permitirá generar texto nuevo, de a un carácter a la vez.

Como ejemplo práctico, suponga que solo tenemos un vocabulario de cuatro letras posibles \texttt{helo} y queremos entrenar a un RNN en la secuencia de entrenamiento \texttt{hello}. Esta secuencia de entrenamiento es de hecho una fuente de 4 ejemplos de entrenamiento separados: 
\begin{enumerate}
	\item La probabilidad de \texttt{e} probablemente debería estar dado el contexto de \texttt{h}.
	\item \texttt{l} debería estar probablemente en el contexto de \texttt{he}.
	\item \texttt{l} probablemente también debería ser dado el contexto de \texttt{hel}.
	\item  Y finalmente \texttt{o} debería ser probablemente dado el contexto de \texttt{hell}.
\end{enumerate}

Concretamente, codificaremos cada carácter en un vector usando la codificación \textit{1-of-k} (es decir, todo cero excepto uno en el índice del carácter en el vocabulario) y los introduciremos en el RNN uno a la vez con el función \texttt{step}. Luego observaremos una secuencia de vectores de salida de 4 dimensiones (una dimensión por carácter), que interpretamos como la confianza que el RNN asigna actualmente a cada carácter que sigue en la secuencia. 

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{tesis_29.png}
  	\caption{Unidad funcional \textit{RNN} detallada.}
  	\label{fig:rnnexample}
  	\end{center}
\end{figure}

En la Fig.~\ref{fig:rnnexample} observamos un ejemplo de \textit{RNN} con capas de entrada y salida de 4 dimensiones y una capa oculta de 3 unidades (neuronas). Las activaciones en el pase hacia adelante cuando el \textit{RNN} recibe los caracteres \texttt{"hell"} como entrada. La capa de salida contiene los pesos que el \textit{RNN} asigna al siguiente carácter (el vocabulario es \texttt{"h, e, l, o"}); Queremos que los números verdes sean altos y los números rojos bajos.

Por ejemplo, vemos que en el primer paso de tiempo cuando el \textit{RNN} vio el carácter \texttt{"h"} asignó un peso de $1.0$ a la siguiente letra que era \texttt{"h"}, $2.2$ a la letra \texttt{"e"}, $-3.0$ a \texttt{"l"} y $4.1$ a "o". Dado que en nuestros datos de entrenamiento (la cadena \texttt{"hello"}) el siguiente carácter correcto es \texttt{"e"}, nos gustaría aumentar su peso (verde) y disminuir los pesos de todas las demás letras (rojo). 

De manera similar, tenemos un carácter objetivo deseado en cada uno de los 4 pasos de tiempo a los que nos gustaría que la red le asignara una mayor confianza. Dado que el \textit{RNN} consta completamente de operaciones diferenciables, podemos ejecutar el algoritmo de \textit{back-propagaton} para averiguar en qué dirección debemos ajustar cada uno de sus pesos para aumentar los pesos de los objetivos correctos. 

Luego podemos realizar una actualización de parámetros, que empuja cada peso una pequeña cantidad en esta dirección de gradiente. Si tuviéramos que alimentar las mismas entradas al RNN después de la actualización del parámetro, encontraríamos que las puntuaciones de los caracteres correctos (por ejemplo, "e" en el primer paso de tiempo) serían ligeramente más altas (por ejemplo, $2.3$ en lugar de $2.2$), y los pesos de los caracteres incorrectos serían ligeramente inferiores.

Luego, repetimos este proceso una y otra vez hasta que la red converge y sus predicciones son finalmente consistentes con los datos de entrenamiento en el sentido de que los caracteres correctos siempre se predicen a continuación.

\subsection{Entrenamiento}
Para entrenar una \textit{RNN}, el truco simplemente es desenrrollarla a través del tiempo y simplemente usar \textit{backpropagation} (estrategia que recibe el nombre de \textit{backpropagation} a través del tiempo (\textit{BPTT})) como observamos en la Fig.~\ref{fig:BPTT} \citep{geron}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.8\textwidth]{tesis_40.png}
  	\caption{\textit{Backpropagation} a través del tiempo.}
  	\label{fig:BPTT}
  	\end{center}
\end{figure}

Primero realizamos una pasada hacía adelante a través de la red desenrollada (representada en la figura por las flechas punteadas). 

Luego la secuencia de salida es evaluada utilizando una función de costo $C(Y_{(0)}, Y_{(1)}, \dots, Y_{(T)})$ (donde $T$ es el paso máximo de tiempo). Notemos que la función de coste puede ignorar algunas salidas en función de lo que necesitemos como se muestra en la Fig.~\ref{fig:BPTT}.
Los gradientes de esa función de costo luego son propagados hacía atrás a través de la red desenrrollada (representada a través de las líneas sólidas).

Finalmente los parámetros del modelo son actualizadas usando los gradientes calculados por \textit{BPTT}. Notar que los gradientes fluyen hacía atrás a través de todas las salidas utilizadas por la función de costo, no solamente a través de la salida final (notar que en el ejemplo no fluye a través de $Y_{(0)}$ e $Y_{(1)}$).

\subsection{Desvanecimiento del gradiente}

Otra forma de alimentar una \textit{RNN} podría ser a través de las palabras individuales de una oración, dado que esto se realiza de forma secuencial, debemos proveerle de una palabra a la vez.

En el ejemplo de la Fig.~\ref{fig:rnnvanishing1} intentaremos predecir la intención del usuario tomando como entrada la oración \texttt{"What time is it?"}. \citep{phi:rnn}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{vanishing1.png}
  	\caption{\textit{RNN} siendo alimentada con las palabras de la oración.}
  	\label{fig:rnnvanishing1}
  	\end{center}
\end{figure}

\begin{enumerate}
	\item Inicializa sus capas de red y el estado oculto inicial. La forma y dimensión del estado oculto dependerá de la forma y dimensión de su \textit{RNN}.
	\item Luego recorre sus entradas, pasa la palabra y el estado oculto al \textit{RNN}.
	\item El \textit{RNN} devuelve la salida y un estado oculto modificado.
	\item Continúas repitiendo hasta que te quedas sin palabras.
	\item Por último, pasa la salida a la capa de \textit{feedforward} y devuelve una predicción (Fig.~\ref{fig:rnnvanishing2}).
\end{enumerate}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{vanishing2.png}
  	\caption{Predicción de la \textit{RNN}.}
  	\label{fig:rnnvanishing2}
  	\end{center}
\end{figure}

Pero prestemos atención a la Fig.~\ref{fig:rnnvanishing3}. Es posible que haya notado la extraña distribución de colores en los estados ocultos. Eso es para ilustrar un problema con los \textit{RNN} conocido como memoria a corto plazo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.2\textwidth]{vanishin3.png}
  	\caption{Estado oculto final de la \textit{RNN}.}
  	\label{fig:rnnvanishing3}
  	\end{center}
\end{figure}

La memoria a corto plazo es causada por el infame problema del desvanecimiento del gradiente, que también prevalece en otras arquitecturas de redes neuronales. A medida que el RNN procesa más pasos, tiene problemas para retener información de los pasos anteriores. 

Como puede ver, la información de la palabra \texttt{"What"} y \texttt{"time"} es casi inexistente en el último paso. La memoria a corto plazo y el desvanecimiento del gradiente se deben a la naturaleza del algoritmo de \textit{back-propagation}.

Al hacer \textit{back-propagation}, cada nodo de una capa calcula su gradiente con respecto a los efectos de los gradientes, en la capa anterior. Entonces, si los ajustes a las capas anteriores son pequeños, los ajustes a la capa actual serán aún más pequeños. 

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.25\textwidth]{vanishing4.png}
  	\caption{Desvanecimiento del gradiente desde las capas superiores a las inferiores.}
  	\label{fig:rnnvanishing4}
  	\end{center}
\end{figure}

Es posible pensar en cada paso de tiempo en una \textit{RNN} como una capa y para entrenarla se usa \textit{back-propagation} a través del tiempo. Los valores del gradiente se reducirán exponencialmente a medida que se propaga a través de cada paso de tiempo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.6\textwidth]{vanishing5.png}
  	\caption{El gradiente se achica a medida que se propaga hacia atrás en el tiempo.}
  	\label{fig:rnnvanishing5}
  	\end{center}
\end{figure}

Nuevamente, el gradiente se utiliza para realizar ajustes en los pesos de las redes neuronales, lo que le permite aprender. Pequeños gradientes significan pequeños ajustes. Eso hace que las capas tempranas no aprendan.

Debido a los gradientes que desaparecen, la \textit{RNN} no aprende las dependencias de largo alcance en los pasos de tiempo. Eso significa que existe la posibilidad de que las palabras \texttt{"What"} y \texttt{"time"} no se consideren al intentar predecir la intención del usuario. Entonces, la red tiene que hacer la mejor suposición con \texttt{"is it?"}. Eso es bastante ambiguo y sería difícil incluso para un humano. Por lo tanto, no poder aprender en pasos de tiempo anteriores hace que la red tenga una memoria a corto plazo.

\subsection{Tipos de \textit{RNNs}}
Para mitigar la memoria a corto plazo, se crearon dos redes neuronales recurrentes especializadas. Las redes denominadas \textit{Long Short-Term Memory} o \textit{LSTM} para abreviar. Las otras se denominan \textit{Gated Recurrent Units} o \texttt{GRU}.

\subsubsection{LSTM}
Los \texttt{LSTM} y \texttt{GRU} funcionan esencialmente como los \textit{RNN}, pero son capaces de aprender las dependencias a largo plazo mediante mecanismos llamados "puertas". Estas puertas son diferentes operaciones de tensor que pueden aprender qué información agregar o quitar al estado oculto. Debido a esta capacidad, la memoria a corto plazo es un problema menor para ellos. \citep{olahlstm}

Si consideramos la celda \texttt{LSTM} como una caja negra, aparenta ser idéntica a una \textit{RNN} excepto que su estado se divide en dos vectores: $h_{(t)}$ y $c_{(t)}$ ("c" se mantiene por celda). Es posible pensar a $h_{(t)}$ como un estado de corto plazo y a $c_{(t)}$ como un estado de largo plazo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{lstmcell5.png}
  	\caption{Celda \texttt{LSTM}.}
  	\label{fig:lstmcell}
  	\end{center}
\end{figure}

El diagrama completo del \texttt{LSTM} lo podemos observar en la Fig.\ref{fig:lstmcell}, pero vamos a ir paso a paso analizando cada una de las partes que lo componen.

La clave de los \texttt{LSTM} es el estado de la celda, la línea horizontal que atraviesa la parte superior de la Fig.~\ref{fig:lstm1}. El estado de la celda es como una cinta transportadora. Corre directamente a lo largo de toda la cadena, con solo algunas interacciones lineales menores. Es muy fácil que la información fluya sin cambios.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{lstm1.png}
  	\caption{Celda de estado.}
  	\label{fig:lstm1}
  	\end{center}
\end{figure}

El \texttt{LSTM} tiene la capacidad de eliminar o agregar información al estado de la celda, regulada cuidadosamente por estructuras llamadas puertas. Las puertas son una forma de dejar pasar información opcionalmente. Están compuestos por una capa de red neuronal \texttt{sig}oidea y una operación de multiplicación.

La capa \texttt{sig}oidea genera números entre $0$ y $1$, que describen cuánto de cada componente debe dejarse pasar. Un valor de $0$ significa "no dejar pasar nada", mientras que un valor de $1$ significa "dejar pasar todo". Un \texttt{LSTM} tiene tres de estas puertas para proteger y controlar el estado de la celda.

El primer paso es decidir qué información vamos a eliminar del estado de la celda. Esta decisión la toma una capa \texttt{sig}oidea llamada \textbf{puerta del olvido} (Fig.~\ref{fig:lstm2}). Examina $h_{t-1}$ y $x_t$, y genera un número entre $0$ y $1$ para cada número en el estado de celda $C_{t-1}$. Un $1$ representa "mantener esto completamente", mientras que un $0$ representa "deshacerse de esto por completo".

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{lstm2.png}
  	\caption{Puerta del olvido.}
  	\label{fig:lstm2}
  	\end{center}
\end{figure}

Por tanto la puerta del olvido queda representada por la siguiente ecuación:

$$f_{(t)} = \\texttt{sig}a (W_{xf}x_{(t)} + W_{hf}h_{(t-1)} + b_f)$$

El siguiente paso es decidir qué nueva información almacenaremos en el estado de la celda. Esto tiene dos partes. 

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{lstm3.png}
  	\caption{Puerta de entrada.}
  	\label{fig:lstm3}
  	\end{center}
\end{figure}

Una capa \texttt{sig}oidea llamada \textbf{puerta de entrada} (Fig~\ref{fig:lstm3}) decide qué valores actualizaremos.

$$i_{(t)}=\\texttt{sig}a(W_{xi}x_{(t)} + W_{hi}h_{(t-1)} + b_i)$$

A continuación, una capa $tanh$ crea un vector de nuevos valores candidatos, que podrían agregarse al estado. 

$$g_{(t)}=tanh(W_{xg}x_{(t)}+W_{hg}h_{(t-1)}+b_g)$$


En el siguiente paso, combinaremos estos dos para crear una actualización del estado.

Ahora es el momento de actualizar el estado de la celda anterior, $C_{(t-1)}$, al nuevo estado de la celda $C_{(t)}$. Los pasos anteriores ya decidieron qué hacer, solo tenemos que hacerlo realmente.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{lstm4.png}
  	\caption{Actualización de la celda.}
  	\label{fig:lstm4}
  	\end{center}
\end{figure}

Multiplicamos el estado anterior por $f_{(t)}$, olvidando las cosas que decidimos olvidar antes. Luego le sumamos $i_{(t)} \otimes g_{(t)}$. Estos son los nuevos valores candidatos, escalados según cuánto decidimos actualizar cada valor de estado.

$$C_{(t)} = f_{(t)} \otimes C_{(t-1)} + i_{(t)} \otimes g_{(t)}$$

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{lstm5.png}
  	\caption{Puerta de salida.}
  	\label{fig:lstm5}
  	\end{center}
\end{figure}

Finalmente, tenemos que decidir qué vamos a producir. Esta salida se basará en el estado de nuestra celda, pero será una versión filtrada. Primero, ejecutamos una capa \texttt{sig}oidea que denominaremos \textbf{puerta de salida} que decide qué partes del estado de la celda vamos a generar. Luego, colocamos el estado de la celda a través de $tanh$ (para presionar los valores entre $-1$ y $1$) y lo multiplicamos por la salida de la puerta, de modo que solo produzcamos las partes que decidimos.

$$ o_{(t)}=\\texttt{sig}a (W_{xo}x_{(t)} + W_{ho}h_{(t-1)} + b_o) $$

$$ h_{(t)}=o_{(t)} \otimes tanh(C_{(t)})$$

Pasando en limpio observando nuevamente la Fig.~\ref{fig:lstmcell}, tendremos nuestro vector de entradas $x_{(t)}$ y el estado anterior de corto plazo $h_{(t-1)}$ que alimenta 4 capas \texttt{FC}. Cada una de ellas sirve a un propósito diferente \citep{geron}:

\begin{itemize}
	\item La capa principal es la que genera $g_{(t)}$. Tiene la función habitual de analizar las entradas actuales $x_{(t)}$ y el estado anterior (a corto plazo) $h_{(t-1)}$. En una celda básica \textit{RNN}, no hay nada más que esta capa, y su salida va directamente hacia $y_{(t)}$ y $h_{(t)}$. Por el contrario, en una celda \texttt{LSTM}, la salida de esta capa no sale directamente, sino que se almacena parcialmente en el estado a largo plazo.
	\item Las otras tres capas son controladores de puerta. Dado que usan la función de activación \texttt{sig}oidea, sus salidas van entre 0 y 1. Sus salidas se alimentan a operaciones de multiplicación elemento a elemento (también conocido como producto de Hadamard \citep{hadamard}), por lo que si generan ceros, cierran la puerta, y si generan 1 la abre. Específicamente:
	\begin{itemize}
	\item La puerta de olvido (controlada por $f_{(t)}$) controla qué partes del estado a largo plazo $C_{(t-1)}$ deben borrarse.
	\item La puerta de entrada (controlada por $i_{(t)}$) controla qué partes de $g_{(t)}$ deben agregarse al estado a largo plazo.
	\item La puerta de salida (controlada por $o_{(t)}$) controla qué partes del estado a largo plazo deben leerse y generarse en este paso de tiempo (tanto en $h_{(t)}$ como en $y_{(t)}$).
	\end{itemize}
\end{itemize}

En resumen, una celda \texttt{LSTM} puede aprender a reconocer una entrada importante (ese es el papel de la puerta de entrada), almacenarla en el estado a largo plazo, aprender a preservarla durante el tiempo que sea necesario (ese es el papel de la puerta del olvido) y aprender a extraerla siempre que sea necesario.

\subsubsection{GRU}

Otra variante popular es la celda \texttt{GRU} (Unidad Recurrente Cerrada, \textit{Gated Recurrent Unit)}. \citep{olahlstm}

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{gru2.png}
  	\caption{Celda \texttt{GRU}.}
  	\label{fig:lstm5}
  	\end{center}
\end{figure}

Hace algunas simplificaciones \citep{geron}:

\begin{itemize}
	\item Ambos vectores de estado son combinados en un único vector $h_{(t)}$.
	\item Un controlador de puerta única $z_{(t)}$ controla tanto la puerta de olvido como la puerta de entrada. Si el controlador de puerta genera un 1, la puerta de olvido está abierta $(=1)$ y la puerta de entrada está cerrada $(1-1=0)$. Si genera un 0, sucede lo contrario. En otras palabras, siempre que se deba almacenar una memoria, primero se borra la ubicación donde se almacenará.
	\item No hay puerta de salida; el vector de estado completo se genera en cada paso de tiempo. Sin embargo, hay un nuevo controlador de puerta $r_{(t)}$ que controla qué parte del estado anterior se mostrará en la capa principal $g_{(t)}$.

\end{itemize}

Las ecuaciones quedan de la siguiente forma:

$$z_{(t)}=\\texttt{sig}a(W_{xz}x_{(t)} + W_{hx}h_{(t-1)} + b_z)$$
$$r_{(t)}=\\texttt{sig}a(W_{xr}x_{(t)} + W_{hr}h_{(t-1)} + b_r)$$
$$g_{(t)}=\\texttt{sig}a(W_{xg}x_{(t)} + W_{hg}h_{(t-1)} + b_g)$$
$$h_{(t)}=z_{(t)} \otimes h_{(t-1)} + (1-z_{(t)}) \otimes g_{(t)}$$

\subsection{Secuencias de entradas y salida}

Si bien en la Sección~\ref{rnnarchitecture} ya desglosamos los diferentes tipos de arquitecturas que puede tener una \textit{RNN}, sería de utilidad ahora que ya poseemos un marco teórico aceptable describirlas un poco más y ejemplificar en que casos sería provechosa su utilización. Tomaremos como referencia la Figura~\ref{fig:rnnnets} \citep{geron}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{tesis_42.png}
  	\caption{\textbf{Superior-izquierda}: \textit{seq-to-seq}. \textbf{Superior-derecha}: \textit{sec-to-vector}. \textbf{Inferior-izquierda}: \textit{vector-to-sec}. \textbf{Inferior-derecha}: \textit{encoder-decoder}.}
  	\label{fig:rnnnets}
  	\end{center}
\end{figure}

\textbf{Secuencia a secuencia}

Una \textit{RNN} puede tomar simultáneamente una secuencia de entradas y producir una secuencia de salidas (red \textbf{superior-izquierda}). Por ejemplo, este tipo de red es útil para predecir series de tiempo como los precios de las acciones: usted le da los precios de los últimos $N$ días y debe generar los precios desplazados un día en el futuro (es decir, de $N - 1$ días hace hasta mañana).

\textbf{Secuencia a vector}

Alternativamente, puede alimentar a la red con una secuencia de entradas e ignorar todas las salidas excepto la última (red \textbf{superior-derecha}). En otras palabras, esta es una red de secuencia a vector. Por ejemplo, podría alimentar a la red con una secuencia de palabras correspondiente a una crítica de película y la red generaría una puntuación de sentimiento (\textit{e.g.}, de 0 [odio] a 1 [amor]).

\textbf{Vector a secuencia}

Por el contrario, puede alimentar la red con una sola entrada en el primer paso de tiempo (y ceros para todos los demás pasos de tiempo) y dejar que genere una secuencia (red \textbf{inferior-izquierda}). Por ejemplo, la entrada podría ser una imagen y la salida podría ser un título para esa imagen.

\textbf{Secuencia a secuencia con retardo}

Por último, podría tener una red de secuencia a vector, llamada \textit{encoder}, seguida de una red de vector a secuencia, llamada \textit{decoder} (consulte la red de la parte inferior derecha). Por ejemplo, esto se puede utilizar para traducir una oración de un idioma a otro. Alimentaría la red con una oración en un idioma, el \textit{encoder} convertiría esta oración en una representación de vector único y luego el \textit{decoder} decodificaría este vector en una oración en otro idioma. Este modelo de dos pasos, llamado \textit{encoder-decoder}, funciona mucho mejor que intentar traducir sobre la marcha con un único \textit{RNN} secuencia a secuencia (red \textbf{Superior-izquierda}), ya que las últimas palabras de una oración pueden afectan las primeras palabras de la traducción, por lo que debe esperar hasta que haya escuchado la oración completa antes de traducirla.

\section{Optimización de hiperparámetros} \label{tuning-hp}
Los parámetros que definen la arquitectura del modelo se denominan hiperparámetros y, por lo tanto, este proceso de búsqueda de la arquitectura del modelo ideal se denomina ajuste de hiperparámetros o \textit{hyperparameters tuning}. \citep{Koehrsen2018Jul}

Los hiperparámetros no son parámetros del modelo y no se pueden entrenar directamente a partir de los datos. Mientras que los parámetros del modelo especifican cómo transformar los datos de entrada en la salida deseada a través del entrenamiento optimizando una función de pérdida, los hiperparámetros definen cómo está realmente estructurado nuestro modelo.

Los métodos de ajuste de hiperparámetros se relacionan con cómo muestreamos posibles candidatos a la arquitectura del modelo a partir del espacio de posibles valores de hiperparámetros. Esto se denomina a menudo "buscar" en el espacio de hiperparámetros los valores óptimos.

En general, este proceso incluye:

\begin{enumerate}
	\item Definir los modelos.
	\item Definir el rango de valores posibles para todos los hiperparámetros, es decir el espacio de búsqueda o \textit{search space}.
	\item Definir un algoritmo para obtener valores de hiperparámetros.
	\item Definir un criterio evaluativo para juzgar el modelo.
\end{enumerate}

La optimización de hiperparámetros se representa en forma de ecuación como:

$$x^* = \operatorname*{argmin}_{x \in X} f(x)$$

Aquí $f(x)$ representa una puntuación objetivo para minimizar (usualmente una métrica), evaluada en el set de validación; $x^*$ es el conjunto de hiperparámetros que produce el menor valor para la función objetivo, y $x$ puede tomar cualquier valor en el dominio $X$. En términos simples, queremos encontrar los hiperparámetros del modelo que producen la mejor puntuación en la métrica del conjunto de validación.

El problema con la optimización de hiperparámetros es que evaluar la función objetivo para encontrar la puntuación es extremadamente costoso. Cada vez que probamos diferentes hiperparámetros, tenemos que entrenar un modelo con los datos de entrenamiento, hacer predicciones sobre los datos de validación y luego calcular la métrica de validación. Con una gran cantidad de hiperparámetros y modelos complejos, como conjuntos o redes neuronales profundas que pueden tardar días en entrenarse, y el problema no escala.

\subsection{Algoritmos} \label{algo-hp}

Los algoritmos de optimización de hiperparámetros se pueden dividir en tres categorías principales:
\begin{itemize}
	\item \textbf{Búsqueda exhaustiva del espacio}: estos métodos son una primera opción atractiva para la optimización, ya que son muy fáciles de codificar, se pueden ejecutar en paralelo y no necesitan ningún tipo de ajuste. Su inconveniente es que no hay garantía de encontrar un mínimo local con cierta precisión, excepto si el espacio de búsqueda se muestrea a fondo. Esto no supone ningún problema si el modelo es muy rápido de ejecutar y el número de hiperparámetros es bajo. Si el modelo tarda mucho en ejecutarse utilizando una gran cantidad de recursos computacionales estos métodos son ineficaces, ya que no utilizan la información obtenida de todos los intentos anteriores.
	\item \textbf{Modelos subrogados:} un modelo subrogado de la función de hiperparámetros puede ajustarse a los intentos anteriores y decir dónde podría estar el mínimo local. Estos métodos se denominan optimización basada en modelos secuenciales (SMBO, \textit{Sequential Model-Based Optimization}).
	\item \textbf{Reducciones sucesivas a la mitad:} son algoritmos que se encargan de seleccionar dos configuraciones de hiperparámetros e irlas descartando tempranamente para así converger a la mejor.
\end{itemize}

Para todos los algoritmos, se debe definir un espacio de búsqueda de antemano para así establecer límites para todos los hiperparámetros.

\subsubsection{Búsqueda exhaustiva del espacio}
\textbf{Búsqueda por grilla (\textit{Grid search})}

El espacio de búsqueda de cada hiperparámetro se discretiza, y el espacio de búsqueda total se discretiza como los productos cartesianos de ellos. Luego, el algoritmo lanza un aprendizaje para cada una de las configuraciones de hiperparámetros y selecciona la mejor al final. Es un problema paralelo (siempre que uno tenga la potencia de cálculo necesaria para entrenar varios modelos al mismo tiempo) pero sufre la maldición de la dimensionalidad (el número de configuraciones a probar es exponencial con respecto al número de hiperparámetros a ser optimizado).

\textbf{Búsqueda aleatoria (\textit{Random search})}

Una variación del algoritmo anterior, que muestrea aleatoriamente el espacio de búsqueda en lugar de discretizarlo con una cuadrícula cartesiana. El algoritmo no tiene fin. En su lugar, se debe especificar un presupuesto de tiempo (en otras palabras, una número de \textit{trials} o pruebas). Este algoritmo también sufre la maldición de la dimensionalidad para alcanzar una densidad de muestreo fija preestablecida. Una de las ventajas de la búsqueda aleatoria es que si dos hiperparámetros están poco correlacionados, la búsqueda aleatoria permite encontrar con mayor precisión los óptimos de cada parámetro, como se muestra en la fig.~\ref{fig:grid_vs_search}. 

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{grid_vs_search_v3.png}
  	\caption{Comparación de \textit{Grid Search} vs \textit{Random Search}.}
  	\label{fig:grid_vs_search}
  	\end{center}
\end{figure}

\subsubsection{Modelos subrogados}

\textbf{Optimización bayesiana (\textit{Bayesian Optimization})} 

Tenemos una función $f(x)$, que es costosa de calcular, no es necesariamente una expresión analítica y no conoce su derivada. Y nuestro objetivo es encontrar los mínimos globales. \citep{Ye2020Oct}

Esta es, sin duda, una tarea difícil, más difícil que otros problemas de optimización dentro del aprendizaje automático. El descenso de gradiente, por ejemplo, tiene acceso a las derivadas de una función y aprovecha los atajos matemáticos para una evaluación de expresiones más rápida.

Alternativamente, en algunos escenarios de optimización, la función es simple de evaluar. Si podemos obtener cientos de resultados para variantes de una entrada $x$ en unos pocos segundos, se puede emplear una simple \textit{grid search} con buenos resultados.

Desafortunadamente, no contamos con estas bondades y estamos limitados en nuestra optimización por varios frentes, en particular:
\begin{itemize}
	\item \textbf{Es costoso de calcular}. Idealmente, podríamos consultar la función lo suficiente como para replicarla, pero nuestro método de optimización debe funcionar con una muestra limitada de entradas.
	\item \textbf{Se desconoce la derivada}. Por tanto no podemos aplicar el descenso del gradiente y sus derivados.
	\item \textbf{Necesitamos encontrar los mínimos globales}. Esta es una tarea difícil incluso para un método sofisticado como el descenso de gradiente, entonces nuestro modelo de alguna manera necesitará un mecanismo para evitar quedar atrapado en los mínimos locales.
\end{itemize}

La solución es \textbf{optimización bayesiana} (\textit{Bayesian Optimization}), que proporciona un marco elegante para abordar problemas que se asemejan al escenario descrito para encontrar el mínimo global en el menor número de pasos.

Construyamos un ejemplo hipotético de la función objetivo $c(x)$, o el costo de un modelo dado una entrada $x$. Por supuesto, el aspecto de la función estará oculto al optimizador; la verdadera forma de $c(x)$ es la de la fig~\ref{fig:cx}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{bayes_opt_1.png}
  	\caption{Representación gráfica de la función $c(x)$.}
  	\label{fig:cx}
  	\end{center}
\end{figure}

La optimización bayesiana aborda esta tarea mediante un método conocido como optimización subrogada. Por contexto, una maternidad subrogada es la práctica por la que, previo acuerdo con otra persona, una mujer se queda embarazada, lleva la gestación a término y da a luz a un bebé para esa otra persona o pareja \citep{vientre_alquiler}; en ese contexto, una función subrogada es una aproximación de la función objetivo y se forma sobre la base de puntos muestreados tal como muestra la fig.~\ref{fig:bayes2}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{bayes_opt_2.png}
  	\caption{Función subrogada.}
  	\label{fig:bayes2}
  	\end{center}
\end{figure}

Basándonos en la función subrogada, podemos identificar qué puntos son mínimos prometedores. Decidimos tomar más muestras de estas regiones prometedoras y actualizar la función subrogada en consecuencia (fig.~\ref{fig:bayes3}).

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{bayes_opt_3.png}
  	\caption{Función subrogada.}
  	\label{fig:bayes3}
  	\end{center}
\end{figure}

En cada iteración, continuamos examinando la función subrogada actual, aprendemos más sobre las áreas de interés mediante el muestreo y actualizamos la función. Tenga en cuenta que la función subrogada se expresará matemáticamente de una manera significativamente menos costosa de evaluar (por ejemplo, $y=x$ es una aproximación de una función más costosa, $y = arcsin \left( \frac{1-cos^2x}{sen x} \right)$ dentro de un cierto rango).

Después de un cierto número de iteraciones, estamos destinados a llegar a un mínimo global, a menos que la forma de la función sea muy extraña (en el sentido de que tiene grandes y salvajes oscilaciones hacia arriba y hacia abajo) en la que se debería hacer más foco sobre sus datos que en la optimización.

Tómese un momento para maravillarse con la belleza de este enfoque. No hace ninguna suposición sobre la función (excepto que es optimizable en primer lugar), no requiere información sobre derivadas y es capaz de utilizar el razonamiento de sentido común mediante el uso ingenioso de una función de aproximación continuamente actualizada. La costosa evaluación de nuestra función objetivo original no es un problema en absoluto.

La esencia de las estadísticas y modelos bayesianos es la actualización de una creencia previa (previa) a la luz de nueva información para producir una creencia posterior ("después") actualizada. Esto es exactamente lo que hace la optimización subrogada en este caso, por lo que se puede representar mejor a través de sistemas, fórmulas e ideas bayesianos.

Echemos un vistazo más de cerca a la función subrogada, que generalmente está representada por \textbf{procesos gaussianos}, que se puede considerar como una tirada de dados que devuelve funciones ajustadas a puntos de datos dados (por ejemplo, $sin$, $log$) en lugar de números del 1 al 6 (fig.~\ref{fig:bayes4}). El proceso devuelve varias funciones, que tienen probabilidades asociadas.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{bayes_opt_4.png}
  	\caption{Varias funciones generadas por procesos gaussianos para cuatro puntos de datos.}
  	\label{fig:bayes4}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{bayes_opt_5.png}
  	\caption{Las funciones agregadas.}
  	\label{fig:bayes5}
  	\end{center}
\end{figure}

Hay una buena razón por la que se utilizan procesos gaussianos, y no algún otro método de ajuste de curvas, para modelar la función subrogada: es de naturaleza bayesiana. Un proceso gaussiano es una distribución de probabilidad, como una distribución de los resultados finales de un evento (por ejemplo, 1/2 probabilidad de lanzar una moneda), pero sobre todas las funciones posibles.

Por ejemplo, podemos definir el conjunto actual de puntos de datos como 40\% representable por la función $a(x)$, 10\% por la función $b(x)$, etc. Al representar la función sustituta como una distribución de probabilidad, se puede actualizar con nueva información a través de procesos bayesianos intrínsecamente probabilísticos. Quizás cuando se introduce nueva información, los datos solo son representables en un 20\% por la función $a(x)$. Estos cambios se rigen por fórmulas bayesianas.

La función subrogada, representada como una distribución de probabilidad se actualiza con una \textbf{función de adquisición}. Esta función es responsable de impulsar la propuesta de nuevos puntos para probar, en una compensación de exploración y explotación:
\begin{itemize}
	\item \textbf{Explotación:} busca muestrear dónde el modelo subrogada predice un buen objetivo. Esto está aprovechando los lugares prometedores conocidos. Sin embargo, si ya hemos explorado una determinada región lo suficiente, la explotación continua de información conocida producirá poca ganancia.
	\item \textbf{Exploración}: busca muestrear en lugares donde la incertidumbre es alta. Esto asegura que no quede ninguna región importante del espacio sin explorar; es posible que los mínimos globales se encuentren allí.
\end{itemize}

Una función de adquisición que fomente demasiada explotación y muy poca exploración hará que el modelo resida solo un mínimo que encuentre primero (generalmente local). Una función de adquisición que fomente lo contrario no se quedará en mínimos, locales o globales, en primer lugar. Dando buenos resultados en un delicado equilibrio.

La función de adquisición, que denotaremos $a(x)$, debe considerar tanto la explotación como la exploración. Las funciones de adquisición comunes incluyen la mejora esperada y la probabilidad máxima de mejora, todas las cuales miden la probabilidad de que una entrada específica pueda dar resultados en el futuro, dada la información sobre el anterior (el proceso gaussiano). Existen diversas funciones de adquisición tales como \textit{Upper Confidence Bound (UCB)}, \textit{Probability of Improvement (PI)}, \textit{Expected Improvement (EI)}, etc. \citep{Agnihotri2020May}

Juntemos las piezas. La optimización bayesiana se puede realizar como tal:
\begin{enumerate}
	\item Inicializar una distribución previa de función subrogada del proceso gaussiano.
	\item Elija varios puntos de datos $x$ de modo que se maximice la función de adquisición $a(x)$ que opera en la distribución anterior actual.
	\item Evalúe los puntos de datos $x$ en la función de costo objetivo $c(x)$ y obtenga los resultados, $y$.
	\item Actualice la distribución previa del proceso gaussiano con los nuevos datos para producir un posterior (que se convertirá en el anterior en el siguiente paso).
	\item Repita los pasos 2 a 5 para varias iteraciones.
	\item Interprete la distribución actual del proceso gaussiano (que es poco costoso de realizar) para encontrar los mínimos globales.
\end{enumerate}

La optimización bayesiana se trata de poner ideas probabilísticas detrás de la idea de optimización subrogada. La combinación de estas dos ideas crea un sistema poderoso con muchas aplicaciones, desde el desarrollo de productos farmacéuticos hasta vehículos autónomos. Sin embargo, más comúnmente en el aprendizaje automático, la optimización bayesiana se usa para la optimización de hiperparámetros. Por ejemplo, si estamos entrenando un clasificador de aumento de gradiente, hay docenas de parámetros, desde la tasa de aprendizaje hasta la profundidad máxima y el valor mínimo de división de impurezas. En este caso, $x$ representa los hiperparámetros del modelo y $c(x)$ representa el rendimiento del modelo.

La principal motivación para utilizar la optimización bayesiana se encuentra en escenarios en los que es muy costoso evaluar la salida, tal como el entrenamiento de múltiples arquitecturas de modelos para encontrar la mejor.

Para dejar en limpio:

\begin{itemize}
	\item La optimización subrogada utiliza una función subrogada o aproximada para estimar la función objetivo a través del muestreo.
	\item La optimización bayesiana coloca la optimización sustituta en un marco probabilístico al representar funciones sustitutas como distribuciones de probabilidad, que pueden actualizarse a la luz de nueva información.
	\item Las funciones de adquisición se utilizan para evaluar la probabilidad de que la exploración de un determinado punto en el espacio produzca un rendimiento "bueno" dado lo que se conoce actualmente de la exploración y la explotación anteriores, equilibrando la exploración.
	\item Utilice la optimización bayesiana principalmente cuando la función objetivo sea costosa de evaluar, comúnmente utilizada en el ajuste de hiperparámetros.
\end{itemize}

Ahora extrapolemos el conocimiento adquirido al ajuste de hiperparámetros teniendo en cuenta que la optimización bayesiana crea un modelo de probabilidad de la función objetivo. \citep{Wang2020Apr}

La función objetivo real es una función fija. Supongamos que se parece a la figura~\ref{fig:bayes6}, pero obviamente desconocemos su forma.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{bayes_opt_6.png}
  	\caption{Función objetivo real.}
  	\label{fig:bayes6}
  	\end{center}
\end{figure}

Dado que calcular la métrica de nuestro modelo es costoso, digamos que solo tenemos 10 muestras, representada como círculos negros en la figura~·\ref{fig:bayes7}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{bayes_opt_7.png}
  	\caption{Muestras de la función objetivo real.}
  	\label{fig:bayes7}
  	\end{center}
\end{figure}

Usando estas 10 muestras, necesitamos construir un modelo subrogado (también llamado modelo de superficie de respuesta) para aproximarnos a la función objetivo real. Observemos la fig.~\ref{fig:bayes8}, el modelo subrogado se representa como la línea azul y la sombra azul representa la desviación.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.55\textwidth]{bayes_opt_8.png}
  	\caption{Modelo subrogado.}
  	\label{fig:bayes8}
  	\end{center}
\end{figure}

Un modelo (o función) subrogado es un modelo probabilístico que asigna hiperparámetros a una probabilidad de la puntuación de una métrica en la función objetivo real. Entonces queda formalmente definido como:

$$ P(y|x) $$

Dónde $y$ representa la puntuación de la métrica y $x$ el conjunto de hiperparámetros seleccionado.

Ahora tenemos 10 muestras de la función objetivo, para decidir qué parámetro probar como la undécima muestra necesitamos construir la función de adquisición. El siguiente hiperparámetro de elección es donde se maximiza la función de adquisición.

En la fig.~\ref{fig:bayes9} abajo, el tono verde es la función de adquisición y la línea recta roja es donde se maximiza. Por lo tanto, el hiperparámetro correspondiente y su puntuación de función objetivo, representada como un círculo rojo, se utiliza como la undécima muestra para actualizar el modelo subrogado (fig.~\ref{fig:bayes9} arriba).

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.50\textwidth]{bayes_opt_9.png}
  	\caption{\textbf{Arriba}: modelo subrogado. \textbf{Abajo:} función de adquisición.}
  	\label{fig:bayes9}
  	\end{center}
\end{figure}

Luego de usar una función de adquisición para determinar el siguiente hiperparámetro, se obtiene la puntuación de la función objetiva real de este nuevo hiperparámetro. Dado que el modelo subrogado se entrena en $P(y|x)$, la adición de un nuevo punto de datos actualiza el modelo subrogado.

Ahora se deben repetir los pasos anteriores hasta que se alcance el tiempo máximo o la iteración máxima. Ahora posee una aproximación precisa de la función objetivo real y puede encontrar fácilmente el mínimo global de las muestras evaluadas en el pasado para así completar la optimización bayesiana.

Finalmente vamos a desglosar el siguiente pseudo-código propuesto en \textit{"Algorithms for Hyper-Parameter Optimization"} \citep{smbo}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.55\textwidth]{smbo.png}
  	\caption{El pseudocódigo genérico de la optimización  basada en modelos secuenciales.}
  	\label{fig:smbo}
  	\end{center}
\end{figure}

\textit{SMBO} significa optimización basada en modelos secuenciales, que es otro nombre de optimización bayesiana. Es "secuencial" porque los hiperparámetros se agregan para actualizar el modelo subrogado uno por uno; está "basado en modelos" porque se aproxima a la función objetivo real con un modelo subrogado que es más barato de evaluar.

Otras representaciones en el pseudocódigo:
\begin{itemize}
	\item{$\mathcal{H}$: historial de observación del par(puntuación, hiperparámetro).}
	\item{$T$: número máximo de iteraciones.}
	\item{$f$: función objetiva real (la puntuación de la métrica del modelo).}
	\item{$M$: modelo subrogado, que se actualiza cada vez que se agrega una nueva muestra.}
	\item{$S$: función de adquisición.}
	\item{$x^*$: el siguiente hiperparámetro elegido para evaluar.}
\end{itemize}

La secuencia se desarrolla de la siguiente manera:
\begin{itemize}
	\item Primero, iniciamos un modelo subrogado y una función de adquisición.
	\item Línea 3: luego, para cada iteración, encuentre el hiperparámetro $x^*$ donde se encuentra el mínimo de la función de adquisición $S$. La función de adquisición es una función del modelo subrogado, lo que significa que se construye utilizando el modelo subrogado en lugar de la verdadera función objetivo real.
	\item Línea 4: Obtenga la puntuación de la función objetivo de $x^*$ para ver cómo se desempeña realmente este punto
	\item Línea 5: Incluya el par (hiperparámetro $x^*$, puntuación de función objetivo real) en el historial de otras muestras.
	\item Línea 6: entrene el modelo subrogado utilizando el último historial de muestras.
	\item Repita hasta alcanzar el número máximo de iteraciones. Al final, se devuelve el historial de (hiperparámetro, puntuación de función objetiva real). Tenga en cuenta que el último registro no es necesariamente el mejor puntaje logrado. Tendría que ordenar la puntuación para encontrar el mejor hiperparámetro.
\end{itemize}

Ya que la Optimización Bayesiana necesita tiempo para construir un buen modelo que conducirá a configuraciones de mejor rendimiento, en su versión básica,se comporta de manera similar a la búsqueda aleatoria al principio. Sin embargo, con el aumento de los presupuestos, el modelo recopila cada vez más información sobre el espacio de búsqueda, lo que genera grandes ventajas sobre la Búsqueda Aleatoria (fig.~\ref{fig:rs_vs_bo}). \citep{automl-bohb}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{comparison_rs_bo.png}
  	\caption{Rendimiento de Búsqueda Aleatoria \textit{vs} Optimización Bayesiana.}
  	\label{fig:rs_vs_bo}
  	\end{center}
\end{figure}

\subsubsection{Sucesivas reducciones a la mitad}
A menudo, en la práctica, las configuraciones prometedoras tienden a obtener puntuaciones más altas, en relación con las peores, incluso en las primeras etapas del proceso. A este hecho se aferran los algoritmos de sucesivas reducciones a la mitad \textit{SuccessiveHalving}.

Y proceden de la siguiente forma:
\begin{enumerate}
	\item Muestra aleatoriamente un conjunto de $n$ configuraciones de hiperparámetros.
	\item Evalua el desempeño de todas las configuraciones restantes actualmente.
	\item Desecha la mitad inferior de las peores configuraciones de puntuación y duplica el presupuesto para el resto hasta que alcanza el presupuesto máximo.
	\item Vuelva al paso 2 y repita hasta que quede una configuración.
\end{enumerate}

La secuencia se ejemplifica en la fig.~\ref{fig:sh} donde cada línea paralela al eje de ordenadas es una iteración y cada línea de color corresponde a una configuración.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{sucessive_halving.png}
  	\caption{Secuencia de algoritmo de \textit{SucessiveHalving}.}
  	\label{fig:sh}
  	\end{center}
\end{figure}

En lugar de perder mucho tiempo de entrenamiento en configuraciones que no nos llevarán a ninguna parte, \textit{SuccessiveHalving} los descarta lo antes posible. Por lo tanto, se puede asignar más tiempo de entrenamiento, es decir, recursos, a modelos más potencialmente valiosos.

Sin embargo, el \textit{Successive Halving} sufre lo que se denomina el \textit{trade-off} $n$ \textit{vs} $B/n$ \citep{Bissuel2019Apr}, dónde:
\begin{itemize}
	\item $B$ es el presupuesto total que se asignará para la búsqueda de hiperparámetros.
	\item $n$ es el número de configuraciones que se explorarán.
	\item $B/n$ es la cantidad promedio de recursos asignados a una configuración específica.
\end{itemize}

Para un presupuesto dado, no está claro si buscar muchas configuraciones ($n$ grande) por un tiempo pequeño, o explorar pocas configuraciones pero asignándoles muchos recursos ($B/n$ grande). Para un problema dado, si las configuraciones de hiperparámetros se pueden discriminar rápidamente (si el conjunto de datos converge rápidamente, si las malas configuraciones se revelan rápidamente, o si el espacio de búsqueda de hiperparámetros no se elige lo suficientemente bien como para que uno elegido al azar sea muy malo), entonces $n$ debe elegirse grande. Por el contrario, si son lentos para diferenciar (o si el espacio de búsqueda es pequeño, pero se busca la mejor configuración con alta confianza), entonces $B/n$ debe ser grande (a expensas del número de configuraciones probadas).

Los inconvenientes de estas estrategias son los siguientes:
\begin{itemize}
	\item Si $n$ es grande, algunas buenas configuraciones que pueden tardar en converger al principio se eliminarán pronto.
	\item Si $B/n$ es grande, entonces se asignarán muchos recursos a las malas configuraciones, aunque podrían haberse detenido antes.
\end{itemize}

El algoritmo que veremos a continuación elimina este dilema al considerar varios valores posibles de $n$ para un $B$ fijo, en esencia realizando una \textit{Grid Search} sobre el valor factible de $n$.

\hfill

\textbf{\textit{Hyperband}} \label{hyperband}

El algoritmo de \textit{Hyperband} se presenta en \textit{ Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization} \citep{li2018hyperband} y el enfoque se basa en una estrategia de detención temprana para algoritmos iterativos de algoritmos de aprendizaje automático.

El principio subyacente del procedimiento explota la intuición de que si una configuración de hiperparámetros está destinada a ser la mejor después de una gran cantidad de iteraciones, es más probable que funcione en la mitad superior de las configuraciones después de una pequeña cantidad de iteraciones. Es decir, incluso si el rendimiento después de un pequeño número de iteraciones es muy poco representativo del rendimiento absoluto de las configuraciones, su rendimiento relativo en comparación con muchas alternativas entrenadas con el mismo número de iteraciones aproximadamente se mantiene.

\textit{Hyperband} es un método que tiene como objetivo encontrar los mejores hiperparámetros para un modelo y un presupuesto (\textit{budget}) determinado. 

Al conjunto de valores para cada hiperparámetro se denomina configuración. Se entiende por presupuesto a la cantidad de recursos disponibles, ya sea para entrenar un modelo una determinada cantidad de épocas (\textit{epochs}) o probar diversas configuraciones del mismo (\textit{trials)}. 

Además \textit{Hyperband} es una extensión del \textit{Successive Halving}, y el problema con estos algoritmos es que a menudo no podemos saber cuál es la compensación correcta entre el número de pruebas y el número de épocas como vimos anteriormente. En ciertos casos, algunas configuraciones de hiperparámetros pueden tardar más en converger, por lo que comenzar con muchas pruebas pero una pequeña cantidad de épocas no será lo ideal; en otros casos, la convergencia es rápida y la cantidad de pruebas es el cuello de botella. \citep{Rosenberg2020Aug}

Ahí es donde entra en juego \textit{Hyperband}, que es básicamente una \textit{Grid Search} sobre la estrategia de asignación óptima. Entonces, en cada prueba individual, el conjunto de hiperparámetros se elige al azar.

\textit{Hyperband} se basa en dos bucles \texttt{for} anidados. El bucle exterior itera de $0$ a $s_{max}$ y ejecuta un procedimiento de \textit{Successive Halving} cada vez. El bucle interno también ejecuta \textit{Successive Halving}, (lo denominamos \texttt{bracket}) repitiendosé $s$ veces. Comienza con $n$ modelos que se ejecutan con un presupuesto $r$, y en cada ciclo, el número de modelos se reduce en $\eta$ mientras que el mismo factor aumenta el presupuesto. \citep{Abraham2021Feb}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{hyperband_algo.png}
  	\caption{Secuencia de algoritmo de \textit{SucessiveHalving}.}
  	\label{fig:hyper_algo}
  	\end{center}
\end{figure}

Dentro del algoritmo:
\begin{itemize}
	\item $\eta$, el factor de poda, es el factor de modelos eliminados en cada paréntesis. Por ejemplo, en el procedimiento de \textit{Successive Halving} original donde $\eta = 2$, se podría comenzar con 64 modelos, mantener los mejores 32 modelos después del primer soporte, luego los mejores 16 modelos, y así sucesivamente.
	\item $\eta$, en azul, es el número de modelos considerados en cada bucle. Observe que la fórmula incluye una división y un operador de techo para obtener un valor entero. Siguiendo el ejemplo anterior, sería 64.
	\item $n^i$, en rojo, es el número de modelos utilizados en la iteración $i$. Observe que se redondea con un operador de piso.
	\item La cantidad sin nombre en verde es el número de modelos guardados de una iteración a la otra. Nuevamente, se redondea con un operador de piso.
\end{itemize}

Dado que la mayoría de estos valores están redondeados, esperamos que sean fracciones, no números enteros exactos. La complicada fórmula de $\eta$ está diseñada para consumir tanto presupuesto como sea posible en cada bucle de \textit{Hyperband}.

En la fig.~\ref{fig:hyper_epochs_trials}, puede ver que el algoritmo de \textit{HyperBand} se ejecutará dividiendo a la mitad sucesivamente en 5 asignaciones de recursos. $s=4$ comienza su primera ronda con $81$ \textit{trials}, ofreciendo a cada uno una única época y luego descarta de forma iterativa $\frac{2}{3}$ de los intentos hasta que queda uno y se entrena durante $81$ épocas. En $s=0$, el algoritmo \textit{Hyperband} básicamente está ejecutando una búsqueda aleatoria con$ 5$ pruebas, cada uno entrenado en el número máximo de épocas.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.7\textwidth]{hyper_trials_vs_epochs.png}
  	\caption{Cantidad de épocas y pruebas a medida que avanza el algoritmo.}
  	\label{fig:hyper_epochs_trials}
  	\end{center}
\end{figure}

En la práctica, \textit{HyperBand} funciona muy bien para presupuestos pequeños a medianos y, por lo general, supera la búsqueda aleatoria y la optimización bayesiana con bastante facilidad en ese entorno. Sin embargo, su convergencia está limitada por su dependencia de configuraciones dibujadas al azar: con presupuestos más grandes, su ventaja sobre la Búsqueda Aleatoria disminuye. Un ejemplo de esto se da en la figura \ref{fig:rs_vs_hb} a continuación. \citep{automl-bohb}

Dado que la Optimización Bayesiana se comporta de manera similar a la Búsqueda Aleatoria al principio, \textit{Hyperband} exhibe la misma ventaja sobre éste para presupuestos pequeños a medianos. Para presupuestos más grandes, el modelo de Optimización Bayesiana normalmente guía la búsqueda a regiones de mejor rendimiento y supera a \textit{Hyperband}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{comparison_rs_hb.png}
  	\caption{Comparación de Búsqueda Aleatoria \textit{vs} \textit{HyperBand}.}
  	\label{fig:rs_vs_hb}
  	\end{center}
\end{figure}

\section{Series temporales}

Una serie temporal o cronológica es una sucesión de datos medidos en determinados momentos y ordenados cronológicamente. \citep{Agrawal2019Mar}

\subsection{Clasificación del modelo}

Los datos de entrada se pueden subdividir aún más para comprender mejor su relación con la variable de salida.

\subsubsection{Tipo de variables de entrada}

Una variable de entrada es:
\begin{itemize}
	\item \textbf{endógena} si se ve afectada por otras variables del sistema y la variable de salida depende de ella.
	\item \textbf{exógena} si es independiente de otras variables del sistema y la variable de salida depende de ella.
\end{itemize}

En pocas palabras, las variables endógenas están influenciadas por otras variables del sistema (incluidas ellas mismas), mientras que las variables exógenas no lo están y se consideran fuera del sistema.

Por lo general, un problema de pronóstico de series de tiempo tiene variables endógenas (por ejemplo, el resultado es una función de cierto número de pasos de tiempo anteriores) y puede tener o no variables exógenas.

A menudo, las variables exógenas se ignoran debido al fuerte enfoque en las series de tiempo. Pensar explícitamente en ambos tipos de variables puede ayudar a identificar datos exógenos que se pasan por alto fácilmente o incluso características de ingeniería que pueden mejorar el modelo.

\subsubsection{Objetivo}

Los problemas de modelado predictivo de:

\begin{itemize}
	\item \textbf{regresión} son aquellos en los que se predice una cantidad.
	\item  \textbf{clasificación} son aquellos en los que se predice una categoría.
\end{itemize}

\subsubsection{Estructura}

Es útil trazar cada variable en una serie temporal e inspeccionar la trama en busca de posibles patrones.

\begin{itemize}
	\item \textbf{No estructurado}: No hay un patrón dependiente del tiempo sistemático obvio o discernible en una variable de serie temporal.

	\item \textbf{Estructurado}: Patrones sistemáticos dependientes del tiempo en una variable de serie temporal (por ejemplo, tendencia y/o estacionalidad).
\end{itemize}

Podemos pensar en una serie sin patrón como \textbf{desestructurada}, ya que no existe una estructura discernible dependiente del tiempo.

Alternativamente, una serie de tiempo puede tener patrones obvios, como una \textbf{tendencia} o \textbf{ciclos estacionales estructurados}.

A menudo podemos simplificar el proceso de modelado identificando y eliminando las estructuras obvias de los datos, como una tendencia creciente o un ciclo repetido. Algunos métodos clásicos incluso le permiten especificar parámetros para manejar estas estructuras sistemáticas directamente.

\subsubsection{Cantidad de variables utilizadas como características}

Una serie temporal según la cantidad de variables medidas que serán utilizadas como entrada al modelo puede ser:
\begin{itemize}
	\item \textbf{Univariada:} una única variable.
	\item \textbf{Multivariada:} múltiples variables. 
\end{itemize}

Es importante clasificar nuestra serie temporal en alguna de estos dos ya que los modelos a aplicar difieren de forma considerable en complejidad (multivariadas).

\subsubsection{Horizonte de pronóstico}

El horizonte de pronóstico es el período de tiempo en el futuro para el cual se nos propusimos predecir. Estos generalmente varían desde horizontes de pronóstico a corto plazo (menos de tres meses) hasta horizontes a largo plazo (más de dos años).

Sin embargo eso es una cuestión relativa a como fueron medidos los datos de nuestro \textit{dataset} (segundos, minutos, horas, días, etc). Dado que nuestro \textit{dataset} está indexado según la marca de tiempo (\textit{timestamp}) nosotros podemos decidir cuantos pasos hacía adelante vamos a realizar nuestra predicción.

\begin{itemize}
	\item \textbf{Un paso:} requiere predecir el próximo paso de tiempo futuro.
	\item \textbf{Varios pasos:} requiere predecir más de un paso de tiempo futuro.
\end{itemize}

Cuantos más pasos de tiempo se proyecten en el futuro, más desafiante será el problema dada la naturaleza agravada de la incertidumbre en cada paso de tiempo previsto.

\subsubsection{Estático \textit{vs} Dinámico}
Se refiere a la actualización del modelo para dar nuevos pronósticos.

\textbf{Estático:} El modelo de pronóstico se ajusta una vez y se usa para hacer predicciones.

\textbf{Dinámica:} El modelo de pronóstico se ajusta a los nuevos datos disponibles antes de cada predicción.

\subsubsection{Uniformidad en el tiempo}

\begin{itemize}
	\item \textbf{Contiguo:} Las observaciones se hacen uniformes a lo largo del tiempo. Muchos problemas de series de tiempo tienen observaciones contiguas, como una observación cada hora, día, mes o año.
	\item \textbf{Discontiguo:} Las observaciones no son uniformes a lo largo del tiempo. La falta de uniformidad de las observaciones puede deberse a valores perdidos o corruptos. También puede ser una característica del problema cuando las observaciones solo están disponibles esporádicamente o en intervalos de tiempo cada vez más o menos espaciados.
\end{itemize}

En el caso de observaciones no uniformes, es posible que se requiera un formato de datos específico al ajustar algunos modelos para que las observaciones sean uniformes a lo largo del tiempo.

\subsection{Modelos disponibles}

Se realizó una investigación de los modelos de \textit{forecasting} disponibles de forma gratuita y los resultados se reflejan en el Cuadro~\ref{tab:ts-models}. Se realizará una breve síntesis de cada uno de ellos, sin embargo no se puede responder a la pregunta de cual es mejor ya que según el tipo de problema a abordar alguno tendrá mejor rendimiento que el otro.

\begin{table}[H]
\centering
\begin{tabular}{l|ccccccc}
\textbf{Librería} & \textbf{Creador/es}   & \textbf{Estrellas} & Versión     & Licencia     & Framework ML \\ \hline
\texttt{Prophet}          & Facebook                      & 12000              & 0.7.1          & MIT          & \textit{PyTorch}      \\
\texttt{Sktime}           & ATI  & 3400               & 0.5.1          & BSD-3-Clause & -            \\
\texttt{GluonTS}          & AWSlabs                           & 1700               & 0.6.4        & Apache-2.0   & \textit{mxnet}        \\
\texttt{pmdarima}         & Alkaline-ml                          & 780                & 1.8.0          & MIT          & -            \\
\texttt{arch}             & bashtage                                    & 630                & 4.15           & Propia       & -            \\
\texttt{DCRNN}            & liyaguang                                  & 600          &  ?                    & MIT          & \textit{TensorFlow}   \\
\texttt{Sktime-dl}        & ATI                    & 340                & 0.1.0   & BSD-3-Clause & \textit{Keras}       
\end{tabular}
\caption{Comparativo de modelos de series temporales.}
\label{tab:ts-models}
\end{table}

Se explorarán los 3 más populares de \href{http://www.github.com}{Github} 

\subsubsection{\texttt{Prophet}}
\texttt{Prophet} es un modelo desarrollado por \textit{Facebook AI} para pronosticar datos de series de tiempo basado en un modelo aditivo donde las tendencias no lineales se ajustan a la estacionalidad anual, semanal y diaria, más los efectos de las vacaciones. \citep{prophet}

\begin{itemize}
	\item Funciona mejor con series de tiempo que tienen fuertes efectos estacionales y varias temporadas de datos históricos.
	\item Robusto ante los datos faltantes y los cambios de tendencia, y normalmente maneja bien los valores atípicos.
	\item Incluye muchas posibilidades para que los usuarios modifiquen y ajusten los pronósticos. Puede utilizar parámetros interpretables por humanos para mejorar su pronóstico agregando su conocimiento del dominio.
	\item Está pensado para series temporales bursátiles en primera instancia, pero es posible utilizarlo en otros campos.
\end{itemize}

\subsubsection{\texttt{GluonTS}}
\textit{Gluon Time Series} (\texttt{GluonTS}) es el kit de herramientas desarrollado por \textit{AWSlabs} para el modelado probabilístico de series de tiempo, que se centra en modelos basados ​​en el aprendizaje profundo. Proporciona utilidades para cargar e iterar sobre conjuntos de datos de series de tiempo, modelos de última generación listos para ser entrenados y bloques de construcción para definir sus propios modelos y experimentar rápidamente con diferentes soluciones. \citep{gluonts}

\subsubsection{\texttt{sktime}}
Kit de herramientas desarrollado por \textit{Alan Turing Institute}, que proporciona algoritmos de series temporales especializados y herramientas compatibles con \textit{scikit-learn} para construir, ajustar y validar modelos de series de tiempo para múltiples problemas de aprendizaje, que incluyen: \citep{sktime}

\begin{itemize}
	\item \textbf{Pronóstico de series temporales}: predecir el mañana dado datos pasados.
	\item \textbf{Clasificación de series temporales}: dada una serie temporal, asignar una etiqueta,
\end{itemize}

No utiliza aprendizaje profundo, pero \textit{ATI} está desarrollando una librería que si lo hace denominada \texttt{sktime-dl}. Se encuentra en fase de desarrollo actualmente.

\subsubsection{Conclusión}

Debido a la naturaleza de nuestra serie temporal (que es de tipo intermitente) no podemos ajustarnos a algunos de estos modelos ya que necesitan series temporales continuas.

\section{Desarrollo}

\subsection{Breve introducción}
Los datos recabados provienen de la línea de fabricación de vehículos utilitarios en una usina de automotores, en la cual se utiliza un sistema de diversas auditorias para identificar defectos.

\subsubsection{Organización de la usina}
La usina se compone de departamentos, talleres y unidades de trabajo en ese orden jerárquico. A continuación listaremos los departamentos:

\begin{itemize}
	\item Calidad (\texttt{CALI})
	\item Embutición (\texttt{EMBU})
	\item Ingeniería (\texttt{DLI})
	\item Logística (\texttt{SQF})
	\item Montaje (\texttt{MONT})
	\item Pintura (\texttt{PINT})
	\item Soldadura (\texttt{SOLD})
	\item Soldadura (\texttt{DIVD})
\end{itemize}

La línea de fabricación consta de 4 departamentos que intervienen directamente en el ensamblaje del vehículo:

\texttt{EMBU --> SOLD -> PINT -> MONT}

Los demás departamentos son una parte vital de la producción pero intervienen indirectamente.

Cabe destacar a su vez que cada departamento posee talleres, que a su vez poseen unidades de trabajo obteniendo una estructura jerárquica de tipo árbol.

\subsubsection{Nomenclatura de defectos}

Un defecto (\texttt{DEF}) en su esencia se compone de un elemento (\texttt{ELE}), un incidente (\texttt{INC}) y opcionalmente una localización (\texttt{LOC}).

\texttt{DEF = ELE + INC + LOC}

Tanto \texttt{ELE} como \texttt{INC} están definido inequívocamente por un código de 4 caracteres mientras que \texttt{LOC} puede variar, lo que lleva a que un defecto posea al menos 8 caracteres.

\subsubsection{Tipo}
A su vez los defectos están categorizados por familia o tipo de defectos:

\begin{itemize}
	\item \texttt{APR}: aprietes.
	\item \texttt{ASP}: aspecto.
	\item \texttt{DGRC}: degradaciones.
	\item \texttt{ELEC}: eléctrico.
	\item \texttt{ESTQ}: estanqueidad.
	\item \texttt{FCIO}: funcionamiento.
	\item \texttt{FLDS}: fluidos.
	\item \texttt{FTES}: faltantes.
	\item \texttt{GMTR}: geometría.
	\item \texttt{MOP}: modo operatorio.
	\item \texttt{NCON}: no conforme.
	\item \texttt{RDOS}: ruidos.
\end{itemize}

\subsubsection{Puntos de captaje} \label{ptocptj}
Los defectos son detectados en diversos puntos de captaje que pueden definir (o no) a que departamento o taller pertenece el defecto con un doble objetivo. En primer lugar para que los vehículos que ya poseen el defecto detectado sean derivados a puntos de retoque para ser reparados. Y además analizar y ejecutar diversas estrategias con el fin atacar el problema raíz para así erradicar el defecto.

Los defectos detectados provienen de las siguientes fuentes de datos:
\begin{itemize}
	\item \textbf{Defectos por unidad} (\texttt{DPU}): los defectos se detectan en puntos de captaje a lo largo de toda la línea de producción.
	\item \textbf{Carrocería pintura-soldadura} (\texttt{CAPS}): se realiza una auditoria al final de la línea de pintura sobre una muestra de carrocerías.
	\item \textbf{Plan estático-dinámico} (\texttt{PESD}): una vez finalizado el proceso de fabricación del vehículo, se le realizan pruebas de estanqueidad, \textit{test-drive} y otros con objeto de encontrar defectos que en línea no serían detectables.
	\item \texttt{SAVES}: Auditoria de 5 minutos de una muestra aleatoria de vehículos en línea final.
\end{itemize}

Sin embargo solo obtenemos datos a partir de los puntos de captaje de \texttt{SOLD} dado que en \texttt{EMBU} las partes aún no poseen el identificador único del vehículo.

\subsection{Procesamiento de datos}

Es vital realizar el procesamiento de los datos para alimentar el modelo, para ello se ha divido la tarea en las etapas que se muestran en la figura \ref{fig:dataproc}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=1\textwidth]{dataproc.png}
  	\caption{Procesamiento de datos segmentado en etapas.}
  	\label{fig:dataproc}
  	\end{center}
\end{figure}

El entorno de trabajo será \texttt{Jupyter-Notebook} que soporta \texttt{Python} como lenguaje de programación y las librerías utilizadas serán: 
\begin{itemize}
	\item \texttt{Pandas}: manejo de tablas y datos.
	\item \texttt{Numpy}: operaciones matriciales.
	\item \texttt{Matplotlib}, \texttt{Seaborn}: visualización de datos.
\end{itemize}

\subsubsection{Recolección de datos}

Etapa de búsqueda y recolección de los datos que serán utilizados para el modelo. En nuestro caso se dividió en recolectar en la organización los datos de defectos y paradas de línea (se obtuvieron para los departamentos \texttt{SOLD}, \texttt{PINT} y \texttt{MONT}).

Los datos fueron obtenidos desde 4 auditorias diferentes que fueron presentadas en \ref{ptocptj}.

\subsubsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataCleaning.ipynb}{\color{blue}Limpieza de datos}}
Consiste en inspeccionar los datos obtenidos en búsqueda de posibles errores o datos que quizás es posible obtener pero por diversas razones se han perdido y debemos intentar recuperar.

Ambos casos sucedieron, por tanto se procedió a recuperar los datos cuando fue posible, y en los que no se procedió al descarte debido a que no eran útiles para la tarea.

\subsubsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataPreproccesing.ipynb}{\color{blue}Preprocesamiento de datos}}
En este punto es donde debemos ponernos finos a la hora de inspeccionar nuestro \textit{dataset} para homogeneizar los datos, detectar incongruencias o datos sin sentido.

En primer lugar se realizó una homogeneización de los datos, \textit{i.e.} quizás el mismo dato se puede estar refiriendo a lo mismo pero tienen \textit{tags} diferentes, por tanto es necesario revisar los datos columna por columna. Esto sucedió debido a que provenir los datos de diversas fuentes cada una tenía una forma de tratarlos. 

Luego fue necesario la detección de incongruencias, y ahí nos topamos con inconvenientes tales como departamentos inexistentes. Además todos los defectos que no hayan sido catalogados en el campo \texttt{GVD} pasaron a ser \texttt{V2}.

Debido a la estructura jerárquica de la usina, si tenemos definido la UET a la que pertenece el defecto, escalando hacía arriba podemos obtener el taller y el departamento correspondiente. Esta operación se realizó para todo el \textit{dataset}.

Finalmente tenemos nuestro \textit{dataset} en condiciones para su análisis.

\subsubsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataPlottingAndAnalysis.ipynb}{\color{blue}Análisis y visualización de datos}}
Haremos una parada en nuestro procesamiento de datos para visualizarlos y dimensionar en que posición nos encontramos.
Nuestro \textit{dataset} se compone de $229934$ filas que cada una representa un defecto y 17 columnas.

Algunas estadísticas a \textit{grosso modo} que podemos sacar del mismo son:

\begin{itemize}
	\item 10780 vehículos fueron fabricados durante 2020.
	\item 5224 defectos diferentes fueron registrados en el sistema.
	\item Se registran en promedio 19,6 defectos por vehículo.
\end{itemize}

Además si verificamos de donde provienen nuestros datos caeremos en cuenta que casi la totalidad de ellos provienen de \texttt{DPU}, disputándose el pequeño margen restante \texttt{PESD}, \texttt{CAPS} y \texttt{SAVES}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{audidist.png}
  	\caption{Distribución de los defectos según \texttt{AUDI}.}
  	\label{fig:distaudi}
  	\end{center}
\end{figure}

Sin embargo no debemos perder el foco que es predecir la cantidad de defectos graves que ocurrirán en función a todos los defectos anteriores. En la fig.~\ref{fig:distgvddpto} observamos que la gran mayoría de defectos registrados son \texttt{V2} lo que podría ser útil para nuestro modelo.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.5\textwidth]{tesis_51.png}
  	\caption{Distribución de los defectos según gravedad.}
  	\label{fig:distgvd}
  	\end{center}
\end{figure}

La distribución por departamento nos muestra que los defectos principalmente ocurren en 3: \texttt{PINT}, \texttt{SOLD} y \texttt{MONT}, un poco más atrás en la participación está \texttt{SQF}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{tesis_53.png}
  	\caption{Distribución de los defectos por \texttt{DPTO}.}
  	\label{fig:distgvddpto}
  	\end{center}
\end{figure}

La distribución resume que casi 7 decimos de los defectos totales son de tipo \texttt{ASP}, seguido muy por detrás por \texttt{DGRC} y en tercer lugar los de \texttt{MOP}.

\begin{figure}[H]
	\begin{center}				
	\includegraphics[width=0.7\textwidth]{tesis_55.png}
  	\caption{Distribución de los defectos por \texttt{TIPO}.}
  	\label{fig:distgvdtype}
  	\end{center}
\end{figure}

A través de un mapa de calor (fig.~\ref{fig:heatmapdptotype}) intentaremos cruzar esta información para obtener de manera visual la correlación que existe entre los departamentos y los tipos de defectos.
Notamos que existe una cierta correlación entre algunos tipos de defectos y los departamentos, a tal en punto que en algunos departamentos ni siquiera existen determinados tipos de defectos. Sería provechoso que nuestro modelo pueda aprender esta característica para maximizar sus posibilidades.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_56.png}
  	\caption{Cantidad de defectos cruzando \texttt{DPTO} y \texttt{TIPO}.}
  	\label{fig:heatmapdptotype}
  	\end{center}
\end{figure}

Sin embargo no hay que pensar que debido a que un determinado departamento y defecto posee muchos defectos la muchos son \texttt{V1}, la fig.~\ref{fig:heatmapfractionv1} se encarga de ilustrar el concepto. Así dilucidamos que hay determinadas combinaciones de \texttt{TIPO} y \texttt{DPTO} que son más probable que sean \texttt{V1}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_58.png}
  	\caption{Fracción de \texttt{V1} por cantidad de defectos.}
  	\label{fig:heatmapfractionv1}
  	\end{center}
\end{figure}

Sin embargo el análisis más importante de manera visual que podemos realizar es el gráfico temporal de la cantidad de defectos según su gravedad como se muestra en la figura~\ref{fig:defectsperday}. 

El 19 de marzo de 2020 se decretó la cuarentena debido a la pandemia provocada por el virus \textit{SARS-CoV-2}, por tanto no se fabricaron vehículos. Los defectos registrados durante esa fecha y el reinicio de la producción en junio se deben a la recuperación de vehículos por retoques. Por esto para que el flujo de los datos hacía el modelo sea continuo solo se tomará desde junio a diciembre (6 meses).

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_64.png}
  	\caption{Defectos diarios agrupados por \texttt{GVD}.}
  	\label{fig:defectsperday}
  	\end{center}
\end{figure}

Sin embargo, aún no podemos notar un patrón especifico debido al ruido, por tanto aumentaremos la ventana de tiempo a una semana (fig.~\ref{fig:defectsperweek}). Afortunadamente para nuestro modelo (aunque la escala del eje de ordenadas es logarítmico) la cantidad de defectos \texttt{V2} sigue la curva de \texttt{V1} de forma muy similar. Esto es positivo ya que podemos intuir que existe una correlación entre los mismos del cúal nuestro modelo podrá obtener provecho.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_63.png}
  	\caption{Defectos semanales agrupados por \texttt{GVD}.}
  	\label{fig:defectsperweek}
  	\end{center}
\end{figure}

Si al gráfico anterior lo desglosamos por auditoria y gravedad (fig.~\ref{fig:defectsperaudigvdweek}) veremos que en las demás auditorias que no sean \texttt{DPU} no hay patrones claros definidos, quizás se deba a la baja densidad de datos.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{audigvd.png}
  	\caption{Defectos semanales agrupados por \texttt{AUDI}/\texttt{GVD}.}
  	\label{fig:defectsperaudigvdweek}
  	\end{center}
\end{figure}

\subsubsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataImputation.ipynb}{\color{blue}Imputación de datos}}
 
En estadística, la imputación es el proceso de reemplazar los datos faltantes (\textit{missing values}) con valores sustituidos. En nuestro \textit{dataset} tenemos 2 \textit{features} con faltantes, una es \texttt{UET} (lo que conlleva a faltantes de \texttt{DPTO} y \texttt{TALL}) y la otra es \texttt{TIPO}.

Como se observa en la fig.~\ref{fig:missingvalue}, los datos faltantes no son demasiados, más precisamente $1052$ de \texttt{UET} y $1228$ de \texttt{TIPO}. Debido a la gran cantidad de datos disponibles podemos rellenar estos valores con algún algoritmo de \textit{Machine Learning} visto con anterioridad.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_66.png}
  	\caption{Datos faltantes por \textit{feature}.}
  	\label{fig:missingvalue}
  	\end{center}
\end{figure}

Luego de revisar los valores faltantes se optó por usar como estimador diversos tipos de clasificadores para así seleccionar el de mejor rendimiento, para posteriormente rellenar los valores.

Los siguientes clasificadores fueron seleccionados:

\begin{itemize}
	\item \textit{KNeighborsClassifier} \texttt{(KNC)}
	\item \textit{SGDClassifier} \texttt{(SGDC)}
	\item \textit{RidgeClassifier} \texttt{(RC)}
    \item \textit{LogisticRegression} \texttt{(LR)}
    \item \textit{XGBClassifier} \texttt{(XGBC)}
    \item \textit{DecisionTreeClassifier} \texttt{(DTC)}
    \item \textit{RandomForestClassifier} \texttt{(RFC)}
    \item \textit{BaggingClassifier} \texttt{(BC)}
\end{itemize}

Se optó por utilizar como \textit{feature} \texttt{ELE}, \texttt{INC}, \texttt{LOC} y \texttt{GVD}, dejando como \textit{target} \texttt{TIPO}.

\hfill

\textbf{Estimación de la precisión del modelo}

Para estimar la precisión de los modelos seleccionados se utilizó \textit{K-fold Cross-Validation} (\texttt{KFCV}) \citep{kfolds}.

Una iteración de \texttt{KFCV} se realiza de la siguiente manera: 

\begin{enumerate}
	\item Se genera una permutación aleatoria del conjunto de muestra y se divide en $K$ subconjuntos (pliegues o \textit{folds}) de aproximadamente el mismo tamaño.
	\item De esos $K$ subconjuntos, un solo subconjunto se retiene como datos de validación para probar el modelo (este subconjunto se llama \textit{testset}), y los restantes $K-1$ subconjuntos juntos se utilizan como datos de entrenamiento (\textit{trainset}).
	\item Luego, se entrena un modelo en el \textit{trainset} y se evalúa su precisión en el \textit{testset}. 
	\item El entrenamiento y la evaluación del modelo se repiten $K$ veces, y cada uno de los $K$ subconjuntos se utiliza exactamente una vez como \textit{trainset}.
\end{enumerate}

En la fig.~\ref{fig:kfolds} se ejemplifica parte del proceso.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.75\textwidth]{tesis_67.png}
  	\caption{Ejemplo de validación cruzada de 5 veces con 30 muestras.}
  	\label{fig:kfolds}
  	\end{center}
\end{figure}

La estimación de precisión resultante depende de la permutación aleatoria que se generó al comienzo del proceso, ya que afecta la forma en que se particiona el conjunto de muestras. 

Por lo tanto, para obtener una estimación más exacta de la precisión, tiene sentido repetir la validación cruzada varias veces y tomar el promedio de todas las estimaciones de precisión obtenidas después de cada iteración como estimación de precisión resultante.

\begin{python}

\end{python}

\textbf{Matriz de correlación}

En un \textit{dataset} con muchos atributos, el conjunto de valores de correlación entre pares de sus atributos forma una matriz que se denomina matriz de correlación.

Existen varios métodos para calcular un valor de correlación. El más popular es el coeficiente de correlación de \textit{Pearson}. Sin embargo, debe notarse que mide solo la relación lineal entre dos variables. En otras palabras, es posible que no pueda revelar una relación no lineal. El valor de la correlación de \textit{Pearson} varía de $-1$ a $+1$, donde $\pm1$ describe una correlación positiva/negativa perfecta y $0$ significa que no hay correlación. \citep{corrmatrix}

La matriz de correlación es una matriz simétrica con todos los elementos diagonales iguales a $+1$. Nos gustaría enfatizar que una matriz de correlación solo brinda información sobre la correlación y NO es una herramienta confiable para estudiar la causalidad. 

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{corrmatrix.png}
  	\caption{Matriz de correlación de nuestro \textit{dataset}.}
  	\label{fig:corrmatrix}
  	\end{center}
\end{figure}

En la fig.~\ref{fig:corrmatrix} nos posaremos sobre las dos \textit{features} que nos propusimos predecir.

\begin{itemize}
	\item \texttt{TIPO}: en este caso notamos que las features \texttt{ELE} y \texttt{INC}, tienen una relación notable con \texttt{TIPO}, por tanto esas serán utilizadas y el resto descartadas.
	\item \texttt{UET}: aquí las relaciones están mucho más difusas, por tanto usaremos \textit{features} que estén en las filas a las cuales queremos predecir su \texttt{UET}. Por tanto se usará \texttt{ELE}, \texttt{INC}, \texttt{LOC}, \texttt{GVD} y \texttt{TIPO}.
\end{itemize}

\textbf{\textit{scikit-learn}}

Los algoritmos utilizados están implementados en la librería \textit{sk-learn}, que nos permite a través de \textit{pipelines} realizar un preprocesamiento de los datos para posteriormente entrenar al modelo. En nuestro caso debido a que todas las \textit{features} son de tipo categoría usamos la codificación \texttt{one-hot encoder}, como observamos en la fig.~\ref{fig:sklearn}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.45\textwidth]{tesis_74.png}
  	\caption{Gráfico de caja del rendimiento de cada modelo para \texttt{TIPO}.}
  	\label{fig:sklearn}
  	\end{center}
\end{figure}

\textbf{Predicción de \texttt{TIPO}}

El tiempo de ejecución se considera un parámetro importante debido a que si nuestro \textit{dataset} creciera y poseemos el mismo poder de cómputo quizás deberíamos optar por entrenar un modelo que dé un buen rendimiento por poco tiempo de entrenamiento, \textit{e.g.} \texttt{DTC} (fig.~\ref{fig:modeltxtipo}).

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\textbf{Modelo} & \textbf{Tiempo de ejecución} & \textbf{Rendimiento} & \textbf{Rend. por tiempo} \\ \hline
\texttt{KNC}  & 0,37  & 99,42 \% & 2,71 \\
\texttt{SGDC} & 1,24  & 98,76 \% & 0,79 \\
\texttt{RC}   & 3,08  & 97,99 \% & 0,32 \\
\texttt{LR}   & 21,95 & 99,25 \% & 0,05 \\
\texttt{XGBC} & 23,20 & 96,72 \% & 0,04 \\
\texttt{DTC}  & 1,53  & 99,72 \% & 0,65 \\
\texttt{RFC}  & 20,05 & 99,73 \% & 0,05 \\
\texttt{BC}   & 11,00 & 99,71 \% & 0,09
\end{tabular}
\caption{Ranking de clasificadores para \texttt{TIPO}}
\label{tab:classifiers-table}
\end{table}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tipotimes.png}
  	\caption{Gráfico de caja del rendimiento de cada modelo para \texttt{TIPO}.}
  	\label{fig:modeltxtipo}
  	\end{center}
\end{figure}

Los resultados de rendimiento de cada modelo se presentan en el cuadro~\ref{tab:classifiers-table} y los interpretaremos de manera visual en la fig.~\ref{fig:boxplotmodeltype}. Hay una leve ventaja de \texttt{RandomForestClassifier} por sobre los demás modelos, así que será el seleccionado para imputar los datos faltantes de \texttt{TIPO}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tipoperf.png}
  	\caption{Gráfico de caja del rendimiento de cada modelo para \texttt{TIPO}.}
  	\label{fig:boxplotmodeltype}
  	\end{center}
\end{figure}

\textbf{Predicción de \texttt{UET}}

Utilizando un \textit{miniset} (\textit{i.e.} una fracción de nuestro \textit{dataset}) se fueron probando diferentes \textit{features} para nuestro modelo (debido a que las pruebas requieren menor tiempo de cálculo y se supone que la muestra representa al menos la mayor parte del \textit{dataset}). Tener en cuenta que para que este enfoque funcione es necesario setear el \texttt{random-state} de las funciones con una \textit{seed} que es un número entero, esto es para quitar el componente de azar de los modelos y poder realizar las pruebas sobre los mismos datos.

Así llegamos a la conclusión que los mejores rendimientos se obtenían con \texttt{ELE}, \texttt{INC} y \texttt{LOC}, por tanto descartamos \texttt{GVD} y \texttt{TIPO}.

Una vez más \texttt{DTC} se lleva la mejor relación entre tiempo de entrenamiento y rendimiento como observamos en la fig.~\ref{fig:timesmodeluet}. El mejor rendimiento lo obtiene \texttt{RFC} (fig.~\ref{fig:perfmodeluet}).

Finalmente imputaremos los datos en nuestro dataset con el mejor clasificador para cada \textit{feature}, así logrando el objetivo de completar los datos faltantes.

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\textbf{Modelo} & \textbf{Tiempo de ejecución} & \textbf{Rendimiento} & \textbf{Rendimiento por tiempo} \\ \hline
\texttt{KNC}  & 0,42   & 76,45 \% & 1,80 \\
\texttt{SGDC} & 6,01   & 73,39 \% & 0,12 \\
\texttt{RC}   & 19,45  & 68,54 \% & 0,04 \\
\texttt{LR}   & 74,98  & 74,88 \% & 0,01 \\
\texttt{XGBC} & 193,92 & 73,52 \% & 0,00 \\
\texttt{DTC}  & 2,29   & 79,60 \% & 0,35 \\
\texttt{RFC}  & 157,46 & 79,74 \% & 0,01 \\
\texttt{BC}   & 22,36  & 79,61 \% & 0,04
\end{tabular}
\caption{Ranking de clasificadores para \texttt{UET}}
\label{tab:ranking-uet-table}
\end{table}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_71.png}
  	\caption{Gráfico de caja del rendimiento de cada modelo para \texttt{TIPO}.}
  	\label{fig:timesmodeluet}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_72.png}
  	\caption{Gráfico de caja del rendimiento de cada modelo para \texttt{TIPO}.}
  	\label{fig:perfmodeluet}
  	\end{center}
\end{figure}

\subsubsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataFormatting.ipynb}{\color{blue}Formateo de datos}}

El paso final antes de alimentar nuestro modelos es obtener rodajas (\textit{slices}) de tiempo con funciones de agregación de tipo \texttt{sum}, \texttt{count}, etc de nuestros datos. 

\hfill
\textbf{Serie temporal univariada}

Pero como observamos en la Fig.~\ref{fig:graf_cantdef_antes}, nuestra serie temporal dista mucho de ser continua. Esto es un inconveniente ya que se ha demostrado que a los modelos les cuesta mucho aprender sobre series temporales intermitentes.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_78.png}
  	\caption{Gráfico temporal de cantidad de defectos.}
  	\label{fig:graf_cantdef_antes}
  	\end{center}
\end{figure}

Por tanto tomaremos las siguientes consideraciones:

\begin{itemize}
	\item tomar el periodo posterior a la cuarentena por \textit{Sars-CoV-2}.
	\item quitar los días sin o con baja producción.
	\item quitar horarios no laborales.
\end{itemize}

En la Fig.~\ref{fig:graf_inter_antes} donde aplicamos la función escalón, notamos los huecos que tenemos en nuestra serie, por tanto aplicaremos diversas funciones sobre la misma para obtener la serie continua deseada.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_79.png}
  	\caption{Gráfico temporal de detección de intermitencia.}
  	\label{fig:graf_inter_antes}
  	\end{center}
\end{figure}

Una vez aplicado los filtros necesarios obtenemos la siguiente serie de la Fig.~\ref{fig:graf_cantdef_desp}. En la Fig.~\ref{fig:graf_inter_desp} si bien siguen apareciendo ceros son mucho menores y se originan en horarios de producción mínima como el horario de inicio y finalización.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_80.png}
  	\caption{Gráfico temporal de cantidad de defectos procesado.}
  	\label{fig:graf_cantdef_desp}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tesis_81.png}
  	\caption{Gráfico temporal de detección de intermitencia  procesado.}
  	\label{fig:graf_inter_desp}
  	\end{center}
\end{figure}

\textbf{Serie temporal multivariada}

El mismo análisis realizado para la serie univariada debe extrapolarse a la cantidad de dimensiones que consideremos necesarias. 

En la fig.~\ref{fig:graf_mult_dpto} observamos la cantidad de defectos que luego son desglosados por departamento, lo que nos daría 8 \textit{features}. En la fig.~\ref{fig:graf_mult_tipo} los defectos son desglosados por tipo, lo que nos daría 11 \textit{features}. Lo que resultaría en 19 \textit{features} que van a alimentar nuestro modelo.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multi_dpto.png}
  	\caption{Gráfico temporal por departamentos.}
  	\label{fig:graf_mult_dpto}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multi_flia.png}
  	\caption{Gráfico temporal por tipo de defecto.}
  	\label{fig:graf_mult_tipo}
  	\end{center}
\end{figure}

Tal como se realizó en la serie temporal univariada, se aplican los mismos pasos a estas series temporales de lo cual resultan las figuras \ref{fig:graf_post_mult_dpto} y \ref{fig:graf_post_mult_tipo}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multi_post_dpto.png}
  	\caption{Gráfico temporal procesado por departamentos.}
  	\label{fig:graf_post_mult_dpto}
  	\end{center}
\end{figure}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multi_post_flia.png}
  	\caption{Gráfico temporal procesado por tipo de defecto.}
  	\label{fig:graf_post_mult_tipo}
  	\end{center}
\end{figure}

Ya tenemos los \textit{datasets} definitivos formateados de una manera correcta para alimentar nuestros modelos.

\section{Modelos}

Los modelos serán creados combinando las diferentes capas que se han visto con anterioridad, en dónde la tabla nos resumirá cada capa en una breve descripción además detallando los parámetros de cada una de ellas. Los parámetros de cada capa son importantes ya que en nuestra búsqueda del mejor modelo en la próxima sección implementaremos el \textit{tuning} de hiperparámetros.

\subsection{Métricas}

Es importante definir métricas para determinar el rendimiento de nuestro modelo de una manera empírica, más allá de que nuestro modelos serán entrenados utilizando como métrica de pérdida \texttt{MAE} (error absoluto medio).

\subsubsection{\texttt{MAPE}}

Una de las métricas más comunes que se utilizan para medir la precisión del pronóstico de un modelo es \texttt{MAPE}, que significa error porcentual absoluto medio (\textit{mean absolute percentage error}). \citep{MAPE}

La fórmula para calcular \texttt{MAPE} es la siguiente:

$$ \texttt{MAPE} = \frac{100}{N} \times \sum_{i=1}^{N} |\frac{y_i - \hat{y_i}}{y_i}| $$

dónde:

\begin{itemize}
	\item $ y_i$: valores de los datos reales.
	\item $ \hat{y_i}$: valor de datos pronosticados.
	\item $ N $: cantidad de puntos.
\end{itemize}

\texttt{MAPE} se usa comúnmente porque es fácil de interpretar y de explicar. Por ejemplo, un valor \texttt{MAPE} de $11,5\%$ significa que la diferencia promedio entre el valor pronosticado y el valor real es $11,5\%$. Cuanto menor sea el valor de \texttt{MAPE}, mejor podrá un modelo pronosticar valores.

Sin embargo esta métrica no nos será de utilidad ya que si alguno de los valores reales es $0$ (como sucede) entonces el valor será incalculable por la división por $0$. 

\subsubsection{\texttt{MASE}}
En el pronóstico de series de tiempo, el \texttt{MASE} es el error medio absoluto escalado (\textit{Mean Absolute Scaled Error}) es una medida para determinar la efectividad de los pronósticos generados a través de un algoritmo al comparar las predicciones con el resultado de un enfoque de pronóstico ingenuo o \textit{naive forecast}. \citep{MASE}

El \textit{naive forecast} se genera en cualquier paso equiparando el pronóstico actual con la salida del último paso de tiempo. Por ejemplo, la predicción de las ventas de una empresa al comienzo de un mes se realiza comparándola con las ventas reales del último mes sin considerar ningún patrón estacional.

El \texttt{MAE} para \textit{naive forecast} se calcula de la siguiente forma:

$$
\texttt{MAE}_{naive} = \frac{1}{N-1} \sum_{i=2}^N |y_i - y_{i-1}|
$$

Para determinar la efectividad de un algoritmo de pronóstico, el \texttt{MASE} se calcula de la siguiente manera:

\begin{enumerate}
	\item Calcular el \texttt{MAE} (\textit{Mean Absolute Error}) para los prónosticos del algoritmo.
	
	$$
	\texttt{MAE} = \frac{1}{N} \sum_{i=1}^N |\hat{y_i} - y_i|	
	$$	
	\item \texttt{MASE} está dado por el ratio entre el \texttt{MAE} del algoritmo y el \texttt{MAE} del \textit{naive forecast}.
	
	$$
	\texttt{MASE} = \frac{\texttt{MAE}}{\texttt{MAE}_{naive}}	
	$$
	
\end{enumerate}

\textbf{Características}
\begin{itemize}
	\item \texttt{MASE} da una indicación de la eficacia del algoritmo de pronóstico con respecto a un \textit{naive forecast}. Su valor mayor que uno indica que el algoritmo está funcionando mal en comparación con el \textit{naive forecast}.

	\item \texttt{MASE} es inmune al problema que enfrenta el Error de porcentaje absoluto medio \texttt{MAPE} cuando la salida de la serie de tiempo real en cualquier paso de tiempo es cero. En esta situación, \texttt{MAPE} da una salida infinita, que no es significativa. Sin embargo, se observa que para una serie de tiempo con todos los valores iguales a cero en todos los pasos, la salida de \texttt{MASE} tampoco se definirá, pero tales series de tiempo no son realistas.
	
	\item \texttt{MASE} es independiente de la escala del pronóstico, ya que se define utilizando la proporción de errores en el pronóstico. Esto significa que los valores de \texttt{MASE} serán similares si pronosticamos series de tiempo de alto valor, como el número de paquetes de tráfico de Internet que cruzan un router por hora, en comparación con el número de peatones que cruzan un semáforo ocupado cada hora.
\end{itemize}

\subsection{\textit{Framework}}

Se utilizó \textit{Keras}, una biblioteca de redes neuronales de código abierto escrita en \texttt{Python}. Es capaz de ejecutarse sobre \textit{TensorFlow}, M\textit{icrosoft Cognitive Toolkit} o \textit{Theano}. Su autor principal y mantenedor ha sido el ingeniero de François Chollet.

Está especialmente diseñada para posibilitar la experimentación en más o menos poco tiempo con redes de Aprendizaje Profundo. Sus fuertes se centran en ser amigable para el usuario, modular y extensible. \citep{keras-wiki}

\subsubsection{Callbacks}
Una característica muy interesante que nos ofrece \textit{Keras} son las \textit{callbacks}. Se define como un objeto que puede realizar acciones en varias etapas del entrenamiento (por ejemplo, al comienzo o al final de una época, antes o después de un solo lote, etc.). Repasaremos algunas de las que fueron utilizadas en el entrenamiento del modelo. \citep{callbacks}

\begin{itemize}
	\item \texttt{EarlyStopping}: detiene el entrenamiento cuando una métrica monitoreada haya dejado de mejorar. Asumimos que el objetivo de un entrenamiento es minimizar la pérdida, \textit{e.g.} la métrica a monitorear sería \texttt{loss} y el modo sería \texttt{min}. Un ciclo de entrenamiento verificará al final de cada época si la pérdida ya no está disminuyendo, considerando el \texttt{min\_delta} y la paciencia (cantidad de épocas sin mejorar). Una vez que se encuentra que ya no disminuye, el entrenamiento termina. Esto es de utilidad para ahorrar tiempo de entrenamiento.
	\item \texttt{ReduceLROnPlateau} \label{plateau}: reducir la tasa de aprendizaje cuando una métrica ha dejado de mejorar. Los modelos a menudo se benefician de la reducción de la tasa de aprendizaje en un factor de 2 a 10 una vez que el aprendizaje se estanca. Esta \textit{callback} monitorea una métrica y si no se ve ninguna mejora en un número de épocas de "paciencia", la tasa de aprendizaje se reduce. Ha demostrado ser de gran utilidad.
	\item \texttt{ModelCheckpoint}: se usa junto con el entrenamiento para guardar un modelo o pesos (en un archivo de punto de control) en algún intervalo, por lo que el modelo o los pesos se pueden cargar más tarde para continuar el entrenamiento desde el estado guardado.
	Algunas opciones que ofrece esta devolución de llamada incluyen:
	\begin{itemize}
		\item Ya sea para mantener solo el modelo que ha logrado el "mejor desempeño" hasta ahora, o si para guardar el modelo al final de cada época sin importar el desempeño.
		\item La frecuencia a la que debería guardar. Actualmente, la devolución de llamada permite guardar al final de cada época o después de un número fijo de lotes de entrenamiento.
		\item Si solo se guardan los pesos o se guarda todo el modelo.
	\end{itemize}
\end{itemize}

\subsection{Arquitecturas} \label{archs}
Se define como la arquitectura del modelo a la disposición del mismo en lo que concierne a sus capas y al flujo de la información a través de las mismas.

En \textit{Keras} es posible definir un modelo \texttt{Sequential}, lo que significa que cada salida alimentará a la siguiente y no habrá bifurcaciones entre el flujo de los tensores. También se podría resumir que \texttt{Sequential} agrupa una pila lineal de capas, lo que para nuestro propósito es suficiente. Esto sería una arquitectura de tipo lineal.

Utilizando \textit{TensorFlow} es posible crear arquitecturas no-lineales que admitan múltiples flujos de datos entre las diferentes capas.

\subsubsection{Hiperpárametros de capas}
En la figura~\ref{fig:layer_types} podemos observar las capas disponibles para crear nuestra arquitectura.

\begin{figure}[H]
	\begin{center}
	\includegraphics{model_types.eps}
  	\caption{Tipos de capas disponibles.}
  	\label{fig:layer_types}
  	\end{center}
\end{figure}

Detallaremos los hiperpárametros que será posible \textit{tunear} de cada capa.

\begin{itemize}
	\item \texttt{INPUT}:
	\begin{itemize}
		\item \texttt{ts}: \textit{timestamps}.
		\item \texttt{features}: características.
		\item \texttt{samples}: cantidad de muestras.
	\end{itemize}
	\item \texttt{CNN-CAUSAL}
	\begin{itemize}
		\item \texttt{n-layers}: cantidad de capas, podemos escalar en la profundidad de \texttt{CNN}s.
		\item \texttt{filters}: filtros.
		\item \texttt{kernelsize}: tamaño del \textit{kernel}.
		\item \texttt{ts} = \textit{timestamps} o rodajas de tiempo.
	\end{itemize}
	\item \texttt{GRU} y \texttt{LSTM}
	\begin{itemize}
		\item \texttt{units}: unidades de la celda.
	\end{itemize}
	\item \texttt{MAXPOOL}:
	\begin{itemize}		
		\item \texttt{poolsize}: tamaño de \textit{pool}.
	\end{itemize}
	\item \texttt{FC}
	\begin{itemize}
		\item \texttt{func}: función de activación.
	\end{itemize}
\end{itemize}

\subsubsection{Tipos}

Por tanto en función a las capas explicadas en el marco teórico con anterioridad serán probados 5 arquitecturas de modelo representadas de forma visual en la fig.~\ref{fig:arch_types} para determinar cual es la mejor utilizando como métrica \texttt{MASE} con respecto a los datos de prueba.

\begin{itemize}
	\item CNN: \texttt{CNN -> FC}
	\item LSTM: \texttt{LSTM -> FC}
	\item GRU: \texttt{GRU -> FC}
	\item CNNLSTM: \texttt{CNN -> MAXPOOL -> LSTM -> FC}
	\item CNNGRU: \texttt{CNN -> MAXPOOL -> GRU -> FC}
\end{itemize}

En el caso de las últimas dos redes se utilizó la capa de \texttt{MAXPOOL} para evitar el cuello de la botella de los datos, corroborando que no se afectó el rendimiento del modelo y mejoraron los tiempos de entrenamiento.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.45\textwidth]{layers.eps}
  	\caption{Tipos de arquitecturas.}
  	\label{fig:arch_types}
  	\end{center}
\end{figure}

%\subsection{\href{https://github.com/GeraCollante/tesis-icomp-machinelearning/blob/main/DataImputation.ipynb}{\color{blue}Entrenamiento}}
\subsection{Entrenamiento}
Link training
\subsubsection{Datos de prueba}
Ya tenemos todos los ingredientes necesarios para empezar el entrenamiento de nuestra red neuronal, sin embargo sería coherente utilizar datos de prueba o \textit{dummy data} para corroborar que todo funcione según lo esperado.

Usaremos un \textit{dataset} de la calidad del aire, más precisamente uno que informa sobre el clima y el nivel de contaminación cada hora durante cinco años en la embajada de Estados Unidos en Beijing, China.

Los datos incluyen la fecha y hora, la contaminación denominada concentración de PM2.5 y la información meteorológica, incluido el punto de rocío, la temperatura, la presión, la dirección del viento, la velocidad del viento y la cantidad acumulada de horas de nieve y lluvia. La lista completa de características en los datos brutos es la siguiente:

\begin{itemize}
	\item \texttt{pollution}: concentración de PM2.5, son partículas muy pequeñas suspendidas en el aire que tienen un diámetro de menos de 2.5 micras. Será nuestro \textit{target}.
	\item \texttt{dew}: punto de rocío, es la más alta temperatura a la que empieza a condensarse el vapor de agua contenido en el aire, produciendo rocío, neblina, cualquier tipo de nube o, en caso de que la temperatura sea lo suficientemente baja, escarcha.
	\item \texttt{temp}: temperatura.
	\item \texttt{press}: presión.
	\item \texttt{wnd\_dir}: dirección del viento combinada.
	\item \texttt{wnd\_spd}: velocidad del viento acumulada.
	\item \texttt{snow}: horas acumuladas de nieve.
	\item \texttt{rain}: horas acumuladas de lluvia.
\end{itemize}

Podemos usar estos datos y enmarcar un problema de pronóstico en el que, dadas las condiciones climáticas y la contaminación de las horas anteriores, pronosticamos la contaminación para la próxima hora.

En la fig.~\ref{fig:pollution_dataset} podemos observar que nuestros datos son bastante armoniosos notando 3 ondas sinusoidales, lo que resultará en que nuestro modelo aprenda mejor (pero la mayoría de las series temporales en la vida real no serán tan convenientes).

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{pollution.png}
  	\caption{\textit{Dataset} de contaminación del aire.}
  	\label{fig:pollution_dataset}
  	\end{center}
\end{figure}

Luego deberemos dividir nuestro \textit{dataset} como vimos con anterioridad en 3 sets en la siguiente proporción:
\begin{itemize}
	\item \texttt{train} (\textit{entrenamiento}): $70\%$.
	\item \texttt{validation} (\textit{validación}): $20\%$.
	\item \texttt{test} (\textit{prueba}): $10\%$.
\end{itemize}

Recordar que la métrica \texttt{val\_loss} será aplicada sobre el set de validación y \texttt{MASE} sobre el set de prueba.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{tvt_pollution.png}
  	\caption{\textit{Dataset} de contaminación del aire.}
  	\label{fig:tvt_pollution}
  	\end{center}
\end{figure}

\subsubsection{Desplazamiento temporal}
Una operación importante a realizar en nuestro dataset es el desplazamiento temporal o \textit{time-shifting} para lograr que nuestra red pueda aprender sobre los \textit{timesteps} pasados de nuestras features y comparar su salida contra los \textit{timesteps} futuros en caso de ser la \textit{target feature}.

Para aclarar esta idea pongamos nuestro enfoque en la figura \ref{fig:shift_explain}. Definiremos \texttt{T} a la cantidad de \textit{timesteps} que queremos que nuestro modelo utilice para extraer información pasada, además del actual.

Y definiremos \texttt{HORIZON} como al horizonte de pronóstico, \textit{i.e.} la cantidad de \textit{timesteps} futuros que queremos predecir de nuestra característica objetivo o \textit{target feature}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.9\textwidth]{time-shift.pdf}
  	\caption{Desplazamiento temporal visualizado.}
  	\label{fig:shift_explain}
  	\end{center}
\end{figure}

\subsubsection{Tipo de horizonte}
Un punto importante a aclarar es que es diferente la arquitectura a implementar según si nuestra \textit{target feature} es de horizonte único o multi-horizonte.

\hfill
\textbf{Horizonte único}

Veremos en una perspectiva más amplia el funcionamiento de nuestro modelo para tomar noción de donde nos encontramos y para esta tarea nos será útil la fig.~\ref{fig:unihorizon_model} \citep{rnn_azure}. Básicamente tomamos los \texttt{T} \textit{timesteps} de nuestras \textit{features} para alimentar a la arquitectura (que fueron presentadas en la sección \ref{archs}), en este caso compuesta de celdas \texttt{RNN}, que luego la salida alimenta una neurona que determina el valor de \texttt{t+1}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.75\textwidth]{unihorizon-model.pdf}
  	\caption{Modelo de horizonte único.}
  	\label{fig:unihorizon_model}
  	\end{center}
\end{figure}

\textbf{Horizonte múltiple}

Observemos la fig.~\ref{fig:multihorizon_model} \citep{rnn_ed_azure}. Si determinamos que nuestra \textit{target feature} será de horizonte múltiple, debemos aumentar la complejidad de nuestra arquitectura lo que se traduce en menor maniobrabilidad en lo que se refiere a adición de capas.

Debemos contar con dos capas \texttt{encoder} y \texttt{decoder}, donde una se encargará de procesar los datos de entrada y la otra de decodificar la salida de la primera para obtener la forma de tensor deseada a la salida de la red, lo que sería \texttt{t+1}, \texttt{t+2} y \texttt{t+3} en nuestro ejemplo.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multihorizon-model.pdf}
  	\caption{Modelo de horizonte múltiple.}
  	\label{fig:multihorizon_model}
  	\end{center}
\end{figure}

En la fig.~\ref{fig:multihorizon_plot} podemos observar la salida de un pronóstico de horizonte múltiple con \texttt{T=16} y \texttt{HORIZON=4}. Notamos que a medida que queremos predecir pasos más lejanos perdemos precisión, en la tabla~\ref{tab:multi-horizon-results} se corrobora de manera numérica esta observación.

Además cuando veamos los resultados de los modelos de horizonte único notaremos que \texttt{t+1} tiene mayor precisión que en este tipo de modelos.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{multi-horizon-plot.png}
  	\caption{Pronóstico de horizonte múltiple.}
  	\label{fig:multihorizon_plot}
  	\end{center}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{ccc}
\hline
\texttt{t} & \texttt{MAE} & \texttt{MASE} \\ \hline
\texttt{t+1}        & 0.00527     & 0.454      \\
\texttt{t+2}        & 0.00728     & 0.628      \\
\texttt{t+3}        & 0.00953     & 0.822      \\
\texttt{t+4}        & 0.01167     & 1.006      \\ \hline
\end{tabular}
\caption{Resultados del modelo de horizonte múltiple.}
\label{tab:multi-horizon-results}
\end{table}

\subsubsection{Resultados}
Se entrenaron los 4 modelos principales (posteriormente se mostrarán los resultados de la \texttt{CNN} causal dilatada, ya que requiere un tratamiento diferente) con los siguientes hiper-parámetros:
\begin{itemize}
	\item \texttt{EPOCHS}=200
	\item \texttt{UNITS}=250
	\item \texttt{BATCH\_SIZE}=64
	\item \texttt{T}=8
	\item \texttt{HORIZON}=1
	\item \texttt{CNN\_LAYERS}=3
\end{itemize}

Se obtuvieron los resultados de la fig.~\ref{fig:training_plot}. Los saltos que observamos en el \texttt{val\_loss} (el más notorio en \texttt{CNNLSTM}) se deben a la callback de \texttt{ReduceLROnPlateau} (sección \ref{plateau}), siendo una función de \textit{Keras} muy provechosa para nuestro objetivo.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{training_plots.png}
  	\caption{Resultados de entrenamiento de los modelos \texttt{GRU/LSTM}.}
  	\label{fig:training_plot}
  	\end{center}
\end{figure}

Para el entrenamiento de modelos compuestos exclusivamente de capas \texttt{CNN} dilatadas causales se utilizaron los siguientes hiper-parámetros:
\begin{itemize}
	\item \texttt{KERNEL\_SIZE}=$2$
	\item \texttt{DEEP\_LAYERS}=$1+i$ dónde $i \in \mathbb{N}: 1 \leq i \leq 8$.
	\item \texttt{dilation\_rate}=$i^2$ dónde $i \in \mathbb{N}: 1 \leq i \leq 8$.
\end{itemize}

En la fig.~\ref{fig:train_cnn} representamos , en las cuáles fuimos variando en cada modelo el valor de \texttt{dilation\_rate}.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{train_cnn.png}
  	\caption{Resultados de entrenamiento de los modelos \texttt{CNN}.}
  	\label{fig:train_cnn}
  	\end{center}
\end{figure}

Las diferencias que podemos notar entre la fig.~\ref{fig:training_plot} y la fig.~\ref{fig:train_cnn} es que los modelos que poseen un componente \texttt{RNN} tienden a ser mucho más caóticos en su entrenamiento pero si la cantidad de épocas es suficiente terminan llegando a buen puerto. En cambio en los modelos \texttt{CNNs} puros el entrenamiento es mucho más armonioso y continuo.

En la tabla~\ref{tab:metrics-models} y en la fig.~\ref{fig:mase_per_model} verificamos que el modelo con el mejor \texttt{MASE} es el \texttt{CNNGRU}, sin embargo esto lo realizamos con valores fijos y estándares para los hiper-parámetros de los modelos.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\hline
\multicolumn{1}{l}{model} & \texttt{MAE} & \texttt{MASE} \\ \hline
\texttt{GRU}                                    & 0.0121    & 0.2894        \\
\texttt{LSTM}                                   & 0.0123    & 0.2777        \\
\textbf{\texttt{CNNGRU}}                                 & \textbf{0.0123}    & \textbf{0.2775}        \\
\texttt{CNNLSTM}                                & 0.0123    & 0.2793        \\
\texttt{CNN-1}                                  & 0.0144    & 0.6526        \\
\texttt{CNN-2}                                  & 0.0124    & 0.3476        \\
\texttt{CNN-4}                                  & 0.0126    & 0.3242        \\
\texttt{CNN-8}                                  & 0.0133    & 0.4389        \\
\texttt{CNN-16}                                 & 0.0124    & 0.3172        \\
\texttt{CNN-32}                                 & 0.0127    & 0.3318        \\
\texttt{CNN-64}                                 & 0.0126    & 0.3597        \\
\texttt{CNN-128}                                & 0.0122    & 0.3121        \\ \hline
\end{tabular}
\caption{Métricas de los modelos.}
\label{tab:metrics-models}
\end{table}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.75\textwidth]{mase_per_model.png}
  	\caption{\texttt{MASE} por modelo.}
  	\label{fig:mase_per_model}
  	\end{center}
\end{figure}

Para una mejor visualización de los resultados del mejor modelo, en la fig.~\ref{fig:fore_best_model} se toman las 200 primeras muestras. En el mismo podemos observar que la precisión del modelo es bastante alta, pronosticando valores muy cercanos a los reales. Por tanto, para estos datos el modelo ha resultado exitoso.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{cnngru_final.png}
  	\caption{Pronóstico del mejor modelo.}
  	\label{fig:fore_best_model}
  	\end{center}
\end{figure}

\subsection{Ajuste de hiperparámetros}
Ahora debemos obtener el mejor modelo, por tanto es necesario explorar diversas configuraciones a través del ajuste de hiperparámetros como vimos en la sección \ref{tuning-hp}.

Para esta labor existen diversas librerías (tanto gratis como comerciales) tales como \textit{Ray.tune, Optuna, Hyperopt, Polyaxon, Talos, BayesianOptimization, Metric Optimization Engine, Spearmint, GPyOpt, Scikit-Optimize, etc}. Sin embargo nosotros nos decantaremos por \textit{Keras-tuner}, básicamente por su facilidad de uso además de contar con el soporte de \textit{TensorFlow}.

\subsubsection{\textit{Keras-tuner}}
La librería se compone de diversas clases y métodos que nos permitirán ajustar los hiperparámetros de nuestros modelos creados en \textit{Keras}. \citep{keras-tuner}

\textbf{Clase \texttt{HyperParameters}}

La librería nos permite definir esta clase que sirve como contenedor de hiperparámetros. Una instancia de \texttt{HyperParameters} contiene información sobre el espacio de búsqueda y los valores actuales de cada hiperparámetro.

Además nos provee diversos métodos para decidir de que manera queremos explorar el espacio de búsqueda de un determinado hiperparámetro:

\begin{itemize}
	\item \texttt{Boolean}: elige entre \texttt{True} o \texttt{False}.
	\item \texttt{Choice}: elige entre una lista de opciones.
	\item \texttt{Fixed}: fijo, no permite cambiar su valor.
	\item \texttt{Int}: explora números enteros entre el valor inicial y final, es posible seleccionar el modo y el paso.
	\item \texttt{Float}: \textit{ídem} al ítem anterior, pero con números reales.
\end{itemize}

\textbf{Clase \texttt{Oracle}}

Cada clase \texttt{Oracle} (óraculo) implementa un algoritmo de ajuste de hiperparámetros particular. Un \texttt{Oracle} se pasa como argumento a un \texttt{Tuner} (clase cuyo objetivo es realizar la búsqueda de hiperpárametros). \texttt{Oracle} le dice al \texttt{Tuner} qué hiperparámetros deben probarse a continuación.

La mayoría de las clases de \texttt{Oracle} se pueden combinar con cualquier subclase de \texttt{Tuner} definida por el usuario. Si no se necesita una subclase de \texttt{Tuner} (el caso más común), también se proporciona una serie de clases que empaquetan un \texttt{Tuner} y un \texttt{Oracle} juntos (cómo veremos a continuación).

\textbf{Clase \texttt{Tuner}}

Su propósito es realizar la búsqueda de hiperparámetros en el espacio de búsqueda definido, y como se aclaró en la sección anterior es posible crearlos de forma personalizada pero los algoritmos ofrecidos son más que suficientes para nuestra tarea, que son los analizados en la sección \ref{algo-hp}:
\begin{itemize}
	\item Búsqueda Aleatoria (descartado).
	\item Optimización Bayesiana: parámetro a destacar:
	\begin{itemize}
		\item \texttt{max\_trials}: número total de pruebas (configuraciones de modelo) para probar como máximo.
	\end{itemize}
	\item \textit{Hyperband}: parámetros a destacar:
	\begin{itemize}
		\item \texttt{factor}: factor de reducción ($\eta$) para el número de épocas y el número de modelos para cada \texttt{bracket}.
		\item \texttt{hyperband\_iterations}: el número de veces que se iterará sobre el algoritmo \textit{Hyperband} completo. Una iteración ejecutará aproximadamente $s_{max}(\log_{\eta} s_{max})^2$ épocas acumulativas en todas las pruebas (notación tomada de la sección \ref{hyperband}). Se recomienda establecer esto en un valor tan alto como esté dentro de su presupuesto de recursos.
	\end{itemize}
\end{itemize}

\subsubsection{Espacio de búsqueda}
Para cada tipo de modelo se definió un espacio de búsqueda determinado en función a su arquitectura.

\textbf{\texttt{RNN}}

Se limitó a explorar la cantidad de unidades en las capas recurrentes e intercambiar la función de activación de la capa final.

\begin{python}
# RNN layer
for i in range(8,256,16):
	units = i
# Dense layer
for j in ['relu', '\texttt{sig}oid', 'tanh']:
		dense = j
\end{python}

\textbf{\texttt{CNN}}

En este caso se debió crear una capa \texttt{CNN} \textit{input} y sobre la misma ir apilando las demás capas \texttt{CNN} que fueron variando en cantidad desde 1 a 3.
Además se ajustaron los hiperpárametros de filtros y tamaño del kernel en cada una de ellas.
 
\begin{python}
# For first layer
for i in range(8,64,16):
	filters_0 = i
	for j in range(2,8,2):
		kernel_size_0

# Others layers
for i in range(1,3):
	cnn_i = stack_cnnlayer(dilation_date=2**i)
	for j in range(8,64,16):
		filters_i = j
		for k in range(2,8,2):
			kernel_size_i = k

# Model compile
for lr in [1e-2, 1e-3, 1e-4]:
	model_lr = lr
\end{python}

\textbf{\texttt{CNNRNN}}

Se realizó una configuración mixta de los hiperparámetros de las otras arquitecturas, dónde la única novedad es la búsqueda en los hiperparámetros de la capa \texttt{MaxPooling1D}.

\begin{python}
# First CNN layer
for i in range(8,64,16):
	filters_0 = i
	for j in range(2,8,2):
		kernel_size_0

# Others CNN layers
for i in range(1,3):
	cnn_i = stack_cnnlayer(dilation_date=2**i)
	for j in range(8,64,16):
		filters_i = j
		for k in range(2,8,2):
			kernel_size_i = k
			
# Pooling layer
for i in range(2,8,2):
	pool_size = i
	
# RNN layer
for i in range(8,256,16):
	units = i
	
# Model compile
for lr in [1e-2, 1e-3, 1e-4]:
	model_lr = lr
\end{python}

\subsubsection{Resultados}

Ya definido el espacio de búsqueda de los modelos, se procede a la búsqueda del mejor conjunto de hiperparámetros obteniendo la tabla \ref{tab:tuning-hp} (recordar que se utiliza como métrica \texttt{MAE}). De la misma podemos sacar varias conclusiones interesantes.
\begin{itemize}
	\item El modelo con la mejor puntuación fue \texttt{CNNGRU} en combinación con el algoritmo \textit{Hyperband}, sin embargo el segundo lugar (\texttt{LSTM}/Optimización Bayesiana) le sigue muy de cerca. De hecho todos los modelos están muy parejos.
	\item Hay una clara tendencia a obtener mejores resultados con combinaciones definidas como \texttt{CNNGRU/HB/sig} y \texttt{LSTM/BO/tanh}.
	\item Es curioso notar que a pesar de su arquitectura simple \texttt{LSTM} logró batir los resultados de otros modelos.
	\item Nuestro tope de unidades \texttt{RNN} fue seteado en 256, observamos que los modelos con mejor puntaje están muy cerca de ese valor. Quizás se habrían mejores resultados si hubiéramos definido un espacio de búsqueda mayor para ese hiperparámetro.
\end{itemize}

\begin{table}[H]
\centering
\begin{adjustwidth}{-1.3cm}{}
\begin{tabular}{lcccccccccccc}
\hline
\textbf{}       & \textbf{}               & \multicolumn{7}{c}{\texttt{CNN}} & \texttt{MAXPOOL} & \texttt{RNN} & \texttt{DENSE}                  & \textbf{} \\ \hline
\multicolumn{1}{c}{model} &
  \multicolumn{1}{c|}{algo} &
  layers &
  $f_0$ &
  $ks_0$ &
  $f_1$ &
  $ks_1$ &
  $f_2$ &
  $ks_2$ &
  $ps$ &
  units &
  \multicolumn{1}{c|}{act} &
  \textbf{score} \\ \hline
\texttt{CNNGRU} & \multicolumn{1}{c|}{\texttt{HB}} & 3  & 40  & 8  & 56  & 4 & 56 & 6 & 6                & 232          & \multicolumn{1}{c|}{\texttt{sig}} & 0.011961  \\
\texttt{LSTM}   & \multicolumn{1}{c|}{\texttt{BO}} & -  & -   & -  & -   & - & -  & - & -                & 248          & \multicolumn{1}{c|}{\texttt{tanh}}    & 0.011973  \\
\texttt{CNNGRU} & \multicolumn{1}{c|}{\texttt{HB}} & 3  & 40  & 8  & 56  & 4 & 56 & 6 & 6                & 232          & \multicolumn{1}{c|}{\texttt{sig}} & 0.011980  \\
\texttt{LSTM}   & \multicolumn{1}{c|}{\texttt{BO}} & -  & -   & -  & -   & - & -  & - & -                & 248          & \multicolumn{1}{c|}{\texttt{tanh}}    & 0.011996  \\
\texttt{CNNGRU} & \multicolumn{1}{c|}{\texttt{HB}} & 3  & 40  & 8  & 56  & 4 & 56 & 6 & 6                & 232          & \multicolumn{1}{c|}{\texttt{sig}} & 0.011996  \\
\texttt{LSTM}   & \multicolumn{1}{c|}{\texttt{BO}} & -  & -   & -  & -   & - & -  & - & -                & 248          & \multicolumn{1}{c|}{\texttt{tanh}}    & 0.012001  \\
\texttt{CNNGRU} & \multicolumn{1}{c|}{\texttt{HB}} & 1  & 56  & 6  & -  & - & - & - & 6                & 88           & \multicolumn{1}{c|}{\texttt{sig}} & 0.012003  \\
\texttt{LSTM}   & \multicolumn{1}{c|}{\texttt{BO}} & -  & -   & -  & -   & - & -  & - & -                & 248          & \multicolumn{1}{c|}{\texttt{tanh}}    & 0.012004  \\
\texttt{CNNGRU} & \multicolumn{1}{c|}{\texttt{HB}} & 1  & 40  & 6  & -  & - & - & - & 6                & 120          & \multicolumn{1}{c|}{\texttt{sig}} & 0.012004  \\
\texttt{CNNGRU}          & \multicolumn{1}{c|}{\texttt{HB}} & 2  & 40  & 6  & 40  & 4 & - & - & 6                & 216          & \multicolumn{1}{c|}{\texttt{sig}} & 0.012008  \\ \hline
\end{tabular}
\end{adjustwidth}
\caption{Resultados del ajuste de hiperpárametros.}
\label{tab:tuning-hp}
\end{table}

Ahora haremos énfasis en los rendimientos por arquitectura de los dos algoritmos de ajuste de hiperparámetros utilizados discriminando los análisis por las dos métricas propuestas (\texttt{MAE} y \texttt{MASE}). 

\hfill

\textbf{\texttt{MAE}}

Para esta tarea utilizaremos de soporte la tabla~\ref{tab:algo_vs_arch_mae} y la fig.~\ref{fig:algo_comparison}. Notamos que exceptuando \texttt{LSTM}, el algoritmo \textit{Hyperband} se desempeña mejor en cada uno de los modelos restantes.

\begin{table}[H]
\centering
\begin{tabular}{l|ccccc}
\hline
algo/model & \texttt{CNN} & \texttt{CNNGRU} & \texttt{CNNLSTM} & \texttt{GRU} & \texttt{LSTM} \\ \hline
\textit{BayesianOptimization}         & 0.01260     & 0.01224        & 0.01222         & 0.01218     & 0.01197      \\
\textit{Hyperband}         & 0.01213     & 0.01196        & 0.01203         & 0.01204     & 0.01202      \\ \hline
\end{tabular}
\caption{Mejor puntuación por algoritmo y arquitectura utilizando \texttt{MAE} cómo métrica.}
\label{tab:algo_vs_arch_mae}
\end{table}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{model_vs_algo_hp_mae.png}
  	\caption{Comparación de algoritmos de ajuste de hiperpárametros por modelo y arquitectura utilizando como métrica \texttt{MAE}.}
  	\label{fig:algo_comparison_mae}
  	\end{center}
\end{figure}

\textbf{\texttt{MASE}}

Es muy díficil crear una función de perdida \texttt{MASE} ya que ésta se alimenta de muestras pasadas, añadiendo una complejidad que no fue posible sortear en su implementación (\textit{Keras} no soportaba esta característica). Por tanto utilizamos \texttt{MAE} cómo métrica para entrenar nuestros modelos, sin embargo \texttt{MASE} es nuestra principal métrica a la hora de definir cúal es el mejor modelo para el objetivo propuesto.

Por tanto utilizando la tabla~\ref{tab:algo_vs_arch_mase} y la fig.~\ref{fig:algo_comparison_mase} concluimos que la combinación que posee el mejor rendimiento en nuestro \textit{set} de datos es \texttt{CNN} con Optimización Bayesiana.

\begin{table}[H]
\centering
\begin{tabular}{l|ccccc}
\hline
algo/model & \texttt{CNN} & \texttt{CNNGRU} & \texttt{CNNLSTM} & \texttt{GRU} & \texttt{LSTM} \\ \hline
\textit{BayesianOptimization}         & 0,208        & 0,300           & 0,306            & 0,243        & 0,297         \\
\textit{Hyperband}         & 0,253        & 0,332           & 0,290            & 0,628        & 0,304         \\ \hline
\end{tabular}
\caption{Mejor resultado por algoritmo y arquitectura utilizando como métrica \texttt{MASE}.}
\label{tab:algo_vs_arch_mase}
\end{table}

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=0.8\textwidth]{model_vs_algo_hp_mase.png}
  	\caption{Comparación de algoritmos de ajuste de hiperpárametros por modelo y arquitectura utilizando como métrica \texttt{MAE}.}
  	\label{fig:algo_comparison_mase}
  	\end{center}
\end{figure}

En la fig.~\ref{fig:best_model_hp} podemos notar la fina precisión del modelo al predecir los instantes \texttt{t+1}, convaleciendo de manera visual el análisis realizado.

\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1\textwidth]{cnn_bo_best.png}
  	\caption{Pronóstico del mejor modelo una vez realizado el ajuste de hiperparámetros.}
  	\label{fig:best_model_hp}
  	\end{center}
\end{figure}

Para destacar la utilidad del ajuste de hiperparámetros recordemos la tabla~\ref{tab:metrics-models}. En la misma notamos que el mejor modelo (\texttt{CNNGRU}) logró un \texttt{MASE} de $0.278$ mientras que el modelo (\texttt{CNN}) de mejor rendimiento una vez aplicado el ajuste de hiperparámetros obtuvo $0.208$, una mejora de casi 8 puntos. También es interesante notar que antes del ajuste de hiperpárametros los modelos \texttt{CNN} estaban muy lejos de los mejores rendimientos, pero luego se posicionaron como la mejor opción.

\subsubsection{Conclusión}
Si bien el ajuste de hiperparámetros es mucho más costoso computacionalmente que el entrenamiento de los modelos, si nuestra necesidad radica en obtener el mejor modelo posible con nuestros recursos definitivamente es un paso que no podremos saltear. Si más que precisión se necesita velocidad para obtener un modelo con un rendimiento aceptable no es necesario pasar por este proceso.

\clearpage

\bibliography{tesis}{}
\bibliographystyle{unsrtnat}
%\bibliographystyle{ieeetr}
%\bibliographystyle{plainnat}

\end{document}